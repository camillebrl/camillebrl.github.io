<!DOCTYPE html> <html lang="fr"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Overview de toutes les approches d'explicabilité des LLMs | Camille Barboule </title> <meta name="author" content="Camille Barboule"> <meta name="description" content="Méthodes d'explicabilité de la génération de texte des LLMs"> <meta name="keywords" content="NLP, IR, Agents, Deep-Learning, XAI"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%93%9A&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://camillebrl.github.io/blog/2025/llm_explainability/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightbox2@2.11.5/dist/css/lightbox.min.css" integrity="sha256-uypRbsAiJcFInM/ndyI/JHpzNe6DtUNXaWEUWEPfMGo=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@5.4.4/dist/photoswipe.min.css" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/spotlight.js@0.7.8/dist/css/spotlight.min.css" integrity="sha256-Dsvkx8BU8ntk9Iv+4sCkgHRynYSQQFP6gJfBN5STFLY=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/venobox@2.1.8/dist/venobox.min.css" integrity="sha256-ohJEB0/WsBOdBD+gQO/MGfyJSbTUI8OOLbQGdkxD6Cg=" crossorigin="anonymous"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Camille</span> Barboule </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Overview de toutes les approches d'explicabilité des LLMs</h1> <p class="post-meta"> Created on July 04, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/xai"> <i class="fa-solid fa-hashtag fa-sm"></i> XAI,</a>   <a href="/blog/tag/transformers"> <i class="fa-solid fa-hashtag fa-sm"></i> transformers</a>   ·   <a href="/blog/category/sample-posts"> <i class="fa-solid fa-tag fa-sm"></i> sample-posts</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Nous identifions 4 familles d’approches d’explicabilité des LLMs:</p> <ul> <li>celles de “training data attribution”, identifiant les données d’entraînement qui ont un fort impact (positif ou négatif) sur la génération du LLM, à l’aides des fonctions d’influence, décrites en détail dans ce <a href="https://camillebrl.github.io/blog/2025/explainability_llm_generation/">post</a>,</li> <li>celles de “context attribution”, identifiant quelles parties des données d’entrée (input) a un impact sur quelle partie de la génération du LLM (à l’aide des cartes de saillance et des cartes d’attention notamment),</li> <li>celles d’explicabilité mécanistique (model inference), consistant à trouver les circuits dans le LLM qui capturent certains concepts, avec des circuits identifiés par approches d’observation de l’espace latent (ACP sur les états cachés du LLM à différents niveaux du réseau), de dictionnary learning pour visualiser quels concepts sont capturés par chaque neurones / groupes de neurones, ou par “patching”, en modifiant certains états cachés pour voir l’effet sur le modèle.</li> <li>On peut considérer également une 4ème famille d’approches en étudiant la génération des modèles, notamment des modèles dits de “raisonnement” (thinking) qui détaillent leur raisonnement dans la réponse qu’ils fournissent.</li> </ul> <p>Ces 4 familles sont détaillées dans le papier <a href="https://arxiv.org/pdf/2506.05451" rel="external nofollow noopener" target="_blank">Interpretation Meets Safety: A Survey on Interpretation Methods and Tools for Improving LLM Safety</a>:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/survey-480.webp 480w,/assets/img/explainability_llms/survey-800.webp 800w,/assets/img/explainability_llms/survey-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/explainability_llms/survey.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2 id="explicabilité-par-training-data-attribution">Explicabilité par Training Data Attribution</h2> <h3 id="attribution-basée-sur-la-représentation">Attribution basée sur la représentation</h3> <p>Compare la similarité entre les vecteurs latents de chaque exemple d’entraînement et la sortie</p> <ul> <li> <strong>Références</strong> : Yeh et al. (2018), Tsai et al. (2023), Su et al. (2024b), He et al. (2024b)</li> <li> <strong>Limitation</strong> : N’établit pas de causalité (Cheng et al., 2025a)</li> </ul> <h3 id="méthodes-basées-sur-le-gradient">Méthodes basées sur le gradient</h3> <p>Estiment la sensibilité des paramètres du modèle aux exemples individuels</p> <ul> <li> <strong>TracIn</strong> (Pruthi et al., 2020) : Trace l’influence en mesurant l’alignement entre les gradients</li> <li> <strong>Variantes améliorées</strong> :</li> <li>Han &amp; Tsvetkov (2021, 2022)</li> <li>Yeh et al. (2022)</li> <li>Wu et al. (2022)</li> <li>Ladhak et al. (2023)</li> <li> <strong>Adaptations pour LLM</strong> : Xia et al. (2024), Pan et al. (2025b)</li> </ul> <h3 id="méthodes-basées-sur-les-fonctions-dinfluence">Méthodes basées sur les fonctions d’influence</h3> <p>Estiment comment la pondération d’un exemple affecte les paramètres et prédictions</p> <ul> <li> <strong>Améliorations de scalabilité</strong> :</li> <li>Han et al. (2020)</li> <li>Ren et al. (2020)</li> <li>Barshan et al. (2020)</li> <li>Guo et al. (2021)</li> <li> <strong>Extensions aux LLM</strong> : Grosse et al. (2023), Kwon et al. (2024), Choe et al. (2024)</li> <li> <strong>Débat</strong> : Efficacité contestée due aux hypothèses fortes (convexité du modèle)</li> </ul> <h3 id="data-shapley">Data Shapley</h3> <p>Estime la contribution en approximant l’effet de suppression/ajout de données</p> <ul> <li> <strong>Références</strong> : Ghorbani &amp; Zou (2019), Jia et al. (2019), Feldman &amp; Zhang (2020)</li> <li> <strong>Applications LLM</strong> : Wang et al. (2024b, 2025a)</li> <li> <strong>Limitation</strong> : Coût computationnel élevé, limité aux petits modèles</li> </ul> <p>Les limites de ces approches TDA sont:</p> <ul> <li>Inaccessibilité des données d’entraînement propriétaires (Bommasani et al., 2021)</li> <li>Scalabilité computationnelle</li> </ul> <h2 id="explicabilité-par-context-attribution">Explicabilité par Context Attribution</h2> <p>Attribuer les sorties du modèle à des tokens d’entrée spécifiques pour comprendre leur influence.</p> <p>Dans ce <a href="https://arxiv.org/pdf/2502.15886" rel="external nofollow noopener" target="_blank">papier: A Close Look at Decomposition-based XAI-Methods for Transformer Language Models, 2025</a>, ils comparent plusieurs approches:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/comparison-480.webp 480w,/assets/img/explainability_llms/comparison-800.webp 800w,/assets/img/explainability_llms/comparison-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/explainability_llms/comparison.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="méthodes-basées-sur-lattention">Méthodes basées sur l’attention</h3> <p>Poids d’attention plus élevés = plus grande importance</p> <ul> <li> <strong>Références fondamentales</strong> :</li> <li>Wiegreffe &amp; Pinter (2019)</li> <li>Abnar &amp; Zuidema (2020)</li> <li>Kobayashi et al. (2020)</li> <li> <strong>Agrégation</strong> : Moyenne, max (Tu et al., 2021), attention rollout (Abnar &amp; Zuidema, 2020)</li> <li> <strong>Applications sécurité</strong> : Détection d’hallucinations (Dale et al., 2023; Chuang et al., 2024)</li> </ul> <p>Ces approches basées sur l’attention peuvent être utilisées pour “forcer” le modèle à porter / ne pas porter leur attention sur certains tokens:</p> <ul> <li>Incitation des LLM à porter attention aux tokens de sécurité</li> <li>Suppression des tokens déclencheurs de jailbreak (Pan et al., 2025a)</li> <li>Manipulation de l’attention vers tokens fiables (Zhang et al., 2024b)</li> </ul> <h3 id="méthodes-basées-sur-les-vecteurs">Méthodes basées sur les vecteurs</h3> <p>Décomposent les vecteurs latents en contributions des tokens d’entrée (ACP)</p> <ul> <li> <strong>Références clés</strong> :</li> <li><a href="https://aclanthology.org/2021.emnlp-main.373.pdf" rel="external nofollow noopener" target="_blank">Kobayashi et al., Incorporating Residual and Normalization Layers into Analysis of Masked Language Models, 2021</a></li> <li><a href="https://aclanthology.org/2022.naacl-main.19.pdf" rel="external nofollow noopener" target="_blank">Modarressi et al. GlobEnc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers, 2022</a></li> <li> <a href="https://aclanthology.org/2022.emnlp-main.595.pdf" rel="external nofollow noopener" target="_blank">Ferrando et al. Measuring the Mixing of Contextual Information in the Transformer, 2022</a>, <a href="https://aclanthology.org/2023.acl-long.301.pdf" rel="external nofollow noopener" target="_blank">Ferrando et al., Explaining How Transformers Use Context to Build Predictions, 2023</a> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/logit_diff-480.webp 480w,/assets/img/explainability_llms/logit_diff-800.webp 800w,/assets/img/explainability_llms/logit_diff-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/explainability_llms/logit_diff.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <strong>Applications LLM modernes</strong> : <a href="https://arxiv.org/pdf/2502.15886" rel="external nofollow noopener" target="_blank">Arras et al., A Close Look at Decomposition-based XAI-Methods for Transformer Language Models, 2025</a> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/wai_pipe-480.webp 480w,/assets/img/explainability_llms/wai_pipe-800.webp 800w,/assets/img/explainability_llms/wai_pipe-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/explainability_llms/wai_pipe.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <strong>Limitation</strong> : Nécessitent des conceptions spécifiques au modèle</li> </ul> <h3 id="méthodes-basées-sur-la-perturbation">Méthodes basées sur la perturbation</h3> <p>Modifient les tokens et observent les changements</p> <ul> <li> <strong>Types de perturbations</strong> :</li> <li>Altération de vecteurs latents <a href="https://arxiv.org/pdf/2301.08110" rel="external nofollow noopener" target="_blank">Deiseroth et al., AtMan: Understanding Transformer Predictions Through Memory Efficient Attention Manipulation, 2023</a> </li> <li>Masquage d’embeddings <a href="https://aclanthology.org/2021.emnlp-main.120.pdf" rel="external nofollow noopener" target="_blank">Jacovi et al., Contrastive Explanations for Model Interpretability, 2021</a> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/antman-480.webp 480w,/assets/img/explainability_llms/antman-800.webp 800w,/assets/img/explainability_llms/antman-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/explainability_llms/antman.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>Remplacement de tokens <a href="https://aclanthology.org/2021.acl-long.144.pdf" rel="external nofollow noopener" target="_blank">Finlayson et al., Causal Analysis of Syntactic Agreement Mechanisms in Neural Language Models, 2021</a> </li> <li>Contrefactuels <a href="https://arxiv.org/pdf/2309.13340" rel="external nofollow noopener" target="_blank">Bhattacharjee et al., Towards LLM-guided Causal Explainability for Black-box Text Classifiers, 2023</a> </li> <li> <strong>Valeur de Shapley</strong> : <a href="https://aclanthology.org/2024.nlp4science-1.1.pdf" rel="external nofollow noopener" target="_blank">Horovicz &amp; Goldshmidt, TokenSHAP: Interpreting Large Language Models with Monte Carlo Shapley Value Estimation, 2024</a>, <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4759713" rel="external nofollow noopener" target="_blank">Mohammadi, Explaining Large Language Models Decisions Using Shapley Values, 2024</a> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/tokenshap-480.webp 480w,/assets/img/explainability_llms/tokenshap-800.webp 800w,/assets/img/explainability_llms/tokenshap-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/explainability_llms/tokenshap.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="méthodes-basées-sur-le-gradient-1">Méthodes basées sur le gradient</h3> <p>Calculent le gradient de la sortie par rapport aux embeddings d’entrée</p> <ul> <li> <strong>Évolution</strong> :</li> <li>Modèles classiques : Simonyan et al. (2014), Bach et al. (2015)</li> <li>LLM : <a href="https://aclanthology.org/2024.findings-emnlp.551.pdf" rel="external nofollow noopener" target="_blank">Barkan et al., Improving LLM Attributions with Randomized Path-Integration, 2024</a>, <a href="https://arxiv.org/pdf/2502.09674" rel="external nofollow noopener" target="_blank">Pan et al., The Hidden Dimensions of LLM Alignment: A Multi-Dimensional Analysis of Orthogonal Safety Directions, 2025</a> </li> </ul> <p>Voir le code: https://github.com/BMPixel/safety-residual-space/blob/main/src/experiments/plrp_relevance_tokens.py:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/plrp-480.webp 480w,/assets/img/explainability_llms/plrp-800.webp 800w,/assets/img/explainability_llms/plrp-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/explainability_llms/plrp.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <strong>Explications contrastives</strong> : Jacovi et al. (2021), Sarti et al. (2024)</li> </ul> <p><strong>2.5 Autres approches</strong></p> <ul> <li> <strong>Similarité</strong> : Comparaison embeddings finaux/tokens d’entrée <a href="https://aclanthology.org/2022.emnlp-main.595.pdf" rel="external nofollow noopener" target="_blank">Ferrando et al., Measuring the Mixing of Contextual Information in the Transformer, 2022</a> avec leur approche ALTI:</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/alti-480.webp 480w,/assets/img/explainability_llms/alti-800.webp 800w,/assets/img/explainability_llms/alti-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/explainability_llms/alti.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <strong>Basées sur les prompts</strong> : Instructions aux LLM pour identifier les tokens influents <a href="https://arxiv.org/pdf/2502.11647" rel="external nofollow noopener" target="_blank">Wang et al., DELMAN: Dynamic Defense Against Large Language Model Jailbreaking with Model Editing, 2025</a> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/delman-480.webp 480w,/assets/img/explainability_llms/delman-800.webp 800w,/assets/img/explainability_llms/delman-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/explainability_llms/delman.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <strong>Optimisation</strong> : Recherche d’attributions maximisant les métriques d’interprétabilité <a href="https://aclanthology.org/2023.findings-eacl.182.pdf" rel="external nofollow noopener" target="_blank">Zhou &amp; Shah, The Solvability of Interpretability Evaluation Metrics, 2023</a> </li> </ul> <h2 id="explicabilité-mécanistique-comprendre-quelle-partie-du-llm-est-responsable-de-quel-concept">Explicabilité mécanistique: comprendre quelle partie du LLM est responsable de quel concept</h2> <p>Le papier <a href="https://arxiv.org/pdf/2501.16496" rel="external nofollow noopener" target="_blank">Open Problems in Mechanistic Interpretability</a></p> <h3 id="sondes-sur-lespace-latent">Sondes sur l’espace latent</h3> <p>Projection de l’espace latent calculé sur différents concepts.</p> <p><strong>Hypothèse de base</strong> : Représentation linéaire - les concepts sont encodés comme directions linéaires (cf ACP).</p> <p><strong>Méthodes principales</strong> :</p> <ul> <li> <strong>Vecteurs moyens</strong> : Calcul pour données avec/sans concept spécifique</li> <li>Applications : hallucinations (<a href="https://aclanthology.org/2024.emnlp-main.1012.pdf" rel="external nofollow noopener" target="_blank">Liu et al., On the Universal Truthfulness Hyperplane Inside LLMs</a>, 2024), jailbreaks (<a href="https://arxiv.org/pdf/2406.11717" rel="external nofollow noopener" target="_blank">Arditi et al. Refusal in Language Models Is Mediated by a Single Direction, 2024</a>)</li> <li> <strong>Réduction dimensionnelle</strong> : PCA, SVD pour découvrir les axes de comportements non sûrs</li> <li>Références : <a href="https://arxiv.org/pdf/2402.09733" rel="external nofollow noopener" target="_blank">Duan et al., Do LLMs Know about Hallucination? An Empirical Investigation of LLM’s Hidden States</a>, Ball et al. (2024), Pan et al. (2025a)</li> <li> <strong>Classificateurs de sondage</strong> : Prédiction de propriétés liées à la sécurité</li> <li>Succès : Détection d’hallucinations <a href="https://arxiv.org/pdf/2212.03827" rel="external nofollow noopener" target="_blank">Burns et al., Discovering Latent Knowledge in Language Models Without Supervision, 2022</a>, <a href="https://arxiv.org/pdf/2407.12831" rel="external nofollow noopener" target="_blank">Truth is Universal: Robust Detection of Lies in LLMs, Bürger et al., 2024</a>, jailbreaks <a href="https://aclanthology.org/2024.findings-emnlp.139.pdf" rel="external nofollow noopener" target="_blank">Zhou et al., How Alignment and Jailbreak Work: Explain LLM Safety through Intermediate Hidden States, 2024</a>,</li> <li>Améliorations : Classificateurs non-linéaires (Azaria &amp; Mitchell, 2023), apprentissage contrastif (He et al., 2024a)</li> </ul> <p>Exemple de <a href="https://arxiv.org/pdf/2407.12831" rel="external nofollow noopener" target="_blank">Truth is Universal: Robust Detection of Lies in LLMs, Bürger et al., 2024</a> pour détecter les hallucinations du modèle ou quand le modèle “ment”: Ils font une ACP pour essayer de prouver qu’une partie du réseau encode les faits vrais (que le modèle a appris) et faux (que le modèles n’a pas appris):</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/acp_true_false_statements.PNG" sizes="95vw"></source> <img src="/assets/img/explainability_llms/acp_true_false_statements.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>En gros ils font l’ACP sur les activations du dernier token de chaque phrases (le “.”). puisque le modèle est autorégressif, ce dernier token encode la globalité de la phrase. Pour un modèle non-autorégressif, comme BERT, on pourrait faire ça sur le token [CLS] par exemple, qui a le même rôle normalement (utilisé d’ailleurs pour la classification). Et ils ont affiché les activations sur un ensemble de données comptenant des phrases vraies (violet) et fausses (jaunes), qu’ils avaient taggé au préalable.</p> <p>Et dans ce papier, ils voient que c’est la layer 12 qui sépare le mieux les faits “vrais” et “faux”, ie appris et non appris:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/layer_acp_true_false.PNG" sizes="95vw"></source> <img src="/assets/img/explainability_llms/layer_acp_true_false.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Cet autre papier <a href="https://aclanthology.org/2024.emnlp-main.1012.pdf" rel="external nofollow noopener" target="_blank">Liu et al., On the Universal Truthfulness Hyperplane Inside LLMs, 2024</a> propose cette analyse:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/probe_training-480.webp 480w,/assets/img/explainability_llms/probe_training-800.webp 800w,/assets/img/explainability_llms/probe_training-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/explainability_llms/probe_training.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><mark>Cette approche d'explicabilité permet de "corriger" le modèle, notamment en dirigeant les Vecteurs Latents</mark>:</p> <ul> <li>Ingénierie de représentation : ajout de vecteurs de sécurité identifiés</li> <li>Applications : hallucinations <a href="https://arxiv.org/pdf/2306.03341" rel="external nofollow noopener" target="_blank">Li et al., Inference-Time Intervention: Eliciting Truthful Answers from a Language Model, 2023</a>, jailbreaks <a href="https://arxiv.org/pdf/2308.10248" rel="external nofollow noopener" target="_blank">Turner et al.,Steering Language Models With Activation Engineering, 2023</a> </li> </ul> <p>Exemple des figures de ces papiers pour “corriger” le modèle selon ces direction:</p> <p>Le papier <a href="https://arxiv.org/pdf/2306.03341" rel="external nofollow noopener" target="_blank">Inference-Time Intervention: Eliciting Truthful Answers from a Language Model</a> propose cette approche:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/intervention-480.webp 480w,/assets/img/explainability_llms/intervention-800.webp 800w,/assets/img/explainability_llms/intervention-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/explainability_llms/intervention.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Le papier <a href="https://arxiv.org/pdf/2308.10248" rel="external nofollow noopener" target="_blank">Steering Language Models With Activation Engineering</a> propose cette approche:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/intervention2-480.webp 480w,/assets/img/explainability_llms/intervention2-800.webp 800w,/assets/img/explainability_llms/intervention2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/explainability_llms/intervention2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Le papier <a href="https://arxiv.org/pdf/2406.11717" rel="external nofollow noopener" target="_blank">Refusal in Language Models Is Mediated by a Single Direction</a> qui corrige les poids du modèle de la direction $r$ identifiée comme générant des jailbreaks : $x’ = x - rr^Tx$. L’<strong>ablation directionnelle</strong> « met à zéro » la composante suivant $r$ pour chaque activation du flux résiduel $x \in \mathbb{R}^{d_{\text{model}}}$.</p> <h3 id="perturber-certains-neurones--couches-et-évaluer-limpact">Perturber certains neurones / couches et Évaluer l’Impact</h3> <p><strong>Analyse basée sur le gradient</strong></p> <ul> <li>Calcul des gradients de sortie par rapport aux paramètres</li> <li>Applications : Conflits de connaissances RAG (Jin et al., 2024), générations biaisées (Liu et al., 2024b)</li> </ul> <p><strong>Knockout de composants</strong></p> <ul> <li>Ablation de couches, têtes d’attention ou paramètres</li> <li>Localisation de composants responsables :</li> <li>Hallucinations : Jin et al. (2024), Li et al. (2024a)</li> <li>Jailbreaks : Zhao et al. (2024d), Wei et al. (2024)</li> <li>Biais : Yang et al. (2023b), Ma et al. (2023)</li> </ul> <p><strong>Patching d’activation</strong></p> <ul> <li>Inspiré de l’analyse de médiation causale (Pearl, 2001)</li> <li>Remplacement d’activations intermédiaires</li> <li>Applications : Hallucinations (Monea et al., 2024), biais (Vig et al., 2020)</li> </ul> <p><strong>Circuits computationnels</strong></p> <ul> <li>Extraction de graphes : nœuds = composants, arêtes = flux d’information</li> <li>Références : Geiger et al. (2021), Elhage et al. (2021)</li> <li>Path patching : Wang et al. (2023), Goldowsky-Dill et al. (2023)</li> </ul> <h3 id="déchiffrer-les-vecteurs-latents-à-partir-de-concepts">Déchiffrer les Vecteurs Latents à partir de concepts</h3> <p><strong>Analyse des neurones individuels</strong></p> <ul> <li>Identification des entrées activant fortement un neurone</li> <li>Références : Geva et al. (2021), Foote et al. (2023)</li> <li>Défi : Polysémantique des neurones (Arora et al., 2018)</li> </ul> <p><strong>Autoencodeurs épars (SAE)</strong></p> <ul> <li>Objectif : Désentrelement des concepts superposés</li> <li>Architecture : Encodeur → vecteur épars de concepts → Décodeur</li> <li> <strong>Références clés</strong> :</li> <li>Fondamentaux : Sharkey et al. (2022), Bricken et al. (2023)</li> <li>Améliorations : Rajamanoharan et al. (2024a), Templeton et al. (2024)</li> <li> <strong>Applications sécurité</strong> :</li> <li>Hallucinations : Ferrando et al. (2025), Theodorus et al. (2025)</li> <li>Jailbreaks : Härle et al. (2024), Muhamed et al. (2025)</li> <li>Biais : Hegde (2024), Zhou et al. (2025a)</li> </ul> <p><strong>Logit lens</strong></p> <ul> <li>Projection des vecteurs latents intermédiaires sur l’espace vocabulaire</li> <li>Origine : nostalgebraist (2020), Elhage et al. (2021)</li> <li>Améliorations : Belrose et al. (2023), Din et al. (2023)</li> <li>Applications : Mécanismes de stockage/rappel (Yu et al., 2023), hallucinations (Yu et al., 2024b)</li> </ul> <p>Ces approches permettent de “corriger” le modèle, par exemple en supprimant de neurones risqués via SAE (Soo et al., 2025).</p> <h2 id="explicabilité-par-génération-de-raisonnement-du-llm">Explicabilité par Génération de Raisonnement du LLM</h2> <p>Explorer comment les LLM peuvent interpréter leurs propres sorties en exprimant le raisonnement sous-jacent.</p> <h3 id="raisonnement-en-génération">Raisonnement en génération**</h3> <ul> <li>LLM incités/entraînés à générer réponses + justifications</li> <li> <strong>Références fondamentales</strong> :</li> <li>Camburu et al. (2018)</li> <li>Rajani et al. (2019)</li> <li>Marasovic et al. (2022)</li> <li> <strong>Estimations d’incertitude</strong> : Kadavath et al. (2022), Amayuelas et al. (2024)</li> </ul> <h3 id="chain-of-thought-cot">Chain-of-Thought (CoT)</h3> <p>Génération d’étapes de raisonnement intermédiaires</p> <ul> <li> <strong>Référence originale</strong> : Wei et al. (2022)</li> <li> <strong>Variantes</strong> :</li> <li>Raisonnement complexe : Yao et al. (2023), Besta et al. (2024)</li> <li>Amélioration de la fidélité : Qu et al. (2022), Lyu et al. (2023)</li> <li> <strong>Limitations</strong> : Explications peu fiables (Gao et al., 2023), nécessitant vérification (Weng et al., 2023)</li> </ul> <h3 id="explications-post-hoc">Explications post-hoc</h3> <ul> <li>Évaluation/explication après génération</li> <li>Division des réponses en affirmations factuelles</li> <li>Vérification contre les connaissances du modèle</li> <li> <strong>Applications</strong> : Hallucinations (Dhuliawala et al., 2024), biais (Li et al., 2024b)</li> </ul> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/positional_biais/">Biais Positionnels dans les transformers auto-régressifs</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/explainability_llm_generation/">Les approches d'explicabilité par les exemples appliquées aux LLMs</a> </li> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'camillebrl/camillebrl.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Camille Barboule. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/lightbox2@2.11.5/dist/js/lightbox.min.js" integrity="sha256-A6jI5V9s1JznkWwsBaRK8kSeXLgIqQfxfnvdDOZEURY=" crossorigin="anonymous"></script> <script defer src="/assets/js/photoswipe-setup.js" type="module"></script> <script defer src="https://cdn.jsdelivr.net/npm/spotlight.js@0.7.8/dist/spotlight.bundle.min.js" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/venobox@2.1.8/dist/venobox.min.js" integrity="sha256-LsGXHsHMMmTcz3KqTaWvLv6ome+7pRiic2LPnzTfiSo=" crossorigin="anonymous"></script> <script defer src="/assets/js/venobox-setup.js?897c1d9c0b6fcf82b949511c1609d055" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>