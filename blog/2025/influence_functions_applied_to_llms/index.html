<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Les fonctions d'influence appliquées aux LLMs | Camille Barboule </title> <meta name="author" content="Camille Barboule"> <meta name="description" content="Détail de ma compréhension des fonctions d'influence appliquées aux LLMs"> <meta name="keywords" content="NLP, IR, Agents, Deep-Learning, XAI"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%93%9A&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://camillebrl.github.io/blog/2025/influence_functions_applied_to_llms/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightbox2@2.11.5/dist/css/lightbox.min.css" integrity="sha256-uypRbsAiJcFInM/ndyI/JHpzNe6DtUNXaWEUWEPfMGo=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@5.4.4/dist/photoswipe.min.css" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/spotlight.js@0.7.8/dist/css/spotlight.min.css" integrity="sha256-Dsvkx8BU8ntk9Iv+4sCkgHRynYSQQFP6gJfBN5STFLY=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/venobox@2.1.8/dist/venobox.min.css" integrity="sha256-ohJEB0/WsBOdBD+gQO/MGfyJSbTUI8OOLbQGdkxD6Cg=" crossorigin="anonymous"> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <style>.toc-list{list-style:none;padding-left:0}.toc-list ul{list-style:none;padding-left:1.5em}.toc-list li{margin:.5em 0}.toc-list a{text-decoration:none;color:inherit}.toc-list a:hover{text-decoration:underline}d-article>*:first-child{margin-top:0!important}d-contents+h1,d-contents+h2,d-contents+h3,d-contents+h4,d-contents+h5,d-contents+h6,d-contents+p{margin-top:2rem!important}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Les fonctions d'influence appliquées aux LLMs",
            "description": "Détail de ma compréhension des fonctions d'influence appliquées aux LLMs",
            "published": "July 20, 2025",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Camille</span> Barboule </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Les fonctions d'influence appliquées aux LLMs</h1> <p>Détail de ma compréhension des fonctions d'influence appliquées aux LLMs</p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div id="toc-container" class="toc-list"> </div> </nav> </d-contents> <blockquote> <p>J’ai eu l’opportunité au sein d’Orange d’explorer les fonctions d’influences appliquées au deep learning, un concept introduit notamment dans le papier <a href="https://arxiv.org/pdf/1703.04730" rel="external nofollow noopener" target="_blank">Understanding Black-box Predictions via Influence Functions</a>. Ces fonctions permettent de quantifier l’impact d’une donnée d’entraînement sur une prédiction du modèle. Elles approximent en réalité la méthode “Leave-one-out” qui compare la prédiction du modèle avec et sans cet échantillon dans le dataset d’entraînement (en entraînant le modèle sans et avec). Cette approche m’a particulièrement intéressée dans le contexte des modèles de langage (LLMs). En travaillant sur l’adaptation d’un LLM à un domaine spécifique, j’ai été confrontée à une question cruciale : comment sélectionner les données de fine-tuning pour garantir que le modèle puisse répondre efficacement à des questions spécifiques? On s’était confronté à plusieurs défis dans le cadre de ce projet sur l’adaptation des LLMs à un domaine métier: Comment identifier quelles données permettent réellement d’améliorer les performances sur des tâches ciblées? Comment évaluer l’impact du “continual pretraining” de mon LLM sur un texte précis sur la performance du modèle final sur la tâche cible? Ainsi, j’ai décidé d’étudier un peu plus en détail les fonctions d’influence appliquées aux LLMs et j’ai trouvé ça facinant (et mathématiquement parlant très complexe). Donc je me suis dit que ça serait utile d’en faire un post, et de mettre un peu les différents papiers que j’ai lu (en diagonale ou de manière plus approfondie), ce que j’ai compris des formules, des approximations, des approches pour adaptées ces méthodes aux LLMs, etc. Ce post est un condensé de tout cela!</p> </blockquote> <h1 id="introduction-à-ce-post">Introduction à ce post</h1> <p>Comme expliqué précédemment, l’influence cherche à approximer le Leave-one-out, c’est à dire cherche à estimer l’<mark>impact qu'aurait un exemple d'entraînement sur la perte d'un exemple de test (ou sur plusieurs résultats du modèle sur un jeu de données test)</mark>.</p> <p>Dans l’ensemble de ce post, on va montrer d’où sort la formule suivante, comment elle est adaptée pour son application aux LLMs, et des cas concrets de son application:</p> \[\begin{split} \mathrm{Influence}\bigl(z_{\mathrm{train}}\to z_{\mathrm{test}}\bigr) &amp;= \frac{d}{d\varepsilon}\, \mathcal{L}\bigl(z_{\rm test},\,\theta_\varepsilon(z_\text{train})\bigr)\Big|_{\varepsilon=0} \\ &amp;\approx -\nabla_\theta \,\mathcal{L}\bigl(z_{\mathrm{test}},\,\hat{\theta} \bigr)\,H_\theta^{-1}(\hat{\theta})\,\nabla_\theta \mathcal{L}(z_{\rm train},\hat{\theta}) \\ &amp;\approx -\nabla_\theta \,\mathcal{L}\bigl(z_{\mathrm{test}},\,\hat{\theta} \bigr)\,(G_\theta(\hat{\theta}) + \lambda I)^{-1}\,\nabla_\theta \mathcal{L}(z_{\rm train},\hat{\theta}) \end{split}\] <p>Où:</p> <ul> <li>\(\hat{\theta}\) sont les poids du modèle préentraîné (poids supposés optimiaux pour le jeu de pretraining du LLM)</li> <li>\(\theta_\varepsilon(z_\text{train})\) sont les poids du modèle “modifié”, ie lorsqu’on upweight de $\varepsilon$ l’exemple d’entraînement $z_\text{train}$</li> <li>\(\theta\) est une variable muette (eg pour le gradient, on dérive par rapport aux paramètres du modèle et on applique en un point)</li> <li>Tous les $\theta$ sont des vecteurs colonne des poids du modèle</li> <li>$\mathcal{L}$ est la loss du modèle, donc ici, vu qu’on est dans le cas des LLMs, c’est la cross-entropy loss (negative log-likelihood), ie pour une séquence de taille $T$ ($x_1$, $x_2$, …, $x_T$): \(\mathcal{L} = -\frac{1}{T}\sum_{t=1}^{T} \log P(x_t \mid x_1, x_2, \ldots, x_{t-1})\)</li> <li>$f$ est une fonction (moyenne, ou autre) sur plusieurs résultats du modèle sur un jeu de données test noté $x$. Il nous permet de ne pas évaluer l’impact de l’upweight d’un $z_\text{train}$ sur la loss d’un $z_\text{test}$, mais sur un ensemble de type de données $x$ (car parfois, on ne veut pas calculer l’influence d’une donnée d’entraînement sur la loss d’un prompt, mais sur la performance (pas forcément la loss d’ailleurs) sur un certain type de prompt)</li> <li>$H_\theta$ est la hessienne (dérivée seconde par rapport aux paramètres du modèle)</li> <li>$G_\theta$ est la Hessienne de Gauss-Newton qu’on va voir plus loin dans ce post, qui est en fait une approximation de la hessienne</li> <li>$\lambda$ est un terme dit de “damping”, dû au fait que la loss n’est pas convexe, qu’on verra plus loin dans le post</li> </ul> <p>Note que dans tout ce post, on va utiliser ces notations!</p> <p>La librairie <a href="https://github.com/pomonam/kronfluence" rel="external nofollow noopener" target="_blank">Kronfluence</a> permet de calculer l’influence dans le cas des LLMs. Des repo github proposent des tutos pour plusieurs de libs de calcul de l’influence dans les modèles de deep learning, comme <a href="https://github.com/deel-ai/influenciae" rel="external nofollow noopener" target="_blank">Influenciae</a> (btw, c’est une lib d’un labo de Toulouse!). Les fonctions d’influence permettent de répondre à ce genre de questions:</p> <ul> <li>Est-ce que je devrais <mark>ajouter cet exemple dans mon set d'apprentissage pour améliorer les performances de cette prédiction?</mark> </li> <li>Quels <mark>exemples d'entraînement ont été utiles à la prédiction</mark> de mon modèle?</li> <li>Le modèle s’est trompé: sur quels exemples d’entraînement s’est-il basé pour cette mauvaise prédiction?</li> </ul> <h2 id="11-introduction-sur-les-fonctions-dinfluence">1.1 Introduction sur les fonctions d’influence</h2> <p>Pour introduire les fonctions d’influence appliquées au deep learning, nous nous basons sur le le papier <a href="https://arxiv.org/pdf/1703.04730" rel="external nofollow noopener" target="_blank">Understanding Black-box Predictions via Influence Functions</a>, et notamment sur l’annexe A pour expliquer les différentes formules.</p> <p><strong>L’influence de</strong> $z_{\rm train}$ <strong>sur</strong> $z_{\rm test}$ ($\mathrm{Influence}(z_{\rm train} \to z_{\rm test})$) <strong>se définit comme</strong></p> \[\mathrm{Influence}(z_{\rm train}\to z_{\rm test})\;=\; \left.\frac{d}{d\varepsilon}\,\mathcal{L}\bigl(z_{\rm test},\,\theta_\varepsilon(z_\text{train})\bigr)\right|_{\varepsilon=0}\] <p><strong>L’influence de</strong> $z_{\rm train}$ <strong>sur</strong> $f(x)$ ($\mathrm{Influence}(z_{\rm train} \to f_{\theta_\varepsilon(z_\text{train})}(x))$) <strong>se définit comme</strong></p> \[\mathrm{Influence}(z_{\rm train}\to f(x))\;=\; \left.\frac{d}{d\varepsilon}\,(f_{\theta_\varepsilon(z_\text{train})}(x)\bigr)\right|_{\varepsilon=0}\] <p>En d’autres termes, elle <mark>mesure la sensibilité de la loss de $z_{\rm test}$ (ou de toute fonction $f(x)$) à un « up-weight » infinitésimal de la loss de $z_{\rm train}$.</mark></p> <p>A noter:</p> <ul> <li>C’est l’<strong>impact de l”up-weight” de la loss sur $z_\text{train}$ sur qqch qu’on mesure avec les fonctions d’influence</strong>. En fait pour mesurer l’impact de l”up-weight” de $z_{\text{train}}$ dans la loss globale, on se pose la question: “on se pose la question : <strong>“<mark>si je donnais un peu plus de poids à ce terme de loss dans l’objectif global, comment cela ferait-il bouger mes paramètres et, avec ces nouveaux paramètres, ma performance sur un point de test, ou sur une fonction?</mark>“</strong> . En effet, en deep learning, modifier le poids d’une donnée dans l’entraînement, c’est modifier le poids qu’on donne à sa loss dans l’apprentissage.</li> <li> <mark>$f_{\theta_\varepsilon(z_\text{train})}(x)$ peut être n'importe quelle fonction (exemple: la moyenne des prédictions sur un ensemble de données types $x$</mark> (cf le papier <a href="https://arxiv.org/pdf/2505.19949" rel="external nofollow noopener" target="_blank">Which Data Attributes Stimulate Math and Code Reasoning? An Investigation via Influence Functions</a> qui cherche à calculer l’influence des textes d’entraînement sur la génération de code (moyenne de des log probabilité de la génération de chaque token de code générés dans un benchmark sachant un problème de code en langage naturel à résoudre)), la différence entre 2 prédictions du modèle, …)</li> </ul> <blockquote> <p><mark>Nous allons, dans ce post, entrer dans le détail de comment on calcule cette influence:</mark></p> \[\begin{split} \mathrm{Influence}\bigl(z_{\mathrm{train}}\to z_{\mathrm{test}}\bigr) &amp;= I_{z_\text{test}}\bigl(z_{\mathrm{train}}\bigr) \\ &amp;= \frac{d}{d\varepsilon}\, \mathcal{L}\bigl(z_{\rm test},\ \theta_\varepsilon(z_\text{train})\bigr) \Big|_{\varepsilon=0} \\ &amp;= -\nabla_\theta \,\mathcal{L}\bigl(z_{\mathrm{test}},\,\hat{\theta}\bigr)\,H_\theta^{-1}(\hat{\theta})\,\nabla_\theta \mathcal{L}(z_{\rm train},\hat{\theta}) \end{split}\] <p>Ou bien, si on veut l’influence sur $f(x)$:</p> \[\begin{split} \mathrm{Influence}\bigl(z_{\mathrm{train}}\to f(x)\bigr) &amp;= I_{f(x)}\bigl(z_{\mathrm{train}}\bigr) \\ &amp;= \left.\frac{d}{d\varepsilon}\bigl(f_{\theta_{\varepsilon}(z_{\mathrm{train}})}(x)\bigr)\right|_{\varepsilon=0} \\ &amp;= - \nabla_\theta f_{\hat{\theta}}(x)^\top \, H_\theta(\hat{\theta})^{-1} \, \nabla_\theta \mathcal{L}\bigl(x_{\mathrm{train}},\hat{\theta}\bigr) \end{split}\] </blockquote> <blockquote> <p>On cherche donc à calculer</p> \[\frac{d}{d\varepsilon}\, \mathcal{L}\bigl(z_{\rm test},\ \theta_\varepsilon(z_\text{train})\bigr) \Big|_{\varepsilon=0}\] </blockquote> <h3 id="111-décomposition-avec-chain-rule">1.1.1. Décomposition avec chain rule</h3> <p>On peut voir \(\frac{d}{d\varepsilon}\, \mathcal{L}\bigl(z_{\rm test},\ \theta_\varepsilon(z_\text{train})\bigr) \Big|_{\varepsilon=0}\) comme : \(\frac{d}{d\varepsilon} f(g(\varepsilon))\)</p> <p>avec :</p> <ul> <li>$g(\varepsilon) = \theta_\varepsilon(z_\text{train})$</li> <li>$f(\theta_\varepsilon(z_\text{train})) = \mathcal{L}(z_{\rm test},\theta_\varepsilon(z_\text{train}))$</li> </ul> <p>d’où, <mark>par chain rule</mark> ( \(\frac{d}{d\varepsilon} f \circ g(\varepsilon) = \nabla_\theta f(g(\varepsilon)) \times \frac{d}{d\varepsilon} g(\varepsilon)\) ), on a :</p> \[\colorbox{yellow}{ $\displaystyle \frac{d}{d\varepsilon}\, \mathcal{L}\bigl(z_{\rm test},\ \theta_\varepsilon(z_\text{train})\bigr) \Big|_{\varepsilon=0} = \nabla_\theta \mathcal{L}(z_{\rm test}, \hat{\theta}) \times \frac{d}{d\varepsilon} \theta_\varepsilon(z_\text{train}) \Big|_{\varepsilon=0} $}\] <p>Du coup, <mark>dans un premier temps, il nous faut calculer</mark></p> \[\colorbox{yellow}{ $\displaystyle \frac{d}{d\varepsilon} \theta_\varepsilon(z_\text{train}) \Big|_{\varepsilon=0} $}\] <h3 id="112-calcul-de-fracddvarepsilon-theta_varepsilonz_texttrain-big_varepsilon0">1.1.2. Calcul de $\frac{d}{d\varepsilon} \theta_\varepsilon(z_\text{train}) \Big|_{\varepsilon=0}$</h3> <p><strong>Calculer cela revient à se demander : <mark>comment $\theta_\varepsilon(z_\text{train})$ varie autour de $\theta$ quand on up-weight très légèrement (voisinage de 0) la loss de notre exemple $z_\text{train}$</mark>?</strong></p> <blockquote> <p>Le but ici est de calculer la dérivée de $\theta_\varepsilon(z_\text{train})$ par rapport à $\varepsilon$ pris en $\varepsilon = 0$.</p> </blockquote> <p>Essayons d’abord de voir ce qu’est ce $\theta_\varepsilon(z_\text{train})$.</p> <p><mark>Pour faire cet "up-weight" de la loss de $z_{\text{train}}$ d'un tout petit $\varepsilon$, on perturbe la fonction de perte en ajoutant un petit coefficient $\varepsilon$ sur la perte de $z_{\rm train}$ et on voit comment les paramètres optimaux $\theta$ évoluent avec $\varepsilon$.</mark></p> <p>On repart de ce que ça veut dire de “perturber la fonction de perte en ajoutant un petit coefficient $\varepsilon$ sur la perte de $z_{\rm train}$”: on obtient une nouvelle la loss totale du modèle ($R_\varepsilon(\theta, z_\text{train})$) avec les nouveau poid $\varepsilon$ donné à la loss de $z_{\text{train}}$:</p> <h4 id="1121-theta_varepsilonz_texttrain-sont-les-poids-qui-minimisent-cette-nouvelle-loss-du-modèle-après-lupweight-de-varepsilon-de-z_texttrain">1.1.2.1. $\theta_\varepsilon(z_\text{train})$ sont les poids qui minimisent cette nouvelle loss du modèle après l’upweight de $\varepsilon$ de $z_\text{train}$</h4> <p>Le modèle réentraîné avec cet “upweight” de $\varepsilon$ de $z_\text{train}$ a donc une nouvelle loss de:</p> \[R_\varepsilon(\theta, z_\text{train}) = \frac{1}{n}\sum_{i=1}^n \mathcal{L}(z_i,\theta) \;+\;\varepsilon\,\mathcal{L}(z_{\rm train},\theta).\] <p>Puis, <mark>on cherche les poids $\theta_\varepsilon(z_\text{train})$ qui minimisent cette nouvelle loss</mark>:</p> \[\theta_\varepsilon(z_\text{train}) \;=\; \arg\min_{\theta}\;R_\varepsilon(\theta, z_\text{train})\] <p><mark>Ce qui revient à chercher les $\theta_\varepsilon(z_\text{train})$ dont le gradient de cette nouvelle loss en $\theta$ est nul</mark> , car $\theta_\varepsilon(z_\text{train})$ est un minimum local de $R_\varepsilon$ si et seulement si (si la fonction est convexe: mais pas le cas pour les réseaux de neurone, cf partie 1.3.2) sa dérivée première (gradient) s’annule:</p> \[\nabla_\theta R_\varepsilon\bigl(\theta_\varepsilon(z_\text{train})\bigr) = 0.\] <h4 id="1122-approximation-de-taylor-à-lordre-1-en-varepsilon--0-de-theta_varepsilon">1.1.2.2. Approximation de Taylor à l’ordre 1 en $\varepsilon = 0$ de $\theta_\varepsilon$</h4> <p>Pour cette partie, on écrit pour simplifier $\theta_\varepsilon$ au lieu de $\theta_\varepsilon(z_\text{train})$</p> <p><mark>En développant à l'aide de l'approximation de Taylor en $\hat{\theta}$ pour un tout petit $\varepsilon$, ie proche de 0, donc pour un $\theta_\varepsilon$ proche de $\hat{\theta}$, cette formule</mark>:</p> \[\frac{1}{n}\sum_{i=1}^n \nabla_\theta \mathcal{L}(z_i,\theta_\varepsilon) \;+\;\varepsilon\,\nabla_\theta \mathcal{L}(z_{\rm train},\theta_\varepsilon)\] <blockquote> <p>Rappel de la formule de Taylor à l’ordre 1 en $\varepsilon = 0$:</p> \[f(\theta_\varepsilon) \approx f(\hat{\theta}) + f'(\hat{\theta})\,(\theta_\varepsilon - \hat{\theta})\] </blockquote> <p>On obtient :</p> \[\begin{split} \nabla_\theta R_\varepsilon\bigl(\theta_\varepsilon\bigr) &amp;= \underbrace{\frac{1}{n}\sum_{i=1}^n \nabla_\theta \mathcal{L}(z_i,\theta_\varepsilon) \;+\;\varepsilon\,\nabla_\theta \mathcal{L}(z_{\rm train},\theta_\varepsilon)}_{f(\theta_\varepsilon)} \\ &amp;\approx \underbrace{[ \frac{1}{n}\sum_{i=1}^n \nabla_\theta \mathcal{L}(z_i,\hat{\theta}) \;+\;\varepsilon\,\nabla_\theta \mathcal{L}(z_{\rm train},\hat{\theta}) ]}_{f(\hat{\theta})} \;\; + \;\; \underbrace{[ \frac{1}{n}\sum_{i=1}^n \nabla^2_\theta \mathcal{L}(z_i,\hat{\theta}) \;+\;\varepsilon\,\nabla^2_\theta \mathcal{L}(z_{\rm train},\hat{\theta}) ]}_{f'(\hat{\theta}) = \nabla_\theta f(\hat{\theta})} \;\;(\theta_\varepsilon - \hat{\theta}) \end{split}\] <p>\(\nabla_\theta R_\varepsilon\bigl(\theta_\varepsilon\bigr) = 0\) nous ramène donc à la formule:</p> \[\left[ \frac{1}{n}\sum_{i=1}^n \nabla_\theta \mathcal{L}(z_i,\hat{\theta}) + \varepsilon\,\nabla_\theta \mathcal{L}(z_{\rm train},\hat{\theta}) \right] + \left[ \frac{1}{n}\sum_{i=1}^n \nabla^2_\theta \mathcal{L}(z_i,\hat{\theta}) + \varepsilon\,\nabla^2_\theta \mathcal{L}(z_{\rm train},\hat{\theta}) \right] \; \; (\theta_\varepsilon - \hat{\theta}) \; \; \approx 0\] <p>D’où, en isolant $(\theta_\varepsilon - \theta)$ dans l’équation:</p> \[\theta_\varepsilon - \hat{\theta} \approx - \left[ \frac{1}{n}\sum_{i=1}^n \nabla^2_\theta \mathcal{L}(z_i,\hat{\theta}) + \varepsilon\,\nabla^2_\theta \mathcal{L}(z_{\rm train},\hat{\theta}(z_\text{train})) \right]^{-1} \;\; \times \;\; \left[ \frac{1}{n}\sum_{i=1}^n \nabla_\theta \mathcal{L}(z_i,\hat{\theta}) + \varepsilon\,\nabla_\theta \mathcal{L}(z_{\rm train},\hat{\theta}) \right]\] <p>Or, <mark>puisque $\hat{\theta}$ sont les poids optimaux pour le modèle de base</mark>, $\frac{1}{n}\sum_{i=1}^n \nabla_\theta \mathcal{L}(z_i,\hat{\theta}) = 0$:</p> \[\theta_\varepsilon - \hat{\theta} \approx - \left[ \frac{1}{n}\sum_{i=1}^n \nabla^2_\theta \mathcal{L}(z_i,\hat{\theta}) + \varepsilon\,\nabla^2_\theta \mathcal{L}(z_{\rm train},\hat{\theta}(z_\text{train})) \right]^{-1} \;\; \times \;\; \varepsilon\,\nabla_\theta \mathcal{L}(z_{\rm train},\hat{\theta})\] <h4 id="1123-theta_varepsilon---theta-donne-un-terme-en-fonction-de-varepsilon-x-une-série-en-varepsilon-on-va-montrer-quon-ne-peut-garder-que-la-constante-indépendante-de-varepsilon-de-la-série-à-lordre-1">1.1.2.3. $\theta_\varepsilon - \theta$ donne un terme en fonction de $\varepsilon$ x une série en $\varepsilon$: on va montrer qu’on ne peut garder que la constante (indépendante de $\varepsilon$) de la série à l’ordre 1</h4> <p>Ici, on a :</p> \[\theta_\varepsilon - \hat{\theta} \approx - \left[ \underbrace{\frac{1}{n}\sum_{i=1}^n \nabla^2_\theta \mathcal{L}(z_i,\hat{\theta})}_{= a} + \varepsilon\,\underbrace{\nabla^2_\theta \mathcal{L}(z_{\rm train},\hat{\theta}(z_\text{train}))}_{= b} \right]^{-1} \;\; \times \;\; \varepsilon\,\underbrace{\nabla_\theta \mathcal{L}(z_{\rm train},\hat{\theta})}_{= \beta}\] <p>On sait que $A(I-A)^{-1}$ peut se ramener à une somme : $A(I-A)^{-1} = \sum_{i=0}^{\inf}{A^i}$ =&gt; cf ce <a href="camillebrl.github.io/blog/2025/tips_mathematics_for_ai/">post de blog</a> ou cf les <a href="https://fr.wikipedia.org/wiki/S%C3%A9rie_de_Neumann" rel="external nofollow noopener" target="_blank">séries de Neumann</a>.</p> <p>Du coup on a $(a + \varepsilon b)^{-1}$ qui peut se ramener à quelque chose de la forme :</p> <p>A COMPLETER!!!</p> <p>En gros, l’idée c’est qu’on obtient qqch du genre: \(\begin{split} (a + \varepsilon b) \times \varepsilon \beta &amp;= \varepsilon \beta a + \varepsilon^2 b \beta \\ &amp;\underbrace{\approx}_{\text{à l'ordre 1}} \varepsilon \beta a \end{split}\)</p> <p>Et que les termes en $\varepsilon ^2$ sont négligeables puisqu’on fait une approximation pour $\varepsilon$ proche de 0 à l’ordre 1.</p> <p>En gros, on se retrouve avec:</p> \[\colorbox{orange}{$\displaystyle(\theta_\varepsilon - \theta)$} \approx - [\frac{1}{n}\sum_{i=1}^n \nabla^2_\theta \mathcal{L}(z_i,\hat{\theta})]^{-1} \times \colorbox{cyan}{$\displaystyle \varepsilon $} \,\colorbox{pink}{$\displaystyle \nabla_\theta \mathcal{L}(z_{\rm train},\hat{\theta}) $}\] <p>Et si on <mark>dérive par rapport à $\varepsilon$</mark> on obtient:</p> \[\begin{split} \frac{d}{d \colorbox{cyan}{$\displaystyle \varepsilon $}}\colorbox{orange}{$\displaystyle(\theta_\varepsilon - \theta)$} &amp;= \colorbox{yellow}{$\displaystyle \frac{d}{d\varepsilon} \theta_\varepsilon $} \\ &amp;\approx - [\underbrace{\frac{1}{n}\sum_{i=1}^n \nabla^2_\theta \mathcal{L}(z_i,\hat{\theta})}_{\colorbox{red}{$\displaystyle H_\theta (\hat{\theta}) $}}]^{-1} \times \colorbox{pink}{$\displaystyle \nabla_\theta \mathcal{L}(z_{\rm train},\hat{\theta})$} \\ &amp;\approx - \colorbox{red}{$\displaystyle H_\theta (\hat{\theta}) $}^{-1} \nabla_\theta \mathcal{L}(z_{\rm train},\hat{\theta}) \end{split}\] <p>D’où :</p> \[\colorbox{yellow}{ $\displaystyle \frac{d}{d\varepsilon} \theta_\varepsilon(z_\text{train}) = - H_\theta^{-1}(\hat{\theta}) \nabla_\theta \mathcal{L}(z_{\rm train},\hat{\theta}) $}\] <p>Ainsi, quand on multiplie par $\nabla_\theta \,\mathcal{L}\bigl(z_{\mathrm{test}},\,\hat{\theta}\bigr)$ on obtient a la formule de l’influence:</p> \[\begin{split} \mathrm{Influence}\bigl(z_{\mathrm{train}}\to z_{\mathrm{test}}\bigr) &amp;= I_{z_\text{test}}\bigl(z_{\mathrm{train}}\bigr) \\ &amp;= \frac{d}{d\varepsilon}\, \mathcal{L}\bigl(z_{\rm test},\ \theta_\varepsilon(z_\text{train})\bigr) \Big|_{\varepsilon=0} \\ &amp;= -\nabla_\theta \,\mathcal{L}\bigl(z_{\mathrm{test}},\,\hat{\theta}\bigr)H_\theta^{-1}(\hat{\theta})\nabla_\theta \mathcal{L}(z_{\rm train},\hat{\theta}) \end{split}\] <p>ou:</p> \[\begin{split} \mathrm{Influence}\bigl(z_{\mathrm{train}}\to f(x)\bigr) &amp;= I_{f(x)}(z_{\mathrm{train}}) \\ &amp;= \left.\frac{d}{d\varepsilon}\bigl(f_{\theta_{\varepsilon}(z_{\mathrm{train}})}(x)\bigr)\right|_{\varepsilon=0} \\ &amp;= - \nabla_\theta f_{\hat{\theta}}(x)^\top \, H_\theta(\hat{\theta})^{-1} \, \nabla_\theta \mathcal{L}\bigl(z_{\mathrm{train}},\hat{\theta}\bigr) \end{split}\] <h2 id="12-corrections-des-fonctions-dinfluence">1.2 “Corrections” des fonctions d’influence</h2> <h3 id="121-données-dentraînement-mal-apprises-prédominantes-comment-corriger">1.2.1 Données d’entraînement mal apprises prédominantes: comment corriger?</h3> <p>A noter que le papier <a href="https://arxiv.org/pdf/2006.14651" rel="external nofollow noopener" target="_blank">Influence Functions in Deep Learning Are Fragile</a> indique que les <mark>fonctions d'influence sont biaisées vers les exemples à forte perte</mark>. en effet, le <mark>gradient de la loss des points d'entraînement est plus élevé pour les exemples mal appris</mark>, entraînant un <mark>biais systématique vers ces exemples dans l’attribution d’influence, indépendamment de leur véritable effet sur la fonction qu'on veut mesurer ou sur la loss sur notre $z_\text{test}$</mark>. Le papier <a href="https://arxiv.org/pdf/2003.11630" rel="external nofollow noopener" target="_blank">RelatIF: Identifying Explanatory Training Examples via Relative Influence</a> propose de recalculer les scores d’influence en <mark>normalisant l'influence par l'inverse de la hessienne x le gradient de la loss du training datapoint</mark>, éliminant ainsi les gradients élevés pour des exemples mal appris par le modèle (sans lien avec la fonction à maximiser).</p> \[\begin{split} \mathrm{Influence}\bigl(z_{\mathrm{train}}\to z_{\mathrm{test}}\bigr) &amp;= \frac{\frac{d}{d\varepsilon} \mathcal{L}\bigl(z_{\rm test},\theta_\varepsilon(z_\text{train})\bigr)}{||H_\theta^{-1}(\hat{\theta}) \;\; \nabla_\theta \mathcal{L}(z_\text{test}, \hat{\theta})||} \Big|_{\varepsilon=0} \\ &amp;= \frac{-\nabla_\theta \,f(\theta_\varepsilon(z_\text{train}))H_\theta^{-1}(\hat{\theta})\nabla_\theta \mathcal{L}(z_{\rm train},\theta_\varepsilon(z_\text{train}))}{||H_\theta^{-1}(\hat{\theta}) \;\; \nabla_\theta \mathcal{L}(z_\text{test}, \hat{\theta})||} \end{split}\] <p>Il faut que je revois un peu plus le papier pour comprendre d’où sort cette formule.</p> <h2 id="13-les-limites-de-linfluence-appliquée-au-deep-learning-et-comment-contrer-cela">1.3 Les limites de l’influence appliquée au deep learning et comment contrer cela</h2> <p>Le papier <a href="https://arxiv.org/pdf/2209.05364" rel="external nofollow noopener" target="_blank">If Influence Functions are the Answer, Then What is the Question?</a> explique que les <strong>fonctions d’influence</strong> n’approximent pas fidèlement le retraining « leave-one-out », mais qu’elles approximent en fait à la <strong>fonction de réponse de Bregman proximale</strong> (PBRF), une formulation plus locale autour des paramètres entraînés.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/pbrf-480.webp 480w,/assets/img/explainability_llms/pbrf-800.webp 800w,/assets/img/explainability_llms/pbrf-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/explainability_llms/pbrf.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>En effet:</p> <p>Dans les LLMs, souvent surparamétrés, les optima peuvent être non uniques. La matrice hessienne $H_\theta$ devient alors parfois singulière, empêchant l’existence d’une fonction de réponse unique. De plus, on n’entraîne généralement pas un réseau jusqu’à convergence totale, d’une part pour limiter le coût de calcul, d’autre part pour éviter le surapprentissage. Hors optimum, l’interprétation de la formule de l’influence n’est plus claire et la hessienne peut présenter des valeurs propres négatives.</p> <p><mark>La fonction de réponse de Bregman proximale offre une meilleure approximation des fonctions d’influence dans le contexte du deep learning : elle ajoute un terme d’amortissement $\lambda$ et utilise un linéarisé de Gauss–Newton $G$ pour corriger les problèmes de singularité, de non-convergence et de non-convexité</mark>. Concrètement, la <mark>PBRF repose sur une hessienne de Gauss–Newton amortie $G + \lambda I$, toujours définie positive, garantissant une réponse bien définie</mark>.</p> <p>Note: je dois revoir plus en détail le PBRF pour mieux le comprendre.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/pbrf2-480.webp 480w,/assets/img/explainability_llms/pbrf2-800.webp 800w,/assets/img/explainability_llms/pbrf2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/explainability_llms/pbrf2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Les fonctions d’influence (trait noir pointillé) et la PBRF (trait rouge) appréhendent différemment la modification locale du paysage de perte :</p> <ul> <li>En réglant la pondération d’un exemple $z_m$, la PBRF suit la trajectoire qui minimise/maximise la perte tout en restant proche de $\theta_\varepsilon(z_\text{train})$.</li> <li>Les <mark>fonctions d’influence classiques se bornent à une expansion de Taylor d’ordre 1 autour de $\epsilon=0$, valable seulement en présence d’une fonction strictement convexe et d’un optimum unique</mark>.</li> </ul> <p>On va voir en quoi ça consiste, cet “amortissement” ($\lambda$) (1.3.1) et ce “gauss-newton” ($G$) (1.3.2):</p> <h3 id="131-hesienne-pas-forcément-inversible">1.3.1 Hesienne pas forcément inversible…</h3> <p>Dans les réseaux de neurones, la loss d’entraînement n’est pas fortement convexe (le minimum local n’est pas forcément un minimum global…) donc la hessienne peut être non inversible. Donc, des approches ont été étudiées pour garantir l’inversibilité de la hessienne, en ajoutant notamment un terme dit de “damping” $\lambda &gt;0$.</p> <h3 id="132-hessienne-par-rapport-aux-paramètres-du-réseau-compliquée-à-calculer-pour-des-réseaux-avec-un-grand-nombre-de-paramètres">1.3.2 Hessienne par rapport aux paramètres du réseau compliquée à calculer pour des réseaux avec un grand nombre de paramètres…</h3> <p>Le papier <a href="https://arxiv.org/pdf/2209.05364" rel="external nofollow noopener" target="_blank">If Influence Functions are the Answer, Then What is the Question?</a> propose d’approximer la Hessienne par la Hessienne de Gauss–Newton (GNH), notée $G_\theta$ :</p> \[G_\theta = J_{y\theta}^T \, H_y \, J_{y\theta}\] <p>où :</p> <ul> <li>$J_{y\theta}$ est la matrice Jacobienne des sorties du réseau par rapport aux paramètres ;</li> <li>$H_y = \nabla^2_y \mathcal{L}(y, \theta)$ est la Hessienne de la fonction de coût par rapport aux sorties du réseau. propose d’approximer la Hessienne par la matrice d’information de Fisher (équivalente à la Hessienne de Gauss-Newton).</li> </ul> <p>En fait, on a:</p> \[\underbrace{H_\theta}_{\text{Très difficile}} =\;\;\; \underbrace{J_{y\theta}^T \, H_y \, J_{y\theta}}_{\text{Facile (GNH)}} \;\;\;+ \underbrace{\sum_i \frac{\partial \mathcal{L}}{\partial y_i} \,\nabla_\theta^2 y_i}_{\text{Cauchemar computationnel}}\] <p>avec:</p> <ul> <li>$J_{y\theta}$ : Déjà calculé par backpropagation standard</li> <li>$H_y$ : Matrice $k \times k$ avec $k =$ le nombre de tokens possibles (ex : 151936 pour Qwen)</li> </ul> <p>On va essayer de décortiquer la hessienne pour retrouver cette formule:</p> \[H_\theta = \frac{\partial ^2 \mathcal{L}}{\partial \theta}\] <p>Or, par chain rule, avec $\mathcal{L}$ qui est une fonction de perte (loss) qui dépend de la sortie du modèle $y$, $y$ qui est la sortie du modèle, qui dépend des paramètres du modèle $\theta$, d’où on a: \(\mathcal{L} = \mathcal{L}(y(\theta))\)</p> <p><strong>Rappel: \(\frac{\partial}{\partial x} f(g(x)) = f'(g(x)) \cdot g'(x)\)</strong></p> <p>D’où:</p> \[\frac{\partial \mathcal{L}}{\partial \theta} = \frac{\partial \mathcal{L}}{\partial y} \cdot \frac{\partial y}{\partial \theta}\] <p>Ainsi:</p> \[\begin{split} \frac{\partial ^2 \mathcal{L}}{\partial \theta} &amp;= \frac{\partial}{\partial \theta} (\frac{\partial \mathcal{L}}{\partial \theta}) \\ &amp;= \frac{\partial}{\partial \theta} (\frac{\partial \mathcal{L}}{\partial y} \cdot \frac{\partial y}{\partial \theta}) \end{split}\] <p>Par dérivée d’une multiplication de fonctions : <strong>Rappel: \((f \times g)' = f'g + g'f\)</strong></p> <p>Avec</p> <ul> <li> \[f = \frac{\partial \mathcal{L}}{\partial y}\] </li> <li> \[g = \frac{\partial y}{\partial \theta}\] </li> </ul> <p>On a:</p> \[\begin{split} H_\theta &amp;= \frac{\partial ^2 \mathcal{L}}{\partial \theta} \\ &amp;= \frac{\partial}{\partial \theta} (\frac{\partial \mathcal{L}}{\partial y} \times \frac{\partial y}{\partial \theta}) \\ &amp;= \colorbox{lime}{$\displaystyle \underbrace{\frac{\partial}{\partial \theta} (\frac{\partial \mathcal{L}}{\partial y})}_{= f'}$} \times \colorbox{pink}{$\displaystyle \underbrace{\frac{\partial y}{\partial \theta}}_{= g} $} \;\; + \;\; \colorbox{brown}{$\displaystyle \underbrace{\frac{\partial}{\partial \theta} (\frac{\partial y}{\partial \theta})}_{= g' = \frac{\partial ^2 y}{\partial \theta ^2} = \nabla ^2_\theta y} \times \underbrace{\frac{\partial \mathcal{L}}{\partial y}}_{= f}$} \end{split}\] <p>Concentrons-nous sur \(\frac{\partial}{\partial \theta} (\underbrace{\frac{\partial \mathcal{L}}{\partial y}}_{= h})\):</p> <p>Par chain rule (car la dérivée de $h$ qu’on cherche dépend d’une variable intermédiaire (ici $y$), qui dépend elle-même de $\theta$)</p> \[\begin{split} \colorbox{lime}{$\displaystyle \frac{\partial}{\partial \theta} \left( \frac{\partial \mathcal{L}}{\partial y} \right) $} &amp;= \frac{\partial h}{\partial y} \cdot \frac{\partial y}{\partial \theta} \\ &amp;= \frac{\partial}{\partial y} (\frac{\partial \mathcal{L}}{\partial y}) \cdot \frac{\partial y}{\partial \theta} \\ &amp;= \underbrace{\frac{\partial^2 \mathcal{L}}{\partial y^2}}_{= H_y} \cdot \underbrace{\frac{\partial y}{\partial \theta}}_{= J_{y \theta}} \\ \end{split}\] <p>On se retrouve donc avec: \(H_\theta = \colorbox{pink}{$\displaystyle J_{y\theta}^T $} \, \colorbox{lime}{$\displaystyle H_y \, J_{y\theta} $} + \colorbox{brown}{$\displaystyle \nabla_\theta^2 y \cdot \frac{\partial \mathcal{L}}{\partial y}$}\)</p> <p>Ainsi, afin de ne pas calculer les dérivées secondes à travers tout le réseau (ce qui est très coûteux quand on a beaucoup de paramètres), on utilise, pour l’ensemble des calculs d’influence (surtout pour les LLMs), ce résultat:</p> \[H_\theta^{-1}(\hat{\theta})\approx \bigl(G_\theta + \lambda I\bigr)^{-1}.\] <h3 id="134-factorisation-par-blocs-de-g_theta-et-factorisation-en-produit-de-kronecker-pour-pourvoir-stocker-cette-matrice--paralléliser-les-calculs-entre-couches">1.3.4 Factorisation par blocs de $G_\theta$ et factorisation en produit de Kronecker pour pourvoir stocker cette matrice &amp; paralléliser les calculs entre couches</h3> <p>Au lieu d’inverser directement la grande matrice $\,G_\theta+\lambda I$, le papier <a href="https://arxiv.org/pdf/2505.05017" rel="external nofollow noopener" target="_blank">Scalable Multi-Stage Influence Function for Large Language Models via Eigenvalue-Corrected Kronecker-Factored Parameterization</a> exploite sa structure en blocs correspondant à chaque couche du réseau.</p> \[G = \begin{bmatrix} G_{1,1} &amp; G_{1,2} &amp; \cdots\\ G_{2,1} &amp; G_{2,2} &amp; \\ \vdots &amp; &amp; \ddots \end{bmatrix},\] <p>Pour un réseau à $L$ couches, on a donc \(G = [G_{i,j}]_{1 \leq i, j \leq L}\), avec \(G_{i,j}\) qui représente le bloc entre les paramètres de la couche $i$ et de la couche $j$.</p> <p>Cette séparation en bloc permet au papier de simplifier $G$ en ne gardant que les blocs diagonaux :</p> \[G \approx \tilde{G} = \mathrm{diag}(G_{1,1}, G_{2,2}, \dots, G_{L,L})\] <p>Cela signifie qu’on ignore les interactions entre différentes couches et qu’on ne considère que les blocs $G_{l,l}$ pour chaque couche $l$.</p> <p>Cette approche par blocs permet :</p> <ul> <li>De traiter chaque couche indépendamment</li> <li>D’éviter de stocker/calculer la matrice complète de taille $p \times p$ (où $p$ est nombre total de paramètres)</li> <li>De paralléliser les calculs entre couches</li> </ul> <p>C’est ce qui rend la méthode scalable pour les grands modèles comme les LLMs avec des milliards (plutôt même billions…) de paramètres.</p> <h2 id="14-le-cas-des-llms-besoin-dune-influence-token-wise-ou-sentence-wise">1.4 Le cas des LLMs: besoin d’une influence token-wise ou sentence-wise</h2> <p>Le papier <a href="https://arxiv.org/pdf/2308.03296" rel="external nofollow noopener" target="_blank">Studying Large Language Model Generalization with Influence Functions</a> présente l’application des fonctions d’influence aux LLMs. Dans le cas des LLMs, la loss est la negative log-vraisemblance. La première <mark>particularité d'un LLM, c'est le fait qu'un datapoint est un peu compliqué à définir. On peut supposer qu'il s'agit d'une phrase (et son label, le token suivant la phrase), ou on peut considérer le token lui-même</mark> (l’input étant la phrase le précédent, le label le token en question, par exemple). Mais il est important de bien définir de quoi on parle quand on parle de “datapoint”.</p> <h3 id="141-linfluence-à-léchelle-de-la-phrase-z_m">1.4.1 L’influence à l’échelle de la phrase $z_m$</h3> <p>Supposons que $z_m$ soit cette phrase “le chat est gris”, soit ce datapoint:</p> \[[\text{BOS, le, chat, est, gris}] \rightarrow \text{[EOS]}\] <p>Vu qu’on est dans un cas autorégressif (c’est-à-dire que les tokens sont prédits à partir des tokens précédents de la séquence) :</p> <p>Si \(z_m\) de taille \(T\) :</p> \[\nabla_\theta L(z_m, \theta) = \sum_{t=1}^T(-\nabla_\theta \log p(z_{m,t} \mid z_{m, \lt t}, \theta))\] <p>Nous, \(z_m\) est de taille 6 :</p> \[\begin{split} \nabla_\theta L(z_m, \theta) &amp;= -\nabla_\theta \log p(\text{le} \mid \text{[BOS]}, \theta) \\ &amp;\quad - \nabla_\theta \log p(\text{chat} \mid \text{[[BOS], le]}, \theta) \\ &amp;\quad - \nabla_\theta \log p(\text{est} \mid \text{[BOS], le, chat}, \theta) \\ &amp;\quad - \nabla_\theta \log p(\text{gris} \mid \text{[BOS], le, chat, est}, \theta) \\ &amp;\quad - \nabla_\theta \log p(\text{[EOS]} \mid \text{[BOS], le, chat, est, gris}, \theta) \end{split}\] <h3 id="142-linfluence-à-léchelle-des-tokens-t-dans-la-phrase-z_m">1.4.2 L’influence à l’échelle des tokens $t\;\;$ dans la phrase $z_m$</h3> <p>On peut aussi considérer l’échelle du token, comme on l’a mis plus haut, en considérant l’input comme étant la phrase précédant ce token, et le label ce token en question.</p> <p>On a ici $\nabla_\theta L(z_m, \theta)$ qui est la somme des gradients de la loss au niveau de chaque token. Du coup, il suffit de prendre $- \nabla_\theta \log p(\text{gris} \mid \text{[BOS], le, chat, est}, \theta)$ pour avoir l’influence du token gris dans la séquence par exemple. On peut ainsi avoir l’information token par token.</p> <p>Et ça c’est ce qu’on obtient ici (cf <a href="https://arxiv.org/pdf/2308.03296" rel="external nofollow noopener" target="_blank">Studying Large Language Model Generalization with Influence Functions, Grosse 2023</a>) :</p> \[\nabla_\theta L(\text{token t dans } z_m, \theta) = \nabla_\theta \log p(\text{token t} \mid \text{ce qui est avant token t dans } z_m, \theta)\] <p>On obtient donc la formule:</p> \[I_f(z_{m,t}) = \nabla_{\theta}f_{\theta_\varepsilon(z_\text{train})}(x)^{T} H^{-1} \nabla_{\theta}\log p(z_{m,t}\mid z_{m,\lt t}, \theta)\] <p>Prenons l’exemple suivant: on prend $f = \log p(\text{“hydrogen and oxygen”} \mid \text{“Water is composed of”})$ et $z_m$ qui est le texte ci-dessous. On peut afficher l’influence token par token dans le texte:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/image-480.webp 480w,/assets/img/explainability_llms/image-800.webp 800w,/assets/img/explainability_llms/image-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/explainability_llms/image.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2 id="15-le-cas-des-llms-beaucoup-de-données-dentraînement-eg-36-trillions-de-tokens-pour-qwen3--query-batching-ou-semantic-matching-pour-ne-pas-calculer-linfluence-sur-toutes-les-données-trop-coûteux">1.5 Le cas des LLMs: beaucoup de données d’entraînement (eg. 36 trillions de tokens pour Qwen3) =&gt; query batching ou semantic matching pour ne pas calculer l’influence sur toutes les données (trop coûteux)</h2> <p>Le papier <a href="https://arxiv.org/pdf/2308.03296" rel="external nofollow noopener" target="_blank">Studying Large Language Model Generalization with Influence Functions</a> propose une approche pour éviter de calculer les gradients de tous les exemples d’entraînement candidats pour chaque requête d’influence. Pour cela, ils “filtrent” les données d’entraînement par rapport à la phrase test via un filtrage TF-IDF et une approche qu’ils introduisent de “query batching”.</p> <h3 id="151-le-filtrage-tf-idf">1.5.1 Le filtrage TF-IDF</h3> <p>Le filtrage TF-IDF utilise une technique classique de recherche d’information pour présélectionner les séquences d’entraînement les plus susceptibles d’être influentes. L’intuition derrière est que les séquences pertinentes devraient avoir au moins un certain chevauchement de tokens avec la requête.</p> <p>Ils retiennent les top 10,000 séquences selon le score TF-IDF Calcul d’influence et calculent les influences uniquement sur ces séquences présélectionnées.</p> <h3 id="152-le-query-batching">1.5.2 Le Query-Batching</h3> <p>Dans un LLM, on a beaucoup d’exemples $z_m$ d’entraînement. Donc, on calcule séparemment $\nabla_\theta\mathcal{L}(z_m, \theta_\varepsilon(z_\text{train}))$ et $\nabla_{\theta} f(\theta_\varepsilon(z_\text{train}))^\top \, H^{-1}$ qui se calcule en une fois.</p> <p>Pour stocker de nombreux gradients de requêtes en mémoire ($\nabla_\theta\mathcal{L}(z_m, \theta_\varepsilon(z_\text{train})) \; \forall \; z_m$), ils approximent chaque matrice de gradient préconditionné comme étant de rang faible (rank-32 dans leurs expériences).</p> <p>Ainsi, pour chaque requête, ils n’ont pas à refaire les calculs! Ils ont juste à calculer $\nabla_{\theta} f(\theta_\varepsilon(z_\text{train}))$.</p> <h2 id="16-le-cas-des-llms-plusieurs-couches-dentraînement-pretraining-fine-tuning-alignement---multi-stage-influence-functions">1.6 Le cas des LLMs: plusieurs couches d’entraînement (pretraining, fine-tuning, alignement, …) =&gt; multi-stage influence functions</h2> <p>Le papier <a href="https://arxiv.org/pdf/2505.05017" rel="external nofollow noopener" target="_blank">Scalable Multi-Stage Influence Function for Large Language Models via Eigenvalue-Corrected Kronecker-Factored Parameterization</a> explique que la fonction d’influence classique $I_f(z_{m,t}) = \nabla_{\theta}f_{\theta_\varepsilon(z_\text{train})}(x)^{T} H^{-1} \sum_{t=1}^T(-\nabla_\theta \log p(z_{m,t} \mid z_{m, &lt;t}, \theta))$ permet de quantifier l’impact d’une phrase d’entraînement sur les prédictions du modèle. Cependant, on a des modèles qui sont passés par plusieurs phases d’entraînement pour les LLMs (avec plusieurs données différentes). En effet, les LLMs sont pré-entraînés (modèles “base”), puis instruct-tuné (modèles “chat”), puis passent par du reinforcement learning (ou du “faux” réinforcement learning (DPO, …)) pour la phase d’alignement. Donc notre formule ne marche plus si on prend un modèle “chat” par exemple (les 3/4 des modèles qu’on trouve sur huggingface) et qu’on veut calculer l’influence d’une phrase du jeu de pre-entraînement par exemple. Or, ce sont ces données de pré-entraînement qui nous intéressent puisque la majorité des connaissances d’un LLM sont acquises pendant le pré-entraînement. Sans pouvoir les tracer, on ne peut pas expliquer d’où viennent les réponses du modèle.</p> <p>Ainsi, les auteurs du papier proposent une connexion entre l’espace des paramètres du modèle pré-entraîné et celui du modèle fine-tuné. L’intuition est que le fine-tuning ne devrait pas trop éloigner les paramètres de leur état pré-entraîné. On reformule donc l’objectif de fine-tuning avec une contrainte de proximité euclidienne :</p> \[\theta^{ft} = \arg\min_\theta \mathcal{L}_{ft}(\theta) + \frac{\alpha}{2}||\theta - \theta^{pt}||_2^2\] <p>où :</p> <ul> <li>$\mathcal{L}_{ft}(\theta)$ est la loss de fine-tuning</li> <li>$\alpha \in \mathbb{R}^+$ est un hyperparamètre contrôlant la proximité</li> <li>\(\|\theta - \theta^{pt}\|_2^2\) est la distance euclidienne entre les paramètres du modèle pré-entraîné avec le modèle fine-tuné (final)</li> </ul> <p>Avec cette reformulation, on peut dériver la fonction d’influence multi-étapes :</p> \[I_f(z_m) = \nabla_\theta f(\theta^{ft})^T \left(\nabla^2_\theta \mathcal{L}_{ft}(\theta^{ft}) + \alpha I\right)^{-1} \left(\nabla^2_\theta \mathcal{L}_{pt}(\theta^{pt})\right)^{-1} \nabla_\theta \mathcal{L}(z_m, \theta^{pt})\] <p>Ainsi, on a 2 hessiennes:</p> <ul> <li> <strong>Hessienne du pré-entraînement</strong> : \(\left(\nabla^2_\theta \mathcal{L}_{pt}(\theta^{pt})\right)^{-1}\) <ul> <li>Calculée aux paramètres $\theta^{pt}$ (modèle pré-entraîné)</li> <li>Capture la courbure de la loss de pré-entraînement</li> </ul> </li> <li> <strong>Hessienne du fine-tuning</strong> : \(\left(\nabla^2_\theta \mathcal{L}_{ft}(\theta^{ft}) + \alpha I\right)^{-1}\) <ul> <li>Calculée aux paramètres $\theta^{ft}$ (modèle fine-tuné)</li> <li>Inclut le terme de régularisation $\alpha I$ qui encode la contrainte de proximité</li> </ul> </li> </ul> <p>Cette double inversion de Hessienne permet de :</p> <ul> <li> <strong>Première inversion</strong> : Transformer le gradient de l’exemple de pré-entraînement en changement de paramètres</li> <li> <strong>Seconde inversion</strong> : Propager ce changement à travers le fine-tuning pour voir son impact final</li> </ul> <p>C’est comme si on “remontait” l’influence à travers deux étapes d’entraînement successives.</p> <h2 id="17-beaucoup-de-sujets-récents-de-recherche-utilisent-les-fonctions-dinfluence-pour-déterminer-les-données-utiles-quon-peut-utiliser-pour-fine-tuner-le-modèle-pour-améliorer-la-génération-dun-llm-ou-ajouter-une-connaissance-au-modèle-par-exemple">1.7 Beaucoup de sujets récents de recherche utilisent les fonctions d’influence pour déterminer les données utiles (qu’on peut utiliser pour fine-tuner le modèle) pour améliorer la génération d’un LLM, ou ajouter une “connaissance” au modèle par exemple</h2> <p>Les fonctions d’influence sont un bon moyen d’évaluer l’impact de chaque exemple de données sur les performances d’un LLM dans un domaine donné. Pour l’instruction-tuning, elles permettent de mesurer précisément quels couples question-réponse contribuent le plus à la qualité des réponses générées dans un domaine donné. Pour le continual pretraining, elles identifient les phrases dont l’ajout ou la suppression modifie le plus la capacité du modèle à maîtriser un vocabulaire ou un style spécifique. Les fonctions d’influence sont de plus en plus utilisées pour affiner le corpus d’apprentissage et maximiser le gain de performance du LLM sur un domaine cible. Voici quelques exemples de papier faisant cela:</p> <p><a href="https://arxiv.org/pdf/2505.12762" rel="external nofollow noopener" target="_blank">IDEAL: Data Equilibrium Adaptation for Multi-Capability Language Model Alignment</a></p> <details> <summary style="cursor: pointer;">Cliquez pour voir l’illustration IDEAL</summary> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/IDEAL-480.webp 480w,/assets/img/explainability_llms/IDEAL-800.webp 800w,/assets/img/explainability_llms/IDEAL-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/explainability_llms/IDEAL.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </details> <p><a href="https://arxiv.org/pdf/2505.12250" rel="external nofollow noopener" target="_blank">Not All Documents Are What You Need for Extracting Instruction Tuning Data</a></p> <details> <summary style="cursor: pointer;">Cliquez pour voir l’illustration de EQUAL</summary> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/EQUAL-480.webp 480w,/assets/img/explainability_llms/EQUAL-800.webp 800w,/assets/img/explainability_llms/EQUAL-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/explainability_llms/EQUAL.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </details> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'camillebrl/camillebrl.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Camille Barboule. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/lightbox2@2.11.5/dist/js/lightbox.min.js" integrity="sha256-A6jI5V9s1JznkWwsBaRK8kSeXLgIqQfxfnvdDOZEURY=" crossorigin="anonymous"></script> <script defer src="/assets/js/photoswipe-setup.js" type="module"></script> <script defer src="https://cdn.jsdelivr.net/npm/spotlight.js@0.7.8/dist/spotlight.bundle.min.js" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/venobox@2.1.8/dist/venobox.min.js" integrity="sha256-LsGXHsHMMmTcz3KqTaWvLv6ome+7pRiic2LPnzTfiSo=" crossorigin="anonymous"></script> <script defer src="/assets/js/venobox-setup.js?897c1d9c0b6fcf82b949511c1609d055" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> <script>
      document.addEventListener('DOMContentLoaded', function() {
        // Fonction pour créer un ID à partir du texte (compatible avec Jekyll slugify)
        function createId(text) {
          return text.trim()
            .toLowerCase()
            .normalize('NFD').replace(/[\u0300-\u036f]/g, '') // Enlever les accents
            .replace(/[^\w\s-]/g, '') // Enlever les caractères spéciaux
            .replace(/\s+/g, '-') // Remplacer les espaces par des tirets
            .replace(/-+/g, '-') // Éviter les tirets multiples
            .replace(/^-|-$/g, ''); // Enlever les tirets au début et à la fin
        }
        
        // Fonction pour décoder et normaliser une ancre d'URL
        function normalizeAnchor(anchor) {
          try {
            // Décoder l'URL
            const decoded = decodeURIComponent(anchor);
            // Appliquer la même normalisation que createId
            return createId(decoded);
          } catch (e) {
            return anchor;
          }
        }
        
        // Récupérer tous les headers dans l'article
        const article = document.querySelector('d-article');
        const headers = article.querySelectorAll('h1, h2, h3, h4, h5, h6');
        const tocContainer = document.getElementById('toc-container');
        
        if (headers.length === 0) return;
        
        // Filtrer les headers pour exclure ceux dans d-contents et d-title
        const filteredHeaders = Array.from(headers).filter(header => {
          return !header.closest('d-contents') && !header.closest('d-title');
        });
        
        if (filteredHeaders.length === 0) return;
        
        // Structure pour construire la hiérarchie
        let currentList = document.createElement('ul');
        tocContainer.appendChild(currentList);
        
        let stack = [{level: 0, list: currentList}];
        
        filteredHeaders.forEach((header) => {
          // Ajouter un ID au header s'il n'en a pas
          if (!header.id) {
            header.id = createId(header.textContent);
          }
          
          // Déterminer le niveau (h1=1, h2=2, etc.)
          const level = parseInt(header.tagName.substring(1));
          
          // Gérer la hiérarchie
          while (stack.length > 1 && stack[stack.length - 1].level >= level) {
            stack.pop();
          }
          
          // Si on descend dans la hiérarchie
          if (stack[stack.length - 1].level < level) {
            const newList = document.createElement('ul');
            const lastItem = stack[stack.length - 1].list.lastElementChild;
            if (lastItem) {
              lastItem.appendChild(newList);
            } else {
              stack[stack.length - 1].list.appendChild(newList);
            }
            stack.push({level: level, list: newList});
          }
          
          // Créer l'élément de liste
          const listItem = document.createElement('li');
          const link = document.createElement('a');
          link.href = '#' + header.id;
          link.textContent = header.textContent;
          link.addEventListener('click', function(e) {
            e.preventDefault();
            header.scrollIntoView({ behavior: 'smooth', block: 'start' });
          });
          
          listItem.appendChild(link);
          stack[stack.length - 1].list.appendChild(listItem);
        });
        
        // Nettoyer les listes vides
        const emptyLists = tocContainer.querySelectorAll('ul:empty');
        emptyLists.forEach(list => list.remove());
        
        // Gérer la navigation depuis l'URL
        function navigateToHash() {
          if (window.location.hash) {
            const hash = window.location.hash.substring(1);
            const normalizedHash = normalizeAnchor(hash);
            
            // Essayer de trouver l'élément par ID normalisé
            let targetElement = document.getElementById(normalizedHash);
            
            // Si pas trouvé, essayer avec le hash original
            if (!targetElement) {
              targetElement = document.getElementById(hash);
            }
            
            // Si toujours pas trouvé, chercher dans tous les headers
            if (!targetElement) {
              filteredHeaders.forEach(header => {
                if (createId(header.textContent) === normalizedHash) {
                  targetElement = header;
                }
              });
            }
            
            if (targetElement) {
              setTimeout(() => {
                targetElement.scrollIntoView({ behavior: 'smooth', block: 'start' });
              }, 100);
            }
          }
        }
        
        // Naviguer au chargement de la page
        navigateToHash();
        
        // Écouter les changements de hash
        window.addEventListener('hashchange', navigateToHash);
      });
    </script> </body> </html>