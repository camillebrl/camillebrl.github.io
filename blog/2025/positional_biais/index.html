<!DOCTYPE html> <html lang="fr"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Biais Positionnels dans les transformers auto-régressifs | Camille Barboule </title> <meta name="author" content="Camille Barboule"> <meta name="description" content="Description du biais positionnel dans les transformers auto-régressifs"> <meta name="keywords" content="NLP, IR, Agents, Deep-Learning, XAI"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%93%9A&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://camillebrl.github.io/blog/2025/positional_biais/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightbox2@2.11.5/dist/css/lightbox.min.css" integrity="sha256-uypRbsAiJcFInM/ndyI/JHpzNe6DtUNXaWEUWEPfMGo=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@5.4.4/dist/photoswipe.min.css" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/spotlight.js@0.7.8/dist/css/spotlight.min.css" integrity="sha256-Dsvkx8BU8ntk9Iv+4sCkgHRynYSQQFP6gJfBN5STFLY=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/venobox@2.1.8/dist/venobox.min.css" integrity="sha256-ohJEB0/WsBOdBD+gQO/MGfyJSbTUI8OOLbQGdkxD6Cg=" crossorigin="anonymous"> <link defer rel="stylesheet" type="text/css" href="https://tikzjax.com/v1/fonts.css"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Camille</span> Barboule </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Biais Positionnels dans les transformers auto-régressifs</h1> <p class="post-meta"> Created on June 16, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/xai"> <i class="fa-solid fa-hashtag fa-sm"></i> XAI,</a>   <a href="/blog/tag/biais"> <i class="fa-solid fa-hashtag fa-sm"></i> biais,</a>   <a href="/blog/tag/transformers"> <i class="fa-solid fa-hashtag fa-sm"></i> transformers</a>   ·   <a href="/blog/category/sample-posts"> <i class="fa-solid fa-tag fa-sm"></i> sample-posts</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="introduction-sur-le-biais-positionnel">Introduction sur le biais positionnel</h2> <h3 id="le-biais-positionnel--cest-quoi-">Le biais positionnel : c’est quoi ?</h3> <p><a href="https://arxiv.org/pdf/2307.03172" rel="external nofollow noopener" target="_blank">L’article <em>Lost in the Middle: How Language Models Use Long Contexts</em></a> décrit comment, pour des contextes très longs, les LLMs se focalisent surtout sur les débuts et la fin de la séquence.</p> <div style="display: flex;"> <div style="flex: 0 0 60%;"> C'est la tendance du modèle à se concentrer excessivement sur certaines parties de l'entrée, qu'importe la sémantique. Et cette concentration influence significativement les performances et la fiabilité des transformers: en fonction de l'ordre des éléments dans le prompt, le modèle va générer des réponses différentes. Notamment, plus on a un contexte long, plus le modèle a tendance à se concentrer sur les éléments au début et à la fin du prompt (effet "lost in the middle"). </div> <div style="flex: 0 0 40%; text-align: center;"> ![U-shape](assets/img/positional_biais/u-shape.PNG) </div> </div> <h3 id="le-biais-positionnel--ça-vient-doù-">Le biais positionnel : ça vient d’où ?</h3> <ul> <li> <p><strong>Le masque causal</strong><br> Avec un masque causal, chaque token ne peut voir que les tokens qui le précèdent. Cela signifie que les tokens situés au début sont accessibles à presque tous les calculs d’attention ultérieurs, alors que ceux situés plus tard ne le sont qu’à partir d’un certain point. \begin{tikzpicture}[scale=0.2, rotate=90] \foreach \y in {0,…,4} { \foreach \x in {0,…,4} { % On remplit si y &gt; x (au-dessus de la diagonale) \ifnum \x&gt;\y % Rien \else</p> <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>    \fill[blue] (\x,\y) rectangle (\x+1,\y+1);
  \fi
}   }   \draw[step=1,gray] (0,0) grid (5,5); \end{tikzpicture}
</code></pre></div> </div> </li> <li> <p><strong>L’encodage de position</strong> (relatif, RoPE…)<br> Ce type d’encodage de position favorisent la proximité avec le token courant. Ainsi, les jetons situés près de la fin de la séquence bénéficient d’une attention renforcée, car leur représentation est plus fortement influencée par la similarité de position avec le token en cours de génération. \begin{tikzpicture}[baseline=(x1.base)] %—————————————— % 1) Placement des noeuds sur une SEULE ligne %—————————————— \node (x1) at (0,0) {$x_1$}; \node (x2) at (1.5,0) {$x_2$}; \node (x3) at (3.0,0) {$x_3$}; \node (dots)at (4.5,0) {$\dots$}; \node (xn2) at (6.0,0) {$x_{n-2}$}; \node (xn1) at (7.5,0) {$x_{n-1}$}; \node (xn) at (9.0,0) {$x_n$};</p> <p>%—————————————— % 2) Tracé des flèches courbes vers le HAUT % depuis x_n vers les autres noeuds %—————————————— % On utilise les options out=… et in=… pour définir % des angles qui font monter les arcs au-dessus de la ligne. % Angles ajustables pour éviter tout chevauchement. \draw[-&gt;, thick, red!70!black] (xn.north) to[out=135, in=45] (x1.north); \draw[-&gt;, thick, red!90!black] (xn.north) to[out=130, in=50] (x2.north); \draw[-&gt;, thick, red!50] (xn.north) to[out=125, in=55] (x3.north); \draw[-&gt;, thick, green!80!black] (xn.north) to[out=120, in=60] (xn2.north); \draw[-&gt;, thick, green!50] (xn.north) to[out=115, in=65] (xn1.north);</p> </li> </ul> <p>\end{tikzpicture}</p> <h2 id="quest-ce-que-lencodage-de-position-">Qu’est-ce que l’encodage de position ?</h2> <blockquote> <p>L’encodage de position est nécessaire dans les transformers à cause du calcul de l’attention qui prend en compte tous les azutres tokens de la séquence. Sans encodage positionnel, chaque token identique aurait la même influence, peu importe l’endroit dans la séquence :<br> “Le chat mange la souris” et “La souris mange le chat” seraient équivalents. Avec l’encodage de position, deux tokens identiques mais à des positions différentes auront des vecteurs différents.</p> </blockquote> <ol> <li> <p><strong>Encodage absolu</strong> (sinusoïdal ou appris)<br> Ajouté aux embeddings initiaux. Limité pour extrapoler à des longueurs supérieures à celles vues à l’entraînement.</p> </li> <li> <strong>Encodages relatifs</strong> <ul> <li>T5 : biais appris pour chaque distance relative.</li> <li>Alibi : biais fonctionnel selon la distance.</li> </ul> </li> <li> <strong>RoPE</strong> (Rotary Positional Encoding)<br> rotation appliqué à la représentation intermédiaire de chaque token dépendante de la position du token au niveau du calcul du score d’attention (chaque paire de dimensions du vecteur est tournée dans le plan d’un angle qui dépend de sa position dans la séquence). Mais du coup, quand on applique cette rotation à query et key, et qu’on fait un produit scalaire, le dot product dépend de la position relative $i−j$, même si on encode chaque position de manière absolue. <img src="assets/img/positional_biais/periodic_attn.PNG" alt="RoPE waveform"> </li> </ol> <h1 id="sota-des-approches-pour-corriger-le-biais-positionnel">SOTA des approches pour corriger le biais positionnel</h1> <h2 id="1-modifier-le-mécanisme-dattention">1. Modifier le mécanisme d’attention</h2> <ul> <li> <p><strong>Stable Mask</strong> (<a href="https://arxiv.org/pdf/2402.04779" rel="external nofollow noopener" target="_blank">2402.04779</a>)<br> Ajout d’un “pseudo-score” (-\gamma\times(j-1)) présente une approche de “compensation” du score d’attention excessif sur les premiers tokens en ajoutant un “pseudo-score”: $-\gamma \times (j-1)$ pour la j-ième position, qui diminue au fur et à mesure de la séquence.</p> </li> <li> <p><strong>Calibration du score d’attention</strong> (<a href="https://arxiv.org/pdf/2406.16008" rel="external nofollow noopener" target="_blank">2406.16008</a>) présente une approche dans laquelle on a le score d’attention de la query avec un doc à la position k qui est fonction ($f$) de la pertinence du doc $\text{rel}(\text{doc}<em>k)$ et du biais positionnel à la position k $b_k$. En simplifiant la fonction $f$ (rasoir d’Occam) par une fonction linéaire, on a alors $\text{Attn}(\text{query}, doc_k) = \text{rel}(\text{doc}_k) + b_k + \epsilon$. \textbf{Donc en gros ils supposent que le score d’attention entre le document et la query est une combinaison linéaire du biais de position et de la pertinence réelle du document avec la query}. Pour isoler $\text{rel}(\text{doc}_k)$, ils \textbf{introduisent un document “dummy” à la même position $k$ que le document}: $\text{doc}</em>\text{k;dum}$, on a alors $\text{rel}(\text{doc}<em>k) = \text{Attn}(\text{query}, doc_k) - \text{Attn}(\text{query}, doc</em>{k;dum}) + \epsilon$. Grâce à la pertinence “effective” de chaque document, ils calculent, $\forall k$, un coefficient de rééchelonnement $\alpha_k$, qui est une fonction Softmax appliquée sur le score de pertinence du document $\text{doc}_k$.</p> </li> <li> <strong>Attention bidirectionnelle entre documents</strong> (PCW <a href="https://arxiv.org/pdf/2212.10947" rel="external nofollow noopener" target="_blank">2212.10947</a>) propose une modification de l’attention entre documents pour un traitement “indépendant”: Au lieu d’utiliser l’attention causale (unidirectionnelle) qui impose un ordre strict. A contrario, le papier <a href="https://openreview.net/attachment?id=fvkElsJOsN&amp;name=pdf" rel="external nofollow noopener" target="_blank">l’approche d’attention bidirectionnelle entre documents</a> propose une approche d’attention bidirectionnelle qui permet à chaque document d’interagir équitablement avec tous les autres. <div style="display: flex; justify-content: space-around;"> ![Bidirectional Attn](assets/img/positional_biais/bidirectional_attn.PNG) ![PCW](assets/img/positional_biais/cropped_attn.PNG) </div> </li> <li> <strong>Diminuer le biais positionnel dû à RoPE</strong>: L’objectif principal de <a href="https://arxiv.org/pdf/2104.09864" rel="external nofollow noopener" target="_blank">ROPE</a> est d’encoder l’information positionnelle de sorte que le produit scalaire des embeddings de requête et de clé contienne intrinsèquement l’information relative à la position, c’est-à-dire $f(q_m, m)^T f(k_n, n) = f(q_m, k_n, m - n)$. Ici, $f$ est la fonction d’encodage positionnel appliquée aux embeddings de requête et de clé aux positions $m$ et $n$, respectivement. Pour satisfaire cette condition, la fonction $f$ est définie comme une fonction complexe vectorielle : $f(x, m) = x e^{im\theta} = \left[(x_1 + i x_2)e^{im\theta_1};(x_3 + i x_4)e^{im\theta_2};\ldots;(x_{l-1} + i x_l)e^{im\theta_{l/2}},\right]^T$. Dans cette équation, $l$ représente la dimension des embeddings, (\theta_k = 10000^{-2k/l}), et $i$ est l’unité imaginaire. Pour le calcul du score d’attention, RoPE considère la partie réelle du produit, spécifiquement $\operatorname{Re}\Bigl(f(q_m, m)^T f(k_n, n)\Bigr).$ La fonction trigonométrique $\cos{((m-n) \theta)}$ est périodique, d’où la waveform qu’on obtient. pour certaines valeurs de $(m−n)$, le cosinus (et le sinus) prend des valeurs élevées (les “pics”), tandis que pour d’autres il prend des valeurs faibles (les “creux”). Ainsi, si une information cruciale se trouve à une position qui correspond à un trough de l’oscillation, son score d’attention sera relativement bas. La fréquence des oscillations est modulée par rapport à $\theta_k$: Donc ce qui se fait couremment pour contrebalancer ça, c’est de prendre plusieurs bases de $\theta$, qui fixe l’échelle exponentielle à laquelle les fréquences décroissent.</li> </ul> <p><img src="assets/img/positional_biais/periodic_attn.PNG" alt=""></p> <p>Ainsi, plusieurs approchyes visent à contre-balancer le biais positionnel induit par RoPE:</p> <ul> <li> <p><strong>Attention Bucket</strong> (<a href="https://arxiv.org/pdf/2312.04455" rel="external nofollow noopener" target="_blank">2312.04455</a>)<br> Plusieurs bases (\theta) traitées en parallèle.</p> </li> <li> <p><strong>MS-PoE</strong> (<a href="https://arxiv.org/pdf/2403.04797" rel="external nofollow noopener" target="_blank">2403.04797</a>)<br> C’est une approche qui applique un facteur $r$ à la position des tokens par tête, modifiant du coup l’oscillation par tête $m/r \theta$ afin de garder la base RoPE sur laquelle a été entraîné le modèle (dans une approche de correction du biais à l’inférence)</p> <div style="display: flex; justify-content: space-around;"> ![MS-PoE](assets/img/positional_biais/ms_poe.PNG) ![Attention Bucket](assets/img/positional_biais/attn_bucket.PNG) </div> </li> </ul> <h2 id="2-jouer-sur-les-données">2. Jouer sur les données</h2> <ul> <li> <p><strong>IN2 training</strong> (<a href="https://arxiv.org/pdf/2404.16811" rel="external nofollow noopener" target="_blank">2404.16811</a>)<br> Fine-tuning sur des QA avec contextes longs formés par concaténation aléatoire de segments courts.</p> </li> <li> <p><strong>Réordonnancement des documents</strong> (<a href="https://aclanthology.org/2024.acl-long.91.pdf" rel="external nofollow noopener" target="_blank">ACL-Long’24</a>)<br> Placer les plus pertinents en début ou fin de prompt.</p> </li> </ul> <h1 id="introduction-du-papier--mitigate-positional-biais-via-scaling-a-single-dimension-">Introduction du papier « Mitigate Positional Biais via Scaling a Single Dimension »</h1> <ul> <li>Le biais positionnel provient des patterns d’attention (focus sur début/fin).</li> <li>Causal mask + encodage positionnel génèrent des “dimensions positionnelles” dans les hidden-states.</li> <li> <strong>Objectifs</strong> : <ol> <li>Identifier ces dimensions.</li> <li>Les modifier (scaling) pour diminuer le biais.</li> </ol> </li> </ul> <p><img src="assets/img/positional_biais/pos_hidden_state.PNG" alt="Hidden-state pos dim"></p> <h1 id="tâche-de-retrieval-utilisée">Tâche de retrieval utilisée</h1> <ul> <li> <strong>Clés/valeurs aléatoires</strong> pour isoler le pure retrieval.</li> <li>On mesure<br> [ A_G = \frac{1}{|G|} \sum_{j\in G} a_{l,j} ]<br> où (l)=dernier token (interrogateur), (G)=positions de la clé, (a_{l,j})=poids d’attention.</li> </ul> <p><img src="assets/img/positional_biais/task.PNG" alt="Retrieval task"></p> <h1 id="identification-des-dimensions-positionnelles">Identification des dimensions positionnelles</h1> <ol> <li> <strong>Monotonie</strong> : (h(p)) doit être strictement croissante ou décroissante.</li> <li> <strong>Smoothness</strong> : dérivée seconde faible (évolution “douce”).</li> <li>Choix de la dimension (t) et de l’échelle (s&lt;1) minimisant la perte<br> [ \arg\min_{h_t,s&lt;1} \mathbb{E}\Bigl[\sum_{i=1}^{|P|} \mathcal{L}(x,y,p_i; F(\theta,h_t,s))\Bigr] ]<br> avec (F(\theta,h_t,s)) le modèle scaled sur la (t)-ième dimension.</li> </ol> <p><img src="assets/img/positional_biais/algo.PNG" alt="Algo identification"></p> <h1 id="construction-de-hp_i">Construction de (h(p_i))</h1> <ul> <li>Approximation par <strong>moindres carrés segmentés</strong> pour lisser le signal.</li> <li>Hypothèse : (\textbf{hidden_state}_i = h(p_i) + \epsilon_i).</li> <li>Permet de distinguer tendance monotone et bruit.</li> </ul> <h1 id="résultats-de-lidentification">Résultats de l’identification</h1> <ul> <li>Tendance monotone dès la couche 1, s’amplifie dans les couches supérieures.<br> <img src="assets/img/positional_biais/example_mistral.PNG" alt="Mistral example"> </li> </ul> <h1 id="dimensions-causées-par-le-masque-causal">Dimensions causées par le masque causal</h1> <ul> <li>Réduction du PE de 200 pour positions 400–600 : effet mineur.</li> <li>Rogner le masque causal pour ces positions : fortes fluctuations.<br> <img src="assets/img/positional_biais/causal_mask_modification_perturb_pos_dim.PNG" alt="Causal mask perturb"> </li> </ul> <h1 id="correction-proposée">Correction proposée</h1> <ul> <li> <p>Prouvé que la performance biasée vient des patterns d’attention : en doublant le score à la position 25, on déplace l’attention.<br> <img src="assets/img/positional_biais/attn_weights_distrib.PNG" alt="Attn distribution"></p> </li> <li> <p><strong>Modification</strong> : scaler la dimension (p) uniquement pour le calcul du score d’attention du dernier token (l) : [ z = \begin{cases} \mathrm{Softmax}\bigl((q_i K^\top + \mathrm{Mask})/\sqrt d\bigr)V, &amp; i&lt;l,\[0.5em] \mathrm{Softmax}\bigl((\bar q_l\;\bar K^\top)/\sqrt d\bigr)V, &amp; i=l. \end{cases} ]</p> </li> </ul> <h1 id="effet-du-scaling">Effet du scaling</h1> <ul> <li>(s&gt;1) → focus début ; (s&lt;0) → focus fin.</li> <li>(s\in[-1,0.5]) → distribution équilibrée.</li> </ul> <div style="display: flex; justify-content: space-around;"> ![Scaling hidden](assets/img/positional_biais/scaling_pos_hidden_states.PNG) ![Scaling factor](assets/img/positional_biais/scaling_factor.PNG) </div> <ul> <li> <strong>Varie selon les modèles</strong> :<br> <img src="assets/img/positional_biais/examples_scaling_factors.PNG" alt="Scaling examples"> </li> </ul> <h1 id="performances">Performances</h1> <p>Gain significatif sur LongBench :<br> <img src="assets/img/positional_biais/longbench_perf.PNG" alt="LongBench perf"></p> <h1 id="limitations-selon-moi">Limitations (selon moi)</h1> <ol> <li>Hypothèses de monotonie et smoothness non prouvées.</li> <li>Lien causal masque→hidden-states pas rigoureusement démontré.</li> <li>Scaling simple, approche très empirique.</li> <li>Ne corrige que le biais dû au masque causal.</li> </ol> <h1 id="points-forts-du-papier">Points forts du papier</h1> <ul> <li>Séparation mécanique sémantique vs. position.</li> <li>Unique à cibler le biais du masque causal.</li> <li>Identification originale des dimensions.</li> <li>État de l’art clair.</li> </ul> <h1 id="pour-aller-plus-loin--découpler-sémantique-et-position">Pour aller plus loin : découpler sémantique et position</h1> <h2 id="spline-based-transformers-eccv25">Spline-based Transformers (ECCV’25)</h2> <ul> <li>Suppression de l’encodage positionnel.</li> <li>Introduction de <em>tokens de contrôle</em> formant une spline :<br> [ s(t)=\sum_{i=0}^n N_{i,k}(t)\,\mathbf p_i ]</li> <li>Position implicite via la géométrie.<br> <img src="assets/img/positional_biais/spline_based_transformers.PNG" alt="Spline Transformers"> </li> </ul> <h2 id="decomposed-positional-vectors-neurips">Decomposed Positional Vectors (NeurIPS)</h2> <ul> <li>Décomposition : (h_{l,t}=p_{l,t}+cs_{l,t}).</li> <li>Estimation : (p_{l,t}=\tfrac1N\sum_{s=1}^N h^{(s)}_{l,t}).</li> <li>Isolation : (cs_{l,t}=h_{l,t}-p_{l,t}).</li> </ul> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/explainability_llm_generation/">Les approches d'explicabilité par les exemples appliquées aux LLMs</a> </li> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'camillebrl/camillebrl.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Camille Barboule. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://tikzjax.com/v1/tikzjax.js" integrity="sha256-+1qyucCXRZJrCg3lm3KxRt/7WXaYhBid4/1XJRHGB1E=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/lightbox2@2.11.5/dist/js/lightbox.min.js" integrity="sha256-A6jI5V9s1JznkWwsBaRK8kSeXLgIqQfxfnvdDOZEURY=" crossorigin="anonymous"></script> <script defer src="/assets/js/photoswipe-setup.js" type="module"></script> <script defer src="https://cdn.jsdelivr.net/npm/spotlight.js@0.7.8/dist/spotlight.bundle.min.js" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/venobox@2.1.8/dist/venobox.min.js" integrity="sha256-LsGXHsHMMmTcz3KqTaWvLv6ome+7pRiic2LPnzTfiSo=" crossorigin="anonymous"></script> <script defer src="/assets/js/venobox-setup.js?897c1d9c0b6fcf82b949511c1609d055" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>