<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Tips mathématiques / points utiles en IA | Camille Barboule </title> <meta name="author" content="Camille Barboule"> <meta name="description" content="Tips mathématiques / points utiles en IA"> <meta name="keywords" content="NLP, IR, Agents, Deep-Learning, XAI"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%93%9A&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://camillebrl.github.io/blog/2025/tips_mathematics_for_ai/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightbox2@2.11.5/dist/css/lightbox.min.css" integrity="sha256-uypRbsAiJcFInM/ndyI/JHpzNe6DtUNXaWEUWEPfMGo=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@5.4.4/dist/photoswipe.min.css" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/spotlight.js@0.7.8/dist/css/spotlight.min.css" integrity="sha256-Dsvkx8BU8ntk9Iv+4sCkgHRynYSQQFP6gJfBN5STFLY=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/venobox@2.1.8/dist/venobox.min.css" integrity="sha256-ohJEB0/WsBOdBD+gQO/MGfyJSbTUI8OOLbQGdkxD6Cg=" crossorigin="anonymous"> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <style>.toc-list{list-style:none;padding-left:0}.toc-list ul{list-style:none;padding-left:1.5em}.toc-list li{margin:.5em 0}.toc-list a{text-decoration:none;color:inherit}.toc-list a:hover{text-decoration:underline}d-article>*:first-child{margin-top:0!important}d-contents+h1,d-contents+h2,d-contents+h3,d-contents+h4,d-contents+h5,d-contents+h6,d-contents+p{margin-top:2rem!important}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Tips mathématiques / points utiles en IA",
            "description": "Tips mathématiques / points utiles en IA",
            "published": "July 25, 2025",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Camille</span> Barboule </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Tips mathématiques / points utiles en IA</h1> <p>Tips mathématiques / points utiles en IA</p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div id="toc-container" class="toc-list"> </div> </nav> </d-contents> <h1 id="introduction">Introduction</h1> <blockquote> <p>Dans ce post, je vais mettre pas mal de <mark>rappels mathématiques</mark> (je reviens aux bases des bases, mais j’ai encore besoin de revenir à ces bases des choses pour comprendre certains concepts…) <mark>qui m'ont été utiles pour comprendre certains papiers de recherche ou débloquer certains points dans mes recherches</mark>.</p> </blockquote> <h1 id="entropie-information-mutuelle-cross-entropie">Entropie, information mutuelle, cross-entropie</h1> <ul> <li>L’entropie quantifie l’incertitude moyenne d’une variable aléatoire, ie la quantité d’information qu’elle contient: $H(X)=\mathbb{E}[-\log p(X)]$ (discret) et $h(X)=-\int f\log f$ (continu). Elle est maximale pour l’uniforme, nulle pour une variable déterministe ; le conditionnement ne l’augmente pas. Elle est au cœur du codage, de l’inférence statistique (cross-entropie, KL) et de l’information mutuelle. En continu, elle n’est pas invariante par changement d’échelle ; préférer souvent $I$, $D_{\mathrm{KL}}$ pour des comparaisons robustes.</li> <li>La cross-entropie entre une “vraie” loi $P$ et un modèle $Q$ est définie par $H(P,Q)=-\sum_x P(x)\log Q(x)$ (ou $\int P\log Q$ au continu). Elle mesure le coût moyen pour coder des échantillons tirés de $P$ avec un code optimal pour $Q$, et vérifie l’identité clé $H(P,Q)=H(P)+D_{\mathrm{KL}}(P\Vert Q)\ge H(P)$, avec minimum atteint quand $Q=P$. En apprentissage supervisé, la NLL (negative log-likelihood) utilisée pour la classification est exactement une cross-entropie entre les étiquettes (one-hot) et les probabilités du modèle (softmax).</li> <li>L’information mutuelle quantifie la dépendance entre deux variables : $I(X;Y)=H(X)-H(X\mid Y)=H(Y)-H(Y\mid X)=H(X)+H(Y)-H(X,Y)$. Elle s’écrit aussi comme une divergence de KL entre la conjointe et le produit des marginales : $I(X;Y)=D_{\mathrm{KL}}!\big(P_{XY}\Vert P_XP_Y\big)\ge 0$. Elle est nulle <strong>ssi</strong> $X$ et $Y$ sont indépendantes, est <strong>symétrique</strong>, et obéit à l’inégalité de traitement de l’information (si $X!\to!Y!\to!Z$, alors $I(X;Z)\le I(X;Y)$).</li> <li> <table> <tbody> <tr> <td>La Divergence de Kullback–Leibler (KL) est une métrique de “distance” directionnelle entre distributions non symétriques. Cette métrique permet de relier information mutuelle et cross-entropie ($\mathrm{D_{KL}}(P</td> <td>Q)=\sum_x P(x)\log\frac{P(x)}{Q(x)}\ \ (\ge 0)$ ; $=0$ ssi $P=Q$)</td> </tr> </tbody> </table> </li> </ul> <h2 id="lentropie">L’entropie</h2> <p>L’entropie mesure l’<strong>incertitude</strong> (ou le <strong>désordre</strong>) d’une variable aléatoire. Plus une variable est imprévisible, plus son entropie est grande. On peut aussi la voir comme la quantité moyenne d’<strong>information</strong> (ou de <strong>surprise</strong>) révélée par l’observation de la variable.</p> <h3 id="dans-le-cas-discret">Dans le cas discret</h3> <p>Soit $X$ une variable aléatoire discrète à valeurs dans un ensemble fini $\mathcal{X}$, de loi $p(x)=\Pr[X=x]$. Pour une base de logarithme fixée (base $2$ pour les <em>bits</em>, base $e$ pour les <em>nats</em>), l’entropie de $X$ est \(H(X) \;=\; - \sum_{x\in\mathcal{X}} p(x)\,\log p(x) \;=\; \mathbb{E}\!\left[-\log p(X)\right].\) Le terme $-\log p(x)$ est l’<strong>information de Shannon</strong> (ou <strong>surprise</strong>) de l’issue $x$.</p> <p><strong>Unités.</strong> Avec $\log_2$, $H(X)$ est en <strong>bits</strong> ; avec $\log$, en <strong>nats</strong> ; avec $\log_{10}$, en <strong>hartleys</strong>.</p> <h4 id="propriétés">Propriétés</h4> <ul> <li> <strong>Bornes.</strong> $0 \le H(X) \le \log |\mathcal{X}|$.<br> $H(X)=0$ ssi $X$ est déterministe (toute la masse sur une seule valeur).<br> $H(X)$ est maximale et vaut $\log|\mathcal{X}|$ si $X$ est <strong>uniforme</strong> sur $\mathcal{X}$.</li> <li> <strong>Concavité.</strong> $H(\cdot)$ est concave en la loi $p$ de $X$.</li> <li> <strong>Transformation déterministe.</strong> Pour toute fonction déterministe $g$, $H(g(X)) \le H(X)$, avec égalité si $g$ est bijective sur le support.</li> <li> <strong>Sous-additivité et indépendance.</strong> Pour $(X,Y)$ discrets, \(H(X,Y) = H(X\mid Y) + H(Y) = H(Y\mid X) + H(X).\) Si $X \perp Y$ (indépendants), alors $H(X,Y)=H(X)+H(Y)$.</li> <li> <strong>Le conditionnement réduit l’entropie.</strong> $H(X\mid Y) \le H(X)$, avec égalité ssi $X$ et $Y$ sont indépendants.</li> </ul> <h4 id="exemples">Exemples</h4> <p><strong>Bernoulli.</strong> Si $X\sim\mathrm{Bernoulli}(p)$, \(H(X) = -\,p\log p - (1-p)\log(1-p),\) maximisée en $p=\tfrac12$ (vaut $1$ bit en base $2$) et nulle en $p\in{0,1}$.</p> <p><strong>Uniforme.</strong> Si $X$ est uniforme sur $n$ symboles, $H(X)=\log n$.</p> <h3 id="dans-le-cas-continu-entropie-différentielle">Dans le cas continu: entropie différentielle</h3> <p>Si $X$ est réelle (densité $f$), l’<strong>entropie différentielle</strong> est \(h(X) \;=\; - \int f(x)\,\log f(x)\,dx.\)</p> <p><strong>Attention :</strong> $h(X)$ peut être <strong>négative</strong> et <strong>n’est pas invariante par changement d’échelle</strong> : \(h(aX) = h(X) + \log|a| \quad (a\neq 0).\) En revanche, des quantités dérivées comme l’information mutuelle et la divergence KL restent bien définies et invariantes de coordonnées.</p> <p><strong>Exemple gaussien.</strong> Si $X\sim\mathcal{N}(\mu,\sigma^2)$, \(h(X) = \tfrac12 \log\!\big(2\pi e\,\sigma^2\big).\) En dimension $d$, pour $X\sim\mathcal{N}(\mu,\Sigma)$, \(h(X) = \tfrac12 \log\!\big((2\pi e)^d \det \Sigma\big).\)</p> <h3 id="estimer-lentropie">Estimer l’entropie</h3> <ul> <li> <strong>Estimateur plug-in (discret).</strong> Remplacer $p(x)$ par la fréquence empirique $\hat{p}(x)$ : \(\widehat{H(X)} = -\sum_x \hat{p}(x)\log \hat{p}(x)\) (biais négatif pour petits échantillons ; corrections type Miller–Madow existent).</li> <li> <strong>Continu.</strong> Estimateurs à noyau, $k$-plus proches voisins (Kozachenko–Leonenko), ou via modèles paramétriques/neuraux (flows, VAEs) pour approcher $f$ ou $D_{\mathrm{KL}}$.</li> </ul> <h2 id="principe-du-maximum-dentropie">Principe du maximum d’entropie</h2> <p>Parmi toutes les lois satisfaisant des contraintes (p. ex. support, moyenne, variance), la loi à entropie maximale est la moins informative (au sens de Shannon) :</p> <ul> <li>Support fini $\Rightarrow$ uniforme.</li> <li>Support $[0,\infty)$ et contrainte de moyenne $\Rightarrow$ exponentielle.</li> <li>Contrainte de moyenne et variance en $\mathbb{R}$ $\Rightarrow$ gaussienne.</li> </ul> <h2 id="lentropie-conditionnelle-et-linformation-mutuelle">L’Entropie conditionnelle et l’information mutuelle</h2> <p>L’entropie conditionnelle de $X$ sachant $Y$ est \(H(X\mid Y) \;=\; \mathbb{E}_Y\big[H(X\mid Y=y)\big] \;=\; -\,\sum_{x,y} p(x,y)\,\log p(x\mid y).\) L’<strong>information mutuelle</strong> mesure la réduction d’incertitude sur $X$ due à la connaissance de $Y$ : \(I(X;Y) \;=\; H(X) - H(X\mid Y) \;=\; H(Y) - H(Y\mid X) \;=\; H(X)+H(Y)-H(X,Y) \;\ge 0.\) <strong>Inégalité de traitement de l’information (data processing).</strong> Si $X \to Y \to Z$ forme une chaîne de Markov, alors $I(X;Z)\le I(X;Y)$.</p> <h2 id="la-cross-entropie-et-la-divergence-de-kullbackleibler">La Cross-entropie et la divergence de Kullback–Leibler</h2> <p>Pour deux lois $P$ et $Q$ sur le même alphabet,</p> <ul> <li> <strong>Cross-entropie :</strong> $H(P,Q) = -\sum_x P(x)\log Q(x)$,</li> <li> <strong>Divergence KL :</strong> $D_{\mathrm{KL}}(P\Vert Q) = \sum_x P(x)\log\frac{P(x)}{Q(x)} \ge 0$,</li> </ul> <p>et l’identité clé : \(H(P,Q) = H(P) + D_{\mathrm{KL}}(P\Vert Q).\) En apprentissage, minimiser la log-vraisemblance négative revient souvent à minimiser une cross-entropie (donc une KL) entre la vraie loi $P$ et le modèle $Q_\theta$.</p> <h2 id="lien-avec-le-codage-théorème-source-de-shannon">Lien avec le codage (théorème source de Shannon)</h2> <p>Pour une source i.i.d. $X$, la longueur moyenne minimale $\bar{\ell}^\star$ d’un code préfixe vérifie \(H(X) \;\le\; \bar{\ell}^\star \;&lt;\; H(X) + 1 \quad \text{(en bits/symbole)}.\) L’entropie est donc une borne fondamentale sur le <strong>taux de compression</strong>.</p> <h1 id="svd-acp">SVD, ACP</h1> <ul> <li>Données : matrice centrée $X \in \mathbb{R}^{n \times d}$, avec $n$ observations (lignes) et $d$ variables (colonnes).<br> On centre toujours : $X = \tilde{X} - \mathbf{1}\mu^\top$, où $\mu \in \mathbb{R}^{d}$ est la moyenne colonne et $\mathbf{1}$ le vecteur de 1.</li> <li>Covariance empirique : $\displaystyle S = \frac{1}{n-1} X^\top X \in \mathbb{R}^{d \times d}$.</li> </ul> <h2 id="svd--décomposition-en-valeurs-singulières">SVD : décomposition en valeurs singulières</h2> <p>La décomposition en valeurs singulières (SVD) factorise toute matrice réelle $X \in \mathbb{R}^{n\times d}$ en $X = U\Sigma V^\top$, où $U \in \mathbb{R}^{n\times r}$ et $V \in \mathbb{R}^{d\times r}$ ont des colonnes orthonormées (vecteurs singuliers gauche et droit), $\Sigma = \mathrm{diag}(\sigma_1 \ge \cdots \ge \sigma_r &gt; 0)$ contient les valeurs singulières, et $r = \mathrm{rang}(X)$. On a $X^\top X = V\Sigma^2 V^\top$ et $X X^\top = U\Sigma^2 U^\top$, d’où le lien avec les décompositions spectrales. La SVD fournit la <strong>meilleure approximation de rang $k$</strong> au sens de la norme de Frobenius : $X_k = U_k \Sigma_k V_k^\top$ minimise $|X - Y|<em>F$ sur toutes les matrices $Y$ de rang $\le k$ (théorème d’Eckart–Young–Mirsky), avec erreur $|X - X_k|_F^2 = \sum</em>{i&gt;k} \sigma_i^2$. Elle est utilisée pour le débruitage, la compression et le calcul de l’ACP (où $V$ donne les chargements et $\sigma_i^2/(n-1)$ les variances expliquées), et reste numé</p> <p>La SVD (Singular Value Decomposition) de $X$ est : \(X = U \Sigma V^\top,\quad U \in \mathbb{R}^{n \times r},\; V \in \mathbb{R}^{d \times r},\; \Sigma=\mathrm{diag}(\sigma_1,\dots,\sigma_r),\; r=\mathrm{rang}(X).\)</p> <ul> <li>$U$ : vecteurs singuliers à gauche (orthonormés),</li> <li>$V$ : vecteurs singuliers à droite (orthonormés),</li> <li>$\sigma_1 \ge \sigma_2 \ge \cdots \ge \sigma_r &gt; 0$ : valeurs singulières.</li> </ul> <h3 id="lien-avec-la-covariance">Lien avec la covariance</h3> <p>\(X^\top X = V \Sigma^2 V^\top \quad\Rightarrow\quad S = \frac{1}{n-1} X^\top X = V \left(\frac{\Sigma^2}{\,n-1\,}\right) V^\top.\) Donc :</p> <ul> <li> <strong>Vecteurs propres de $S$</strong> $=$ <strong>vecteurs singuliers droits</strong> $V$.</li> <li> <strong>Valeurs propres</strong> $\lambda_k = \sigma_k^2/(n-1)$.</li> </ul> <h2 id="lacp">L’ACP</h2> <p>L’Analyse en Composantes Principales (ACP) est une méthode linéaire de réduction de dimension qui projette des données centrées $X$ sur des directions orthogonales — les <strong>composantes principales</strong> — choisies pour maximiser la variance expliquée. On diagonalise la matrice de covariance $S=\tfrac{1}{n-1}X^\top X$ (ou on calcule la SVD $X=U\Sigma V^\top$) : les vecteurs propres de $S$ (colonnes de $V$) sont les <strong>chargements</strong>, et les <strong>scores</strong> sont $T=XV=U\Sigma$. En ne conservant que $k$ composantes associées aux plus grandes valeurs propres $\lambda_i=\sigma_i^2/(n-1)$, on obtient une représentation à $k$ dimensions qui minimise l’erreur quadratique de reconstruction (théorème d’Eckart–Young) avec une <strong>variance expliquée</strong> cumulée $\sum_{i=1}^k \lambda_i \big/ \sum_j \lambda_j$. L’ACP sert à visualiser, compresser et débruiter des données et se pratique souvent après <strong>standardisation</strong> des variables si leurs échelles diffèrent fortement.</p> <p>Il y a 3 façons de voir l’ACP:</p> <h3 id="1-maximisation-de-variance-optimisation-quadratique">1) Maximisation de variance (optimisation quadratique)</h3> <p>La première composante principale est la direction unitaire $v_1 \in \mathbb{R}^{d}$ qui maximise la variance projetée : \(v_1 \;=\; \arg\max_{\|v\|_2=1}\; \mathrm{Var}(X v) \;=\; \arg\max_{\|v\|_2=1}\; v^\top S\, v.\) Par multiplicateurs de Lagrange, on obtient l’équation aux valeurs propres : \(S v_1 \;=\; \lambda_1 v_1,\quad \lambda_1=\max \text{ eigenvalue}.\) Les composantes suivantes $v_k$ se construisent de manière itérative sous contrainte d’orthogonalité $(v_k^\top v_j=0, j&lt;k)$, donnant les $d$ vecteurs propres de $S$ rangés par $\lambda_1 \ge \lambda_2 \ge \cdots \ge \lambda_d \ge 0$.</p> <p><strong>Scores (coordonnées factorielles)</strong> : $t_k = X v_k \in \mathbb{R}^{n}$.<br> <strong>Chargements</strong> : colonnes de $V=[v_1,\dots,v_d]$.</p> <h3 id="2-meilleure-approximation-de-rang-k-moindres-carrés">2) Meilleure approximation de rang $k$ (moindres carrés)</h3> <p>L’ACP de rang $k$ cherche une approximation de $X$ par une matrice de rang $k$ : \(\min_{\substack{W \in \mathbb{R}^{d \times k},\, H \in \mathbb{R}^{n \times k}\\ W^\top W = I_k}} \; \|X - H W^\top\|_F^2.\) La solution optimale est obtenue en prenant $W=V_k$ (les $k$ premiers vecteurs propres de $S$) et $H = X V_k$.<br> Cette vue mène au théorème d’Eckart–Young–Mirsky (voir SVD ci-dessous).</p> <h3 id="3-diagonalisation-de-la-covariance-vue-statistique">3) Diagonalisation de la covariance (vue statistique)</h3> <p>En décomposant $S$ : \(S = V \Lambda V^\top,\quad \Lambda=\mathrm{diag}(\lambda_1,\dots,\lambda_d),\) la projection $Z = X V$ a covariance diagonale : \(\frac{1}{n-1} Z^\top Z = \Lambda,\) c’est-à-dire des <strong>composantes non corrélées</strong> dont les variances expliquées sont $\lambda_k$.</p> <h3 id="choix-du-nombre-de-composantes-k">Choix du nombre de composantes $k$</h3> <ul> <li> <strong>Cumul de variance expliquée</strong> : choisir le plus petit $k$ tel que $\sum_{i=1}^k \sigma_i^2 / \sum_{j=1}^r \sigma_j^2 \ge \tau$ (p.ex. $\tau=0{,}90$ ou $0{,}95$).</li> <li> <strong>Coude</strong> (scree plot) sur $\sigma_i^2$.</li> <li> <strong>Validation croisée</strong> sur l’erreur de reconstruction ou la performance aval.</li> </ul> <h3 id="reconstruction--projection">Reconstruction &amp; projection</h3> <ul> <li> <strong>Projection (scores)</strong> : $T_k = X V_k$.</li> <li> <strong>Reconstruction</strong> : $\widehat{X} = T_k V_k^\top = U_k \Sigma_k V_k^\top$.</li> <li> <strong>Hors échantillon</strong> : pour un nouveau $x$, centrer $x_c=x-\mu$, puis $t = x_c^\top V_k$ et $\hat{x}= \mu + V_k t$.</li> </ul> <h3 id="centrage-standardisation-et-acp-sur-corrélations">Centrage, standardisation et ACP sur corrélations</h3> <ul> <li> <strong>Centrage indispensable</strong> : sinon la 1ʳᵉ CP peut simplement « pointer » la moyenne.</li> <li> <strong>Standardisation</strong> (PCA sur la matrice de corrélation) si les échelles des variables diffèrent fortement : travailler sur \(X_{\text{std}} = (X - \mathbf{1}\mu^\top) D^{-1},\) où $D$ contient les écarts-types des colonnes, puis appliquer ACP/SVD à $X_{\text{std}}$.</li> </ul> <h3 id="blanchiment-whitening">Blanchiment (whitening)</h3> <p>L’ACP fournit une base orthonormée où les variances sont $\lambda_k$. Le <strong>whitening</strong> met ces variances à 1 : \(X_{\text{white}} = X V \Lambda^{-1/2} = X V \left(\frac{\Sigma}{\sqrt{n-1}}\right)^{-1} = \sqrt{n-1}\, U.\) Ainsi, $\frac{1}{n-1} X_{\text{white}}^\top X_{\text{white}} = I$.</p> <h3 id="propriétés-et-détails-numériques">Propriétés et détails numériques</h3> <ul> <li> <strong>Rang</strong> : $\mathrm{rang}(X) \le \min(n-1,d)$ (le centrage retire au plus un degré de liberté).</li> <li> <strong>Stabilité</strong> : calculer l’ACP via la SVD de $X$ est numériquement plus stable que l’EVD de $S$.</li> <li> <strong>Cas $n \ll d$ ou $d \ll n$</strong> : utiliser la SVD tronquée (méthodes itératives, p.ex. Lanczos/power method).</li> <li> <strong>Indétermination de signe</strong> : $v_k$ et $-v_k$ sont équivalents (mêmes sous-espaces).</li> </ul> <h3 id="acp-noyau-kernel-pca--en-bref">ACP noyau (kernel PCA) — en bref</h3> <p>Remplace la projection linéaire par un <strong>noyau</strong> $K$ (centré en RKHS) et diagonalise $K$ (taille $n\times n$) pour capturer des <strong>variations non linéaires</strong>. Les scores sont obtenus via combinaisons des noyaux, sans construire explicitement la base de caractéristiques.</p> <h2 id="lien-acp--svd-formules-utiles">Lien ACP ↔ SVD (formules utiles)</h2> <p>Si $X$ est <strong>centrée</strong> :</p> <ul> <li> <strong>Chargements ACP</strong> : $V = [v_1,\dots,v_r]$ (vecteurs singuliers droits).</li> <li> <strong>Scores ACP</strong> : $T = X V = U \Sigma$ (chaque colonne $t_k = \sigma_k\, u_k$).</li> <li> <strong>Variance expliquée</strong> : \(\text{EVR}_k = \frac{\lambda_k}{\sum_{j=1}^r \lambda_j} = \frac{\sigma_k^2}{\sum_{j=1}^r \sigma_j^2}.\)</li> <li> <strong>Approximation de rang $k$</strong> (Eckart–Young–Mirsky, meilleure au sens $|\cdot|_F$) : \(X_k = U_k \Sigma_k V_k^\top \quad\text{et}\quad \|X - X_k\|_F^2 = \sum_{i=k+1}^{r} \sigma_i^2.\)</li> </ul> <h1 id="rang-dune-matrice-permet-de-déterminer-si-un-espace-est-sur--sous-dimensionné">Rang d’une matrice: permet de déterminer si un espace est sur / sous dimensionné</h1> <p>Soit $A \in \mathbb{R}^{n \times d}$ et $r=\mathrm{rang}(A)$.</p> <ul> <li> <strong>Espace image (colonnes)</strong> : $ \mathrm{Im}(A)={A x : x\in\mathbb{R}^d} \subseteq \mathbb{R}^n$. \(\boxed{\;\mathrm{rang}(A)=\dim(\mathrm{Im}(A))=\dim(\mathrm{span}\{\text{colonnes de }A\})\;}\)</li> <li> <strong>Rang des lignes</strong> : $ \mathrm{rang}(A)=\dim(\mathrm{span}{\text{lignes de }A})$ (rang des colonnes = rang des lignes).</li> <li> <strong>Mineurs</strong> : $ \mathrm{rang}(A)$ est l’ordre maximal $k$ tel qu’il existe un mineur $k\times k$ de déterminant non nul.</li> <li> <strong>SVD</strong> : si $A=U\Sigma V^\top$ avec $\Sigma=\mathrm{diag}(\sigma_1\ge\cdots\ge\sigma_r&gt;0)$, \(\boxed{\;\mathrm{rang}(A)=\#\{i:\sigma_i&gt;0\}\;}\)</li> </ul> <p>Bornes : $\;0 \le r \le \min(n,d)$.</p> <h2 id="théorème-du-rang-rang-noyau">Théorème du rang (rang-noyau)</h2> <p>Le <strong>noyau</strong> (espace des solutions homogènes) est $\ker(A)={x\in\mathbb{R}^d: Ax=0}$. \(\boxed{\; \dim(\ker(A)) + \mathrm{rang}(A) = d \;}\)</p> <ul> <li>$\dim(\ker(A)) = d-r$ s’appelle parfois la <strong>nullité</strong>.</li> <li>Interprétation : chaque dépendance linéaire entre colonnes ajoute une dimension au noyau.</li> </ul> <h2 id="indépendance-linéaire-et-base">Indépendance linéaire et base</h2> <p>Pour des vecteurs $a_1,\dots,a_m \in \mathbb{R}^d$, posons $A=[a_1~\cdots~a_m]\in\mathbb{R}^{d\times m}$.</p> <ul> <li>$ \mathrm{rang}(A) = $ <strong>nombre maximal de vecteurs linéairement indépendants</strong> parmi $a_1,\dots,a_m$.</li> <li>Si $ \mathrm{rang}(A)=m\le d$, la famille est libre.</li> <li>Si $ \mathrm{rang}(A)&lt;m$, la famille est redondante (sur-paramétrée) pour décrire son sous-espace vectoriel.</li> </ul> <h2 id="systèmes-linéaires-axb-et-sursous-détermination">Systèmes linéaires $Ax=b$ et (sur/sous)-détermination</h2> <ul> <li> <strong>Existence (compatibilité)</strong> : le système $Ax=b$ a une solution ssi \(\mathrm{rang}(A) = \mathrm{rang}([A~|~b]).\)</li> <li> <strong>Unicité</strong> : si une solution existe, elle est <strong>unique</strong> ssi $\ker(A)={0}$, i.e. $\mathrm{rang}(A)=d$ (colonnes libres).</li> </ul> <h3 id="cas-selon-la-forme-de-a-avec-ainmathbbrntimes-d">Cas selon la forme de $A$ (avec $A\in\mathbb{R}^{n\times d}$)</h3> <ol> <li> <strong>Sur-déterminé (plus d’équations que d’inconnues)</strong> : $n&gt;d$. <ul> <li>Si $ \mathrm{rang}(A)=d$ (<strong>plein rang colonne</strong>), l’égalité exacte $Ax=b$ n’est pas garantie, mais la <strong>moindre carrés</strong> a solution unique : \(x^\star = (A^\top A)^{-1}A^\top b.\)</li> <li>Si $ \mathrm{rang}(A)&lt;d$, colonnes dépendantes $\Rightarrow$ solution LS <strong>non unique</strong> (ill-posée).</li> </ul> </li> <li> <strong>Sous-déterminé (plus d’inconnues que d’équations)</strong> : $d&gt;n$. <ul> <li>Si $ \mathrm{rang}(A)=n$ (<strong>plein rang ligne</strong>), il y a soit <strong>infinité de solutions</strong> (si compatible), la solution <strong>de norme minimale</strong> est \(x^\star = A^\top (AA^\top)^{-1} b.\)</li> <li>Si $ \mathrm{rang}(A)&lt;n$, encore plus de liberté (noyau plus grand).</li> </ul> </li> <li> <strong>Carré</strong> : $n=d$. <ul> <li>$ \mathrm{rang}(A)=n$ $\Leftrightarrow$ $A$ <strong>inversible</strong>, $\det(A)\neq 0$, $\sigma_i&gt;0$ $\forall i$.</li> <li>Sinon, $A$ est <strong>singulière</strong> : pas d’unicité (noyau non trivial) et potentielle incompatibilité.</li> </ul> </li> </ol> <h2 id="diagnostic--sursous-dimensionné--via-le-rang">Diagnostic « sur/sous-dimensionné » via le rang</h2> <h3 id="familles-de-vecteurs-dans-mathbbrd">Familles de vecteurs dans $\mathbb{R}^d$</h3> <ul> <li>Vous avez $m$ vecteurs. <ul> <li>Si $\mathrm{rang}(A)=m&lt;d$ : la famille est <strong>sous-dimensionnée pour $\mathbb{R}^d$</strong> (elle ne peut pas engendrer tout $\mathbb{R}^d$), mais <strong>bien dimensionnée</strong> pour son sous-espace de dimension $m$.</li> <li>Si $\mathrm{rang}(A)&lt;m$ : la famille est <strong>sur-dimensionnée</strong> pour son sous-espace (des vecteurs redondants).</li> <li>Pour obtenir une <strong>base</strong> de $\mathbb{R}^d$, il faut et il suffit d’avoir $\mathrm{rang}(A)=d$ avec $m\ge d$; une base minimale a $m=d$.</li> </ul> </li> </ul> <h3 id="données-n-times-d-et-colinéarités">Données $(n \times d)$ et colinéarités</h3> <p>Considérez une matrice de données $X\in\mathbb{R}^{n\times d}$ (lignes = observations, colonnes = variables).</p> <ul> <li>Si $\mathrm{rang}(X)=d$ et $n\ge d$ : les variables sont <strong>non redondantes</strong> (pas de colinéarité parfaite).</li> <li>Si $\mathrm{rang}(X)&lt;d$ : variables <strong>redondantes</strong> $\Rightarrow$ espace de caractéristiques <strong>sur-dimensionné</strong> par rapport à l’information disponible; on peut réduire la dimension (p.ex. ACP/SVD).</li> <li>Si $d \gg n$ : même avec $\mathrm{rang}(X)=n$, l’espace des variables est <strong>sous-contraint</strong> par les données (risque de sur-apprentissage) ; régularisation ou réduction de dimension sont recommandées.</li> </ul> <h2 id="outils-de-calcul-et-liens-utiles">Outils de calcul et liens utiles</h2> <ul> <li> <strong>RREF / pivots</strong> : le nombre de pivots (après élimination de Gauss) = $ \mathrm{rang}(A)$.</li> <li> <strong>SVD</strong> : $\mathrm{rang}(A)=#{ \sigma_i &gt; 0}$.</li> <li> <strong>Pseudo-inverse de Moore–Penrose</strong> $A^+$ (via SVD) : \(A^+ = V\,\Sigma^+\,U^\top,\quad \Sigma^+=\mathrm{diag}\Big(\tfrac{1}{\sigma_i}\mathbf{1}_{\sigma_i&gt;0}\Big),\) donne les solutions LS minimales en norme : $x^\star=A^+ b$.</li> </ul> <h1 id="information-de-fisher-et-bornes-de-cramer-rao">Information de Fisher et Bornes de Cramer-Rao</h1> <p>L’information de Fisher d’un modèle paramétrique $p_\theta(y)$ est la matrice $I(\theta)=\mathbb{E}<em>{y\sim p</em>\theta}!\big[\nabla_\theta \log p_\theta(y)\,\nabla_\theta \log p_\theta(y)^\top\big]=-\mathbb{E}<em>{y\sim p</em>\theta}!\big[\nabla_\theta^2 \log p_\theta(y)\big]$ (sous conditions de régularité) ; elle quantifie la quantité d’information sur $\theta$ contenue dans une observation et mesure la courbure moyenne de la log-vraisemblance. Pour $n$ échantillons i.i.d., $I_n(\theta)=n\,I(\theta)$, ce qui induit la borne de Cramér–Rao $\mathrm{Cov}(\hat\theta)\succeq I_n(\theta)^{-1}$ pour tout estimateur sans biais et l’optimalité asymptotique du MLE : $\sqrt{n}(\hat\theta-\theta^\star)\Rightarrow \mathcal{N}!\big(0,\,I(\theta^\star)^{-1}\big)$. On distingue la <strong>Fisher observée</strong> $-\nabla_\theta^2 \log L(\theta)$ évaluée en $\hat\theta$ (avec $L$ la vraisemblance) et la <strong>Fisher attendue</strong> (son espérance). En apprentissage, on minimise la NLL $\mathcal{L}(\theta)=-\sum_i \log p_\theta(y_i)$, égale à la cross-entropie dans les modèles de langue ; au minimum $\hat\theta$, la Hessienne $\nabla_\theta^2 \mathcal{L}(\hat\theta)$ coïncide en moyenne avec $I_n(\hat\theta)$, d’où l’interprétation de la Fisher comme “courbure de la loss”. Enfin, l’approximation de second ordre de la divergence de KL donne $\mathrm{KL}\big(p_\theta\,|\,p_{\theta+\delta}\big)\approx \tfrac{1}{2}\,\delta^\top I(\theta)\,\delta$, plaçant $I(\theta)$ au cœur de la <strong>natural gradient</strong> et de la géométrie de l’information.</p> <p>A noter que dans les modèles de langue, c’est l’information de Fisher qui est utilisée comme loss (Trouver les $theta$ qui maximisent les probabilités des observations (log-likelihood), ou minimiser la negative log-likelihood).</p> <h1 id="jacobienne">Jacobienne</h1> <p>Soit $X\in\mathbb{R}^d$ à densité $f_X$ et une application $T:\mathbb{R}^d\to\mathbb{R}^d$ de classe $C^1$ inversible presque partout, avec matrice jacobienne $J_T(x)=\big[\partial T_i/\partial x_j\big]<em>{i,j}$ et déterminant $\det J_T(x)$. Pour $Y=T(X)$, la densité de $Y$ est le <strong>pushforward</strong> de $f_X$ par $T$ : $f_Y(y)=f_X!\big(T^{-1}(y)\big)\,\big|\det J</em>{T^{-1}}(y)\big|=f_X(x)\,/\,\big|\det J_T(x)\big|$ où $x=T^{-1}(y)$. Cette formule découle de la conservation de la masse $\int_{A} f_Y(y)\,dy=\int_{T^{-1}(A)} f_X(x)\,dx$ et du fait que $|\det J_T(x)|$ mesure la dilatation locale de volume (la valeur absolue assure des densités positives, l’orientation n’ayant pas d’effet probabiliste). En dimension 1, pour une transformation monotone $Y=g(X)$, $f_Y(y)=f_X!\big(g^{-1}(y)\big)\,\big|\tfrac{d}{dy}g^{-1}(y)\big|$; si $g$ est décroissante la dérivée est négative mais la valeur absolue corrige le signe. En cas de transformation <strong>non injective</strong> mais $C^1$ par morceaux (par ex. $T$ $k$-à-1 sur une partition), on somme sur les antécédents : $f_Y(y)=\sum_{x\in T^{-1}({y})} f_X(x)\,/\,\big|\det J_T(x)\big|$. Les espérances suivent la même règle : pour toute $\varphi$ intégrable, $\mathbb{E}[\varphi(Y)]=\int \varphi!\big(T(x)\big)f_X(x)\,dx=\int \varphi(y)\,f_Y(y)\,dy$. Si $\det J_T(x)=0$ sur un ensemble de mesure non négligeable, l’image peut être <strong>singulière</strong> (masse portée par une variété de dimension $&lt;d$) et $Y$ peut ne pas admettre de densité Lebesgue; sinon, dans le cas classique (diffeomorphisme a.e.), la jacobienne fournit la relation fondamentale de changement de variables en probabilité. <em>Ex. compact : coordonnées polaires</em> $T(r,\theta)=(r\cos\theta,r\sin\theta)$ sur $\mathbb{R}^2\setminus{0}$ ont $|\det J_T|=r$, d’où $f_{X,Y}(x,y)=f_{R,\Theta}(r,\theta)/r$ avec $r=\sqrt{x^2+y^2}$, $\theta=\mathrm{atan2}(y,x)$.</p> <h1 id="transformation-dun-problème-complexe-à-un-problème-convexe">Transformation d’un problème complexe à un problème convexe</h1> <p><strong>Transformer un problème complexe en problème convexe</strong> consiste à remplacer un programme non convexe $\min_{x\in\mathcal{X}} f(x)$ (où $f$ ou $\mathcal{X}$ est non convexe) par un <strong>relaxé convexe</strong> $\min_{x\in\mathcal{X}<em>{\mathrm{cvx}}} f</em>{\mathrm{cvx}}(x)$ résoluble globalement avec certificats d’optimalité (dualité forte, KKT). La démarche canonique combine : (i) <strong>relaxation convexe</strong> par enveloppe convexe et conv(hull) : on prend $f_{\mathrm{cvx}}=f^{<strong>}$ (biconjuguée de Fenchel, plus grand convexe l.s.c. majoré par $f$) et un sur-ensemble convexe $\mathcal{X}<em>{\mathrm{cvx}}\supseteq\mathcal{X}$, donnant un **borne inférieure** $p^\star</em>{\mathrm{cvx}}\le p^\star$ (ex.: $\ell_0\to\ell_1$, $\min_x \tfrac12|Ax-b|<em>2^2$ s.c. $|x|_0\le k$ $\leadsto$ $\min_x \tfrac12|Ax-b|_2^2+\lambda|x|_1$; **exactitude** si p.ex. $A$ vérifie une RIP/N.S.P., alors la solution $\ell_1$ coïncide avec $\ell_0$); (ii) **relaxation de rang** via norme nucléaire : $\min_X \operatorname{rank}(X)$ s.c. $\mathcal{A}(X)=b$ $\leadsto$ $\min_X |X|</em>\ast$ s.c. $\mathcal{A}(X)=b$ (exact sous incohérence/échantillonnage suffisant) ; (iii) **lifting/SDP</strong> pour bilinéaires/quadratiques : $x^\top Qx$ avec contraintes non convexes $\leadsto$ $X=xx^\top\succeq 0$ et contraintes linéaires sur $X$, en relâchant $\operatorname{rank}(X)=1$ ; (iv) <strong>épigraphe/perspective</strong> pour convexifier max/fractions : minimiser $f(x)$ $\equiv$ $\min_{t}{t: (x,t)\in\operatorname{epi} f}$, et pour un objectif fractionnel linéaire $\min \frac{c^\top x+d}{e^\top x+f}$ s.c. $Ax\le b$, la <strong>transformation de Charnes–Cooper</strong> $t=(e^\top x+f)^{-1}$, $y=xt$ donne $\min c^\top y+dt$ s.c. $Ay\le bt$, $e^\top y+ft=1$, $t&gt;0$ (LP) ; (v) <strong>changements de variables</strong> (mono. difféomorphes) pour rendre convexes des formes multiplicatives, p.ex. <strong>programmation géométrique</strong> : $\min \sum_k c_k \prod_i x_i^{a_{ik}}$ s.c. $\sum_k d_k \prod_i x_i^{b_{ik}}\le 1$ devient convex en posant $y_i=\log x_i$ et en log-transformant (somme de fonctions convexes log-somme-exp) ; (vi) <strong>prox/ pénalisation exacte</strong> pour absorber contraintes non convexes dans l’objectif, puis remplacer par une norme convexe (p.ex. pénalités SCAD/MCP $\to$ majorations convexes via MM). Sur le plan théorique, le relâché fournit un <strong>dualisme de Lagrange</strong> avec écart $p^\star-p^\star_{\mathrm{cvx}}\ge 0$ ; si la relaxation est <strong>serrée</strong> (p.ex. solution située à un <strong>point extrême</strong> de $\mathcal{X}<em>{\mathrm{cvx}}$ respectant les <strong>KKT</strong> du problème original), on a $p^\star=p^\star</em>{\mathrm{cvx}}$ (exactitude) et la solution du convexe résout l’original. Au final, on obtient un problème convexe de la forme standard $\min_x g(x)$ s.c. $h_i(x)\le 0,\;Ax=b$ (avec $g,h_i$ convexes), solvable en temps polynomial (méthodes de points intérieurs, descente proximale), garantissant un optimum <strong>global</strong> et des <strong>certificats</strong> via conditions KKT et <strong>dualité forte</strong> (écart nul).</p> <h1 id="transformation-dun-signal-du-domaine-temporel-au-domaine-fréquentiel">Transformation d’un signal du domaine temporel au domaine fréquentiel</h1> <p>Pour un signal continu $x(t)\in L^1(\mathbb{R})$ (ou $L^2$), sa transformée de Fourier est $X(f)=\mathcal{F}{x}(f)=\int_{-\infty}^{\infty} x(t)\,e^{-j2\pi f t}\,dt$ et l’inverse s’écrit $x(t)=\int_{-\infty}^{\infty} X(f)\,e^{j2\pi f t}\,df$; $X(f)$ encode l’<strong>amplitude</strong> et la <strong>phase</strong> des composantes sinusoïdales de fréquence $f$. La transformation est linéaire, convertit les <strong>décalages temporels</strong> en facteurs de phase $x(t-t_0)\;\leftrightarrow\;X(f)\,e^{-j2\pi f t_0}$, la <strong>modulation</strong> temporelle en <strong>translation fréquentielle</strong> $x(t)\,e^{j2\pi f_0 t}\;\leftrightarrow\;X(f-f_0)$, et surtout transforme la <strong>convolution temporelle</strong> en <strong>produit fréquentiel</strong> $(x*y)(t)\;\leftrightarrow\;X(f)Y(f)$ (et le produit temporel en convolution fréquentielle). L’<strong>identité de Parseval/Plancherel</strong> préserve l’énergie : $\int |x(t)|^2 dt=\int |X(f)|^2 df$. Pour un signal discret $x[n]=x(nT_s)$ échantillonné à $f_s=1/T_s$, la DTFT est $X(e^{j\omega})=\sum_{n=-\infty}^{\infty} x[n]\,e^{-j\omega n}$ ($\omega=2\pi f/f_s$) et, sur une fenêtre de longueur $N$, la <strong>DFT</strong> est $X[k]=\sum_{n=0}^{N-1} x[n]\,e^{-j2\pi nk/N}$, $k=0,\dots,N-1$, avec inverse $x[n]=\frac{1}{N}\sum_{k=0}^{N-1} X[k]\,e^{j2\pi nk/N}$; la résolution fréquentielle vaut $\Delta f=f_s/N$ et l’algorithme <strong>FFT</strong> calcule la DFT en $O(N\log N)$. L’<strong>échantillonnage</strong> d’un signal continu $x(t)$ produit un spectre périodisé : si $x_s(t)=\sum_{n} x(nT_s)\,\delta(t-nT_s)$, alors $X_s(f)=\frac{1}{T_s}\sum_{m\in\mathbb{Z}} X(f-mf_s)$; l’absence d’<strong>aliasing</strong> requiert un spectre borné $|f|\le B$ et $f_s&gt;2B$ (théorème de Shannon–Nyquist), avec reconstruction idéale $x(t)=\sum_{n\in\mathbb{Z}} x[n]\,\mathrm{sinc}!\big(\tfrac{t-nT_s}{T_s}\big)$. En pratique, l’analyse de signaux non stationnaires utilise des <strong>fenêtres</strong> (STFT) : $X(t_0,f)=\int x(t)\,w(t-t_0)\,e^{-j2\pi f t}\,dt$, où le choix de $w$ contrôle le compromis temps–fréquence (fuites spectrales et élargissement des pics). Ces outils fournissent une cartographie rigoureuse du contenu fréquentiel à partir du temps, ouvrant la voie au filtrage linéaire, à la débruitage, à la démodulation et à l’identification de systèmes via leurs réponses fréquentielles.</p> <h1 id="quand-on-a-linverse-dune-somme-on-peut-toujours-revenir-à-une-somme-des-puissances">Quand on a l’inverse d’une somme: on peut toujours revenir à une somme des puissances!</h1> <h2 id="démo">Démo</h2> <blockquote> <p>On va montrer que \(\colorbox{cyan}{$\displaystyle(1-x)^{-1} = 1 + x + x^2 + x^3 + x^4 + ...$}\)</p> </blockquote> <p>Prenons la somme $S = 1 + x + x^2 + x^3 + x^4 + …$</p> \[\begin{split} S - Sx &amp;= 1 + x + x^2 + x^3 + x^4 + ... - (x + x^2 + x^3 + x^4 + ...) \\ &amp;= 1 \end{split}\] <p>D’où:</p> \[\begin{split} S - Sx &amp;= 1 \\ S(1-x) &amp;= 1 \\ \colorbox{cyan}{$\displaystyle S = \frac{1}{1-x}$} \end{split}\] <p>A noter : quand on fait une approximation en $x$ proche de 0 (développement limité), on s’arrête à une certaine puissance $n$ (en fonction de l’approximation à l’ordre $n$).</p> <h2 id="utilité">Utilité</h2> <p>Dans le papier <a href="https://aclanthology.org/2024.emnlp-main.731.pdf" rel="external nofollow noopener" target="_blank">Chain and Causal Attention for Efficient Entity Tracking</a>, j’essayais de comprendre :</p> \[A + A^2 + A^3 + A^4 + ... = A(I - A)^{-1}\] <p>Et j’ai compris que ça venait de l’extension aux matrices de la formule précédente:</p> \[\begin{split} 1 + x + x^2 + x^3 + x^4 + ... &amp;= \frac{1}{1-x} \\ I + A + A^2 + A^3 + A^4 + ... &amp;= I(I - A)^{-1} \\ A + A^2 + A^3 + A^4 + ... &amp;= A(I - A)^{-1} \end{split}\] <h1 id="la-formule-de-taylor-dordre-n-pour-approximer-une-fonction-en-un-point-x_i">La formule de Taylor d’ordre n pour approximer une fonction en un point $x_i$</h1> <p>Pour une fonction $f(x)$ suffisamment dérivable au voisinage de $x_i$, la formule de Taylor d’ordre $n$ s’écrit :</p> \[f(x) \approx \sum_{k=0}^{n} \frac{f^{(k)}(x_i)}{k!}(x - x_i)^k + R_n(x)\] <p>où :</p> <ul> <li>$f^{(k)}(x_i)$ désigne la dérivée $k$-ième de $f$ évaluée en $x_i$</li> <li>$f^{(0)}(x_i) = f(x_i)$ par convention</li> <li>$R_n(x)$ est le reste de Taylor d’ordre $n$</li> </ul> <p>En développant explicitement :</p> \[f(x) \approx f(x_i) + f'(x_i)(x - x_i) + \frac{f''(x_i)}{2!}(x - x_i)^2 + \cdots + \frac{f^{(n)}(x_i)}{n!}(x - x_i)^n + R_n(x)\] <p>On a donc par exemple, l’approximation de Taylor à l’ordre 1: \(f(x) \approx f(x_i) + f'(x_i)\,(x - x_i)\)</p> <h2 id="démo-par-récurrence-et-intégrations-par-parties">Démo (par récurrence et intégrations par parties)</h2> <p><strong>Étape 1 : Cas de base (n = 0)</strong></p> <p>Partons du théorème fondamental du calcul intégral : \(f(x) - f(x_i) = \int_{x_i}^{x} f'(t) \, dt\)</p> <p>Donc : \(f(x) = f(x_i) + \int_{x_i}^{x} f'(t) \, dt\)</p> <p>C’est la formule de Taylor d’ordre 0 avec $R_0(x) = \int_{x_i}^{x} f’(t) \, dt$.</p> <p><strong>Étape 2 : Passage de l’ordre k à l’ordre k+1</strong></p> <p>Supposons que nous ayons établi la formule à l’ordre $k$ : \(f(x) = \sum_{j=0}^{k} \frac{f^{(j)}(x_i)}{j!}(x - x_i)^j + R_k(x)\)</p> <p>avec $R_k(x) = \int_{x_i}^{x} \frac{f^{(k+1)}(t)}{k!}(x - t)^k \, dt$.</p> <p>Appliquons une intégration par parties à $R_k(x)$ :</p> <p>Posons :</p> <ul> <li>$u = f^{(k+1)}(t)$, donc $du = f^{(k+2)}(t) \, dt$</li> <li>$dv = \frac{(x - t)^k}{k!} \, dt$, donc $v = -\frac{(x - t)^{k+1}}{(k+1)!}$</li> </ul> <p>L’intégration par parties donne : \(R_k(x) = \left[ f^{(k+1)}(t) \cdot \left(-\frac{(x - t)^{k+1}}{(k+1)!}\right) \right]_{x_i}^{x} + \int_{x_i}^{x} \frac{(x - t)^{k+1}}{(k+1)!} f^{(k+2)}(t) \, dt\)</p> <p>Le terme entre crochets devient : \(\left[ -\frac{f^{(k+1)}(t)(x - t)^{k+1}}{(k+1)!} \right]_{x_i}^{x} = 0 - \left(-\frac{f^{(k+1)}(x_i)(x - x_i)^{k+1}}{(k+1)!}\right) = \frac{f^{(k+1)}(x_i)}{(k+1)!}(x - x_i)^{k+1}\)</p> <p>Donc : \(R_k(x) = \frac{f^{(k+1)}(x_i)}{(k+1)!}(x - x_i)^{k+1} + \int_{x_i}^{x} \frac{f^{(k+2)}(t)}{(k+1)!}(x - t)^{k+1} \, dt\)</p> <p>En substituant dans l’expression de $f(x)$ : \(f(x) = \sum_{j=0}^{k} \frac{f^{(j)}(x_i)}{j!}(x - x_i)^j + \frac{f^{(k+1)}(x_i)}{(k+1)!}(x - x_i)^{k+1} + R_{k+1}(x)\)</p> <p>où $R_{k+1}(x) = \int_{x_i}^{x} \frac{f^{(k+2)}(t)}{(k+1)!}(x - t)^{k+1} \, dt$.</p> <p>Ceci établit la formule à l’ordre $k+1$ : \(f(x) = \sum_{j=0}^{k+1} \frac{f^{(j)}(x_i)}{j!}(x - x_i)^j + R_{k+1}(x)\)</p> <p><strong>Conclusion</strong></p> <p>Par récurrence, la formule de Taylor d’ordre $n$ est démontrée pour tout $n \geq 0$.</p> <h2 id="utilité-1">Utilité</h2> <p>Si on cherche à approximer les nouveaux poids du modèle $\theta_\varepsilon$ qui sont les poids du modèle modifié (avec retrait d’un exemple d’entraînement), on a la nouvelle loss finale du modèle $R_\varepsilon(\theta, z_\text{train})$ qui est:</p> \[R_\varepsilon(\theta, z_\text{train}) = \frac{1}{n}\sum_{i=1}^n \mathcal{L}(z_i,\theta) \;+\;\varepsilon\,\mathcal{L}(z_{\rm train},\theta).\] <p>Et donc les nouveaux poids $\theta_\varepsilon$ qui minimisent cette loss:</p> \[\theta_\varepsilon = \arg\min_{\theta}\;R_\varepsilon(\theta, z_\text{train})\] <p>D’où:</p> \[\begin{split} R_\varepsilon(\theta_\varepsilon, z_\text{train}) &amp;= \frac{1}{n}\sum_{i=1}^n \mathcal{L}(z_i,\theta_\varepsilon) \;+\;\varepsilon\,\mathcal{L}(z_{\rm train},\theta_\varepsilon) \\ &amp;= 0 \end{split}\] <p>Maintenant, on peut utiliser une approximation de Taylor à $\varepsilon$ proche de 0 de \(\frac{1}{n}\sum_{i=1}^n \mathcal{L}(z_i,\theta_\varepsilon) \;+\;\varepsilon\,\mathcal{L}(z_{\rm train},\theta_\varepsilon)\), car on connait $\theta$ et donc on peut peut-être arriver à quelque chose!</p> <p>Pour rappel, l’approximation de Taylor de $f$ à l’ordre 1 en $\varepsilon$ proche de 0 donne: \(f(\theta_\varepsilon) \approx f(\hat{\theta}) + f'(\hat{\theta})\,(\theta_\varepsilon - \hat{\theta})\)</p> \[\begin{split} \underbrace{\frac{1}{n}\sum_{i=1}^n \nabla_\theta \mathcal{L}(z_i,\theta_\varepsilon) \;+\;\varepsilon\,\nabla_\theta \mathcal{L}(z_{\rm train},\theta_\varepsilon)}_{f(\theta_\varepsilon)} \approx \underbrace{[ \frac{1}{n}\sum_{i=1}^n \nabla_\theta \mathcal{L}(z_i,\hat{\theta}) \;+\;\varepsilon\,\nabla_\theta \mathcal{L}(z_{\rm train},\hat{\theta}) ]}_{f(\hat{\theta})} \;\; &amp;+ \\ \;\; \underbrace{[ \frac{1}{n}\sum_{i=1}^n \nabla^2_\theta \mathcal{L}(z_i,\hat{\theta}) \;+\;\varepsilon\,\nabla^2_\theta \mathcal{L}(z_{\rm train},\hat{\theta}) ]}_{f'(\hat{\theta})} \;\;(\theta_\varepsilon - \hat{\theta}) \end{split}\] <p>ça nous permet d’isoler $\theta_\varepsilon$ et donc d’écrire $\theta_\varepsilon$ en fonction de $\theta$!</p> \[\begin{split} \theta_\varepsilon \approx \hat{\theta} - \left[ \frac{1}{n}\sum_{i=1}^n \nabla^2_\theta \mathcal{L}(z_i,\hat{\theta}) + \varepsilon\,\nabla^2_\theta \mathcal{L}(z_{\rm train},\hat{\theta}(z_\text{train})) \right]^{-1} \;\; &amp;\times \\ \;\; \left[ \frac{1}{n}\sum_{i=1}^n \nabla_\theta \mathcal{L}(z_i,\hat{\theta}) + \varepsilon\,\nabla_\theta \mathcal{L}(z_{\rm train},\hat{\theta}) \right] \end{split}\] <p>Et ça c’est cool parce qu’on sait le simplifier puisqu’on connait $\hat{\theta}$! Notamment $\hat{\theta}$ sont les poids optimaux pour le modèle de base, donc $\frac{1}{n}\sum_{i=1}^n \nabla_\theta \mathcal{L}(z_i,\hat{\theta}) = 0$. D’autres simplifications peuvent ensuite être faites. Et donc on arrive à approximer les nouveaux poids du modèle!</p> <h1 id="penser-à-la-chain-rule-exemple-pour-calculer-fracpartial-mathcallpartial-theta">Penser à la chain rule: exemple pour calculer $\frac{\partial \mathcal{L}}{\partial \theta}$</h1> <p>On a par exemple $\mathcal{L}$ est une fonction de perte (loss) qui dépend de la sortie du modèle $y$, $y$ qui est la sortie du modèle, qui dépend des paramètres du modèle $\theta$, d’où on a: $\frac{\partial \mathcal{L}}{\partial \theta} = \frac{\partial \mathcal{L(y(\theta))}}{\partial \theta}$. ça doit nous faire penser à la chain rule! \(\frac{\partial}{\partial x} f(g(x))\) avec $f = \mathcal{L}$, ou $f(y) = \mathcal{L(y)}$ et $g(\theta) = y(\theta)$.</p> <h2 id="rappel-de-la-formule">Rappel de la formule…</h2> <p>Il faut donc ici penser à la chain rule! Un petit rappel…</p> \[\frac{\partial}{\partial x} f(g(x)) = f'(g(x)) \cdot g'(x)\] <h2 id="application-concrète-pour-calculer-fracpartial-mathcallpartial-theta">Application concrète pour calculer $\frac{\partial \mathcal{L}}{\partial \theta}$</h2> \[\frac{\partial \mathcal{L(y(\theta))}}{\partial \theta} = \frac{\partial \mathcal{L}}{\partial y} \cdot \frac{\partial y}{\partial \theta}\] <h1 id="optimisation-sous-contrainte-en-ia">Optimisation sous contrainte en IA</h1> <p>TODO</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'camillebrl/camillebrl.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Camille Barboule. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/lightbox2@2.11.5/dist/js/lightbox.min.js" integrity="sha256-A6jI5V9s1JznkWwsBaRK8kSeXLgIqQfxfnvdDOZEURY=" crossorigin="anonymous"></script> <script defer src="/assets/js/photoswipe-setup.js" type="module"></script> <script defer src="https://cdn.jsdelivr.net/npm/spotlight.js@0.7.8/dist/spotlight.bundle.min.js" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/venobox@2.1.8/dist/venobox.min.js" integrity="sha256-LsGXHsHMMmTcz3KqTaWvLv6ome+7pRiic2LPnzTfiSo=" crossorigin="anonymous"></script> <script defer src="/assets/js/venobox-setup.js?897c1d9c0b6fcf82b949511c1609d055" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> <script>
      document.addEventListener('DOMContentLoaded', function() {
        // Fonction pour créer un ID à partir du texte (compatible avec Jekyll slugify)
        function createId(text) {
          return text.trim()
            .toLowerCase()
            .normalize('NFD').replace(/[\u0300-\u036f]/g, '') // Enlever les accents
            .replace(/[^\w\s-]/g, '') // Enlever les caractères spéciaux
            .replace(/\s+/g, '-') // Remplacer les espaces par des tirets
            .replace(/-+/g, '-') // Éviter les tirets multiples
            .replace(/^-|-$/g, ''); // Enlever les tirets au début et à la fin
        }
        
        // Fonction pour décoder et normaliser une ancre d'URL
        function normalizeAnchor(anchor) {
          try {
            // Décoder l'URL
            const decoded = decodeURIComponent(anchor);
            // Appliquer la même normalisation que createId
            return createId(decoded);
          } catch (e) {
            return anchor;
          }
        }
        
        // Récupérer tous les headers dans l'article
        const article = document.querySelector('d-article');
        const headers = article.querySelectorAll('h1, h2, h3, h4, h5, h6');
        const tocContainer = document.getElementById('toc-container');
        
        if (headers.length === 0) return;
        
        // Filtrer les headers pour exclure ceux dans d-contents et d-title
        const filteredHeaders = Array.from(headers).filter(header => {
          return !header.closest('d-contents') && !header.closest('d-title');
        });
        
        if (filteredHeaders.length === 0) return;
        
        // Structure pour construire la hiérarchie
        let currentList = document.createElement('ul');
        tocContainer.appendChild(currentList);
        
        let stack = [{level: 0, list: currentList}];
        
        filteredHeaders.forEach((header) => {
          // Ajouter un ID au header s'il n'en a pas
          if (!header.id) {
            header.id = createId(header.textContent);
          }
          
          // Déterminer le niveau (h1=1, h2=2, etc.)
          const level = parseInt(header.tagName.substring(1));
          
          // Gérer la hiérarchie
          while (stack.length > 1 && stack[stack.length - 1].level >= level) {
            stack.pop();
          }
          
          // Si on descend dans la hiérarchie
          if (stack[stack.length - 1].level < level) {
            const newList = document.createElement('ul');
            const lastItem = stack[stack.length - 1].list.lastElementChild;
            if (lastItem) {
              lastItem.appendChild(newList);
            } else {
              stack[stack.length - 1].list.appendChild(newList);
            }
            stack.push({level: level, list: newList});
          }
          
          // Créer l'élément de liste
          const listItem = document.createElement('li');
          const link = document.createElement('a');
          link.href = '#' + header.id;
          link.textContent = header.textContent;
          link.addEventListener('click', function(e) {
            e.preventDefault();
            header.scrollIntoView({ behavior: 'smooth', block: 'start' });
          });
          
          listItem.appendChild(link);
          stack[stack.length - 1].list.appendChild(listItem);
        });
        
        // Nettoyer les listes vides
        const emptyLists = tocContainer.querySelectorAll('ul:empty');
        emptyLists.forEach(list => list.remove());
        
        // Gérer la navigation depuis l'URL
        function navigateToHash() {
          if (window.location.hash) {
            const hash = window.location.hash.substring(1);
            const normalizedHash = normalizeAnchor(hash);
            
            // Essayer de trouver l'élément par ID normalisé
            let targetElement = document.getElementById(normalizedHash);
            
            // Si pas trouvé, essayer avec le hash original
            if (!targetElement) {
              targetElement = document.getElementById(hash);
            }
            
            // Si toujours pas trouvé, chercher dans tous les headers
            if (!targetElement) {
              filteredHeaders.forEach(header => {
                if (createId(header.textContent) === normalizedHash) {
                  targetElement = header;
                }
              });
            }
            
            if (targetElement) {
              setTimeout(() => {
                targetElement.scrollIntoView({ behavior: 'smooth', block: 'start' });
              }, 100);
            }
          }
        }
        
        // Naviguer au chargement de la page
        navigateToHash();
        
        // Écouter les changements de hash
        window.addEventListener('hashchange', navigateToHash);
      });
    </script> </body> </html>