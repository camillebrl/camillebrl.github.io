<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="fr"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://camillebrl.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://camillebrl.github.io/" rel="alternate" type="text/html" hreflang="fr"/><updated>2025-08-15T14:14:14+00:00</updated><id>https://camillebrl.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Explicabilité en deep learning</title><link href="https://camillebrl.github.io/blog/2025/explainable_deep_learning/" rel="alternate" type="text/html" title="Explicabilité en deep learning"/><published>2025-08-15T11:00:00+00:00</published><updated>2025-08-15T11:00:00+00:00</updated><id>https://camillebrl.github.io/blog/2025/explainable_deep_learning</id><content type="html" xml:base="https://camillebrl.github.io/blog/2025/explainable_deep_learning/"><![CDATA[<h1 id="evaluation-de-la-robustesse-du-modèle">Evaluation de la robustesse du modèle</h1> <p>TODO : <a href="https://github.com/deel-ai/deel-lip">deel-lip de Deel-IA</a></p> <h1 id="explication-de-la-prédiction-du-modèle">Explication de la prédiction du modèle</h1> <p>Il existe plusieurs approches pour expliquer la génération d’un modèle de deep learning:</p> <h2 id="explication-de-la-génération-par-carte-dattribution-de-la-donnée-dentrée-pour-identifier-les-zones-dintérêt-sur-linput-portées-par-le-modèle-pour-quil-effectue-sa-génération">Explication de la génération par carte d’attribution de la donnée d’entrée pour identifier les zones d’intérêt sur l’input portées par le modèle pour qu’il effectue sa génération</h2> <p>Dépend du cas d’usage et du type de modèle et du type d’input.</p> <p>Certaines fonctions de la librairie sont applicables à tout type de modèle (pytorch via wrapper, tensorflow, keras) et tout type de données et use-case, comme la fonction Saliency.</p> <h3 id="la-méthode-saliency-de-xplain-de-deel-ai">La méthode <a href="https://github.com/deel-ai/xplique/blob/master/xplique/attributions/saliency.py">Saliency de Xplain de deel-ai</a></h3> <p>La méthode Saliency calcule le gradient absolu de la sortie par rapport à l’entrée :</p> \[\forall \text{dimension } i \text{ de l'entrée x (pixel, l'embedding d'un token, etc):} S_i = \left| \frac{\partial f(x)}{\partial x_i} \right|\] <p>A notée qu’ici, chaque dimension de l’entrée peut elle-même être de plusieurs dimensions (exemple d’un pixel en 3 dimensions (R,G,B), ou d’un token en dim_model dimensions). Dans ce cas, soit on prend le maximum des gradients de la prédiction du modèle par rapport à chaque dimension de l’embedding du token d’entrée ou du pixel d’entrée, soit on prend la norme L2 des gradients des dimensions (exemple du papier https://aclanthology.org/2024.emnlp-main.347.pdf), soit on prend la moyenne, …</p> <p>Dans le code https://github.com/deel-ai/xplique/blob/master/xplique/attributions/saliency.py:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Saliency</span><span class="p">(</span><span class="n">WhiteBoxExplainer</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span>
                 <span class="n">model</span><span class="p">:</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">Model</span><span class="p">,</span>
                 <span class="n">output_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                 <span class="n">batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
                 <span class="n">operator</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Tasks</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">OperatorSignature</span><span class="p">]]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                 <span class="n">reducer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="sh">"</span><span class="s">max</span><span class="sh">"</span><span class="p">,):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">output_layer</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">operator</span><span class="p">,</span> <span class="n">reducer</span><span class="p">)</span>

    <span class="nd">@sanitize_input_output</span> <span class="c1"># Cette fonction s'assure que les inputs et outputs du modèle sont des tf.Tensors
</span>    <span class="nd">@WhiteBoxExplainer._harmonize_channel_dimension</span>

    <span class="k">def</span> <span class="nf">explain</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
        <span class="c1"># 1. Calcul du gradient via backpropagation
</span>        <span class="n">gradients</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">batch_gradient</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">model</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">batch_size</span><span class="p">)</span>
        
        <span class="c1"># 2. Application de la valeur absolue
</span>        <span class="n">gradients</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">gradients</span><span class="p">)</span>
        
        <span class="c1"># 3. Pour les images RGB, réduction sur les canaux (max par défaut)
</span>        <span class="c1"># Cela donne l'importance maximale parmi R, G, B pour chaque pixel
</span>        <span class="k">return</span> <span class="n">gradients</span>
</code></pre></div></div> <p>Exemple d’application:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torchvision.models</span> <span class="k">as</span> <span class="n">models</span>
<span class="kn">from</span> <span class="n">xplique.wrappers</span> <span class="kn">import</span> <span class="n">TorchWrapper</span>
<span class="kn">from</span> <span class="n">xplique.attributions</span> <span class="kn">import</span> <span class="n">Saliency</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># Exemple avec un modèle pytorch
</span><span class="n">model_pt</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="nf">resnet18</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">model_pt</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">'</span><span class="s">mon_modele.pt</span><span class="sh">'</span><span class="p">))</span>
<span class="n">model_pt</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>

<span class="n">device</span> <span class="o">=</span> <span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span>
<span class="n">wrapped_model</span> <span class="o">=</span> <span class="nc">TorchWrapper</span><span class="p">(</span><span class="n">model_pt</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span> <span class="c1"># pas besoin pour un modèle tensorflow
</span>
<span class="n">explainer</span> <span class="o">=</span> <span class="nc">Saliency</span><span class="p">(</span><span class="n">wrapped_model</span><span class="p">,</span> <span class="n">reducer</span><span class="o">=</span><span class="sh">"</span><span class="s">max</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># reducer pour le traitement d'un input de plusieurs dimensions (exemple R, G, B pour des pixels)
</span>
<span class="n">attributions</span> <span class="o">=</span> <span class="nf">explainer</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span> <span class="c1"># les inputs et targets peuvent être en numpy array
</span></code></pre></div></div> <p>A noter que cette approche a plusieurs limites, notamment:</p> <ul> <li>Quand les unités d’entrée sont de haute dimension ; pour les images, il s’agit des pixels, qui sont de faibles dimensions (R, G, B uniquement), donc “faciles” à aggréger pour obtenir un score unique de saillance par unité (pixel) d’entrée. Par contre, quand on a des unités de l’input qui sont de haute dimension, cela peut être compliqué. C’est par exemple le cas pour les modèles de langue, où l’unité d’un input est un token, plus précisément l’embedding d’un token, qui lui est de très grande dimension (en fonction du modèle). Ainsi, il est compliqué d’agréger les D (D = dim_model) gradients obtenus pour avoir un score unique par unité de l’input.</li> <li>La valeur absolue du gradient de la prédiction du modèle par rapport aux unités de l’input ne permet pas de distinguer les unités de l’input qui ont un impact négatifs de ceux qui ont un impact positif sur la prédiction modèle</li> <li>Le gradient de la prédiction par rapport aux unités d’entrée n’a pas forcément de sens en termes absolu: un gradient petit n’implique pas forcément que l’unité de l’input n’est pas important: une unité peut être très influente mais avoir un gradient (local) faible. $\nabla_{x_i}(f(x))$ approxime l’effet de micro-perturbations de l’unité $i$ de l’input, pas le remplacement de cette unité (opération discrète et non-locale). Une unité peut être cruciale pour la classe, tout en ayant un gradient local faible. En effet, Le gradient est une pente locale. Si, autour de l’unité $i$ de l’input, la fonction “s’aplatit”, alors la pente est proche de 0, donc le gradient local est faible, même si l’unité $i$ porte une info décisive pour la prédiction.</li> </ul> <p>D’autres approches permettent de combler l’effet “local” du gradient, notamment les approches d’Integrated Gradients, ou de Gradient x Input.</p> <h3 id="la-méthode-des-gradients-intégrés-de-xplain-de-deel-ai">La méthode des <a href="https://github.com/deel-ai/xplique/blob/master/xplique/attributions/integrated_gradients.py">Gradients Intégrés de Xplain de deel-ai</a></h3> <p>Les <strong>gradients intégrés (IG)</strong> attribuent à chaque caractéristique $i$ une contribution <strong>cumulative</strong> le long d’un chemin qui relie une <strong>référence (baseline)</strong> $x’$ à l’entrée $x$. En choisissant le <strong>chemin linéaire</strong> $\gamma(\alpha)=x’ + \alpha\,(x-x’)$, $\alpha \in [0,1]$, l’attribution IG pour la $i$-ème dimension est \(\boxed{\;\mathrm{IG}_i(x; x') \;=\; (x_i - x'_i)\,\int_{0}^{1} \frac{\partial F\big(\gamma(\alpha)\big)}{\partial x_i}\, d\alpha\;}\) et, en pratique, on l’approxime par une somme de Riemann avec $m$ pas : \(\mathrm{IG}_i(x; x') \;\approx\; (x_i - x'_i)\,\frac{1}{m}\sum_{k=1}^{m} \left.\frac{\partial F(z)}{\partial x_i}\right|_{z\,=\,x' + \tfrac{k}{m}(x-x')}.\) En <strong>NLP</strong>, on applique IG sur l’<strong>espace d’embedding</strong> : si $e(x)\in\mathbb{R}^{d}$ est l’entrée réelle du réseau (concaténation des embeddings par token), on interpole $e’ + \alpha\,(e-e’)$ et on dérive $F$ par rapport aux composantes d’<strong>embedding</strong> (la baseline $e’$ est souvent le vecteur nul, un token [PAD], ou un embedding « neutre »).</p> <p>IG contourne le biais <strong>local</strong> des approches de gradient pur en <strong>agrégeant</strong> l’information de gradient <strong>le long du chemin</strong> $\gamma(\alpha)$ depuis la baseline $x’$ (où la sortie est « neutre ») vers $x$. Intuitivement, même si $\nabla F(x)\approx 0$, il existe souvent des $\alpha\in(0,1)$ où $\nabla F(\gamma(\alpha))$ est <strong>grand</strong> (région non saturée) ; l’intégrale \(\int_{0}^{1} \frac{\partial F\big(x' + \alpha(x-x')\big)}{\partial x_i}\, d\alpha\) <strong>accumule</strong> ces contributions, produisant une attribution fidèle au <strong>chemin causal continu</strong> qui mène de $x’$ à $x$. IG est ainsi une version <strong>chemin-intégrée</strong> du gradient, reliée à la <strong>valeur d’Aumann–Shapley</strong> (analogue continu des valeurs de Shapley), et hérite d’une interprétation en termes de <strong>coût marginal moyen</strong> le long de l’activation du feature $i$.</p> <p>A noter que le choix de la baseline $x’$ est majeur: il doit représenter une <strong>entrée de référence</strong> « absence d’information » (image noire, bruit faible, embedding nul/[\mathrm{PAD}], etc.). Le résultat dépend de ce choix, mais la <strong>complétude</strong> garantit $\sum_i \mathrm{IG}_i = F(x)-F(x’)$.</p> <p>Exemple d’application:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">xplique.attributions</span> <span class="kn">import</span> <span class="n">IntegratedGradients</span>

<span class="n">explainer</span> <span class="o">=</span> <span class="nc">IntegratedGradients</span><span class="p">(</span>
    <span class="n">wrapped_model</span><span class="p">,</span>
    <span class="n">steps</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="c1"># Nombre de points d'interpolation
</span>    <span class="n">baseline_value</span><span class="o">=</span><span class="mf">0.0</span> <span class="c1"># Valeur de référence pour la baseline
</span><span class="p">)</span>
<span class="n">attributions</span> <span class="o">=</span> <span class="nf">explainer</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
</code></pre></div></div> <p>Attention en revanche: Dans la librairie Xplique, on approxime l’intégrale continue $I_i=\int_0^1 g_i(\alpha)\,d\alpha$ — i.e. la moyenne du gradient $g_i(\alpha)=\frac{\partial F(x’+\alpha(x-x’))}{\partial x_i}$ le long du chemin $\gamma(\alpha)=x’+\alpha(x-x’)$ avec la règle du trapèze. Comme on ne dispose pas d’une primitive explicite pour un réseau de neurones, on discrétise l’intégrale en une somme finie sur $m$ points. Plusieurs schémas sont possibles :</p> <ul> <li><strong>Riemann (rectangle)</strong> : $I_i \approx \frac{1}{m}\sum_{k=1}^{m} g_i!\big(\tfrac{k}{m}\big)$ (erreur $O(1/m)$) ;</li> <li><strong>Milieu (midpoint)</strong> : $I_i \approx \frac{1}{m}\sum_{k=1}^{m} g_i!\big(\tfrac{k-\tfrac12}{m}\big)$ (erreur $O(1/m^2)$) ;</li> <li><strong>Trapèze (composite)</strong> : $I_i \approx \frac{1}{m}!\Big[\tfrac12 g_i(0)+\sum_{k=1}^{m-1} g_i!\big(\tfrac{k}{m}\big)+\tfrac12 g_i(1)\Big]$ (erreur $O(1/m^2)$, plus précis et plus symétrique) ;</li> </ul> <p>La librairie Xplique utilise la règle du trapèze, qui offre une meilleure précision (et respecte mieux la propriété de complétude $\sum_i \mathrm{IG}_i \approx F(x)-F(x’)$) à coût identique.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">IntegratedGradients</span><span class="p">(</span><span class="n">WhiteBoxExplainer</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span>
                 <span class="n">model</span><span class="p">:</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">Model</span><span class="p">,</span>
                 <span class="n">output_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                 <span class="n">batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
                 <span class="n">operator</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Tasks</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">OperatorSignature</span><span class="p">]]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                 <span class="n">reducer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="sh">"</span><span class="s">mean</span><span class="sh">"</span><span class="p">,</span>
                 <span class="n">steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
                 <span class="n">baseline_value</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="p">.</span><span class="mi">0</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">output_layer</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">operator</span><span class="p">,</span> <span class="n">reducer</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">steps</span> <span class="o">=</span> <span class="n">steps</span>
        <span class="n">self</span><span class="p">.</span><span class="n">baseline_value</span> <span class="o">=</span> <span class="n">baseline_value</span>

    <span class="nd">@sanitize_input_output</span>
    <span class="nd">@WhiteBoxExplainer._harmonize_channel_dimension</span>
    <span class="k">def</span> <span class="nf">explain</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
        <span class="c1"># Créer la baseline (point de référence neutre)
</span>        <span class="n">baseline</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">baseline_value</span>
        
        <span class="c1"># Créer le chemin interpolé
</span>        <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">steps</span><span class="p">):</span>
            <span class="n">alpha</span> <span class="o">=</span> <span class="n">step</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">steps</span>
            <span class="n">interpolated</span> <span class="o">=</span> <span class="n">baseline</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">inputs</span> <span class="o">-</span> <span class="n">baseline</span><span class="p">)</span>
            
        <span class="c1"># Calculer les gradients pour chaque point
</span>        <span class="n">interpolated_gradients</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">batch_gradient</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">interpolated_inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
        
        <span class="c1"># Moyenner avec la règle trapézoïdale
</span>        <span class="n">trapezoidal_gradients</span> <span class="o">=</span> <span class="n">gradients</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">gradients</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
        <span class="n">averaged_gradients</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reduce_mean</span><span class="p">(</span><span class="n">trapezoidal_gradients</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.5</span>
        
        <span class="c1"># Multiplier par la différence input-baseline
</span>        <span class="n">integrated_gradients</span> <span class="o">=</span> <span class="p">(</span><span class="n">inputs</span> <span class="o">-</span> <span class="n">baseline</span><span class="p">)</span> <span class="o">*</span> <span class="n">averaged_gradients</span>
        
        <span class="k">return</span> <span class="n">integrated_gradients</span>
</code></pre></div></div> <h3 id="détection-dobjets-sur-des-images-exemple-de-lard">Détection d’objets sur des images (exemple de LARD)</h3> <p>Exemple de Yolov8 fine-tuné sur la détection de pistes d’atterrissage (LARD_train_BIRK_LFST: https://entrepot.recherche.data.gouv.fr/dataset.xhtml?persistentId=doi:10.57745/MZSH2Y). Le modèle utilisé est https://github.com/AnnabellePundaky/runway-bounding-box-detection-NEW.</p> <h3 id="classification-de-texte">Classification de texte</h3> <p>Exemple de <a href="https://huggingface.co/answerdotai/ModernBERT-base">ModernBert</a> fine-tuné sur la classification de <a href="https://huggingface.co/datasets/DEEL-AI/NOTAM">NOTAMS</a>. Voici quelques métriques d’entraînement:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainable_deeplearning/modernbert_notam-480.webp 480w,/assets/img/explainable_deeplearning/modernbert_notam-800.webp 800w,/assets/img/explainable_deeplearning/modernbert_notam-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/explainable_deeplearning/modernbert_notam.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h4 id="exemple-de-lapproche-saliency-de-la-lib-xplain-de-deel-ai">Exemple de l’approche <a href="https://github.com/deel-ai/xplique/blob/master/xplique/attributions/saliency.py">Saliency de la lib Xplain de deel-ai</a></h4> <p>On utilise <a href="https://github.com/deel-ai/xplique/blob/master/xplique/attributions/saliency.py">Saliency de Xplain</a> pour calculer le gradient absolu de la prédiction (classe de NOTAM, one-hot) du modèle par rapport à chaque dimension de chaque token d’entrée et qu’on applique une norme L2 sur les gradients de chaque dimension d’un token pour obtenir une valeur de saliency par token.</p> <h5 id="code-dimplémentation-de-cette-approche-dexplicabilité-sur-un-modèle-de-classification-de-texte">Code d’implémentation de cette approche d’explicabilité sur un modèle de classification de texte</h5> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span><span class="p">,</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">xplique.wrappers</span> <span class="kn">import</span> <span class="n">TorchWrapper</span>
<span class="kn">from</span> <span class="n">xplique.attributions</span> <span class="kn">import</span> <span class="n">Saliency</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="n">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>
<span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForSequenceClassification</span>
<span class="kn">import</span> <span class="n">torch</span>

<span class="c1"># Le modèle: c'est un modèle de langue donc il faut loader le modèle et le tokenizer avec la lib transformers
</span><span class="n">MAX_LENGTH</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">OUTPUT_DIR</span> <span class="o">=</span> <span class="sh">"</span><span class="s">./modernbert_notam</span><span class="sh">"</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">OUTPUT_DIR</span><span class="p">)</span>
<span class="k">if</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">pad_token</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
    <span class="n">tokenizer</span><span class="p">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">eos_token</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">OUTPUT_DIR</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">).</span><span class="nf">eval</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">().</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="c1"># Wrapper qui attend des embeddings et injecte le mask pour être adapté aux requirements de TorchWrapper de xplain
</span><span class="k">class</span> <span class="nc">ModernBERTEmbedsWrapper</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">base_model</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">base</span> <span class="o">=</span> <span class="n">base_model</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">register_buffer</span><span class="p">(</span><span class="sh">"</span><span class="s">am</span><span class="sh">"</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">base</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span> <span class="c1"># On s'assure que le modèle interne est en eval
</span>        <span class="n">self</span><span class="p">.</span><span class="nf">train</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span> <span class="c1"># Et on met aussi le wrapper en eval
</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">base</span><span class="p">(</span><span class="n">inputs_embeds</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">am</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span><span class="p">.</span><span class="n">logits</span>

<span class="c1"># le dataset de test
</span><span class="n">dataset</span> <span class="o">=</span> <span class="nf">load_dataset</span><span class="p">(</span><span class="sh">"</span><span class="s">DEEL-AI/NOTAM</span><span class="sh">"</span><span class="p">)</span>
<span class="n">unique_labels</span> <span class="o">=</span> <span class="nf">set</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">label</span><span class="sh">'</span><span class="p">])</span>
<span class="n">num_labels</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">unique_labels</span><span class="p">)</span>
<span class="n">label2id</span> <span class="o">=</span> <span class="p">{</span><span class="n">label</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="nf">sorted</span><span class="p">(</span><span class="n">unique_labels</span><span class="p">))}</span>
<span class="n">id2label</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="n">label</span> <span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">label2id</span><span class="p">.</span><span class="nf">items</span><span class="p">()}</span>
<span class="n">k</span> <span class="o">=</span> <span class="mi">16</span> <span class="c1"># batch_size
</span><span class="n">texts</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">test</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">][:</span><span class="n">k</span><span class="p">]</span>
<span class="n">labels_str</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">test</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">label</span><span class="sh">'</span><span class="p">][:</span><span class="n">k</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="n">label2id</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">labels_str</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># shape (batch_size, )
</span><span class="n">tok</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span><span class="n">texts</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="sh">"</span><span class="s">pt</span><span class="sh">"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">MAX_LENGTH</span><span class="p">)</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="n">tok</span><span class="p">[</span><span class="sh">"</span><span class="s">input_ids</span><span class="sh">"</span><span class="p">].</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="c1"># shape (batch_size, amount of tokens per sentence)
</span><span class="n">attention_mask</span> <span class="o">=</span> <span class="n">tok</span><span class="p">[</span><span class="sh">"</span><span class="s">attention_mask</span><span class="sh">"</span><span class="p">].</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="c1"># shape (batch_size, amount of tokens per sentence)
# Embeddings d'entrée de shape (batch_size, amount of tokens per sentence, model_dim)
</span><span class="n">emb_layer</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">get_input_embeddings</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
    <span class="n">embeds</span> <span class="o">=</span> <span class="nf">emb_layer</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span> <span class="c1"># shape (batch_size, amount of tokens per sentence, model_dim)
</span>
<span class="c1"># On wrap le modèle avec le TorchWrapper de xplain
</span><span class="n">wrap</span> <span class="o">=</span> <span class="nc">ModernBERTEmbedsWrapper</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">wrapped</span> <span class="o">=</span> <span class="nc">TorchWrapper</span><span class="p">(</span><span class="n">wrap</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># On appliquer la classe Saliency
</span><span class="n">explainer</span> <span class="o">=</span> <span class="nc">Saliency</span><span class="p">(</span><span class="n">wrapped</span><span class="p">,</span> <span class="n">operator</span><span class="o">=</span><span class="sh">"</span><span class="s">classification</span><span class="sh">"</span><span class="p">)</span>
<span class="c1"># Xplique attend des targets en format numpy arrays (ici, des one-hot (batch_size, num_labels))
</span><span class="n">num_labels</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">id2label</span><span class="p">)</span>
<span class="n">targets</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">one_hot</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">num_labels</span><span class="p">).</span><span class="nf">float</span><span class="p">().</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>
<span class="c1"># Explanations: saliency sur les embeddings des tokens d'entrée (batch_size, amount of tokens per sentence, model_dim)
</span><span class="n">embeds_np</span> <span class="o">=</span> <span class="n">embeds</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">float</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>
<span class="n">E</span> <span class="o">=</span> <span class="n">explainer</span><span class="p">.</span><span class="nf">explain</span><span class="p">(</span><span class="n">embeds_np</span><span class="p">,</span> <span class="n">targets</span><span class="p">).</span><span class="nf">numpy</span><span class="p">()</span>
<span class="c1"># Obtention d'un score de saillance par token (norme L2 des gradients par rapport à chaque dim_model des tokens) + masquage padding -&gt; (shape (batch_size, amount of tokens per sentence)
</span><span class="n">token_scores</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">E</span><span class="p">,</span> <span class="nb">ord</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># norme L2 sur dim_model (sur les dim_model gradients)
</span><span class="n">token_scores</span> <span class="o">=</span> <span class="n">token_scores</span> <span class="o">*</span> <span class="n">attention_mask</span><span class="p">.</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>
</code></pre></div></div> <p>Et maintenant on peut afficher les cartes de saillance sur les tokens pour les exemples test:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">IPython.display</span> <span class="kn">import</span> <span class="n">display</span><span class="p">,</span> <span class="n">HTML</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">text</span> <span class="o">=</span> <span class="n">texts</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># on affiche pour le premier exemple du test set
</span>
<span class="n">enc</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span>
    <span class="n">text</span><span class="p">,</span>
    <span class="n">add_special_tokens</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">truncation</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">max_length</span><span class="o">=</span><span class="n">MAX_LENGTH</span><span class="p">,</span>
    <span class="n">return_offsets_mapping</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">return_attention_mask</span><span class="o">=</span><span class="bp">True</span>
<span class="p">)</span>

<span class="n">ids_single</span> <span class="o">=</span> <span class="n">enc</span><span class="p">[</span><span class="sh">"</span><span class="s">input_ids</span><span class="sh">"</span><span class="p">]</span>
<span class="n">offsets</span> <span class="o">=</span> <span class="n">enc</span><span class="p">[</span><span class="sh">"</span><span class="s">offset_mapping</span><span class="sh">"</span><span class="p">]</span>
<span class="n">mask_single</span> <span class="o">=</span> <span class="n">enc</span><span class="p">[</span><span class="sh">"</span><span class="s">attention_mask</span><span class="sh">"</span><span class="p">]</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">token_scores</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

<span class="c1"># Normalisation (pour avoir des scores entre 0 et 1)
</span><span class="n">smin</span><span class="p">,</span> <span class="n">smax</span> <span class="o">=</span> <span class="nf">float</span><span class="p">(</span><span class="n">scores</span><span class="p">.</span><span class="nf">min</span><span class="p">()),</span> <span class="nf">float</span><span class="p">(</span><span class="n">scores</span><span class="p">.</span><span class="nf">max</span><span class="p">())</span>
<span class="n">denom</span> <span class="o">=</span> <span class="p">(</span><span class="n">smax</span> <span class="o">-</span> <span class="n">smin</span><span class="p">)</span> <span class="nf">if </span><span class="p">(</span><span class="n">smax</span> <span class="o">-</span> <span class="n">smin</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mf">1e-12</span> <span class="k">else</span> <span class="mf">1.0</span>
<span class="n">scores_norm</span> <span class="o">=</span> <span class="p">(</span><span class="n">scores</span> <span class="o">-</span> <span class="n">smin</span><span class="p">)</span> <span class="o">/</span> <span class="n">denom</span>

<span class="c1"># Construit l'HTML avec un &lt;span&gt; par token non-spécial
</span><span class="n">parts</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">cursor</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">T_use</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">ids_single</span><span class="p">),</span> <span class="nf">len</span><span class="p">(</span><span class="n">scores</span><span class="p">))</span> <span class="c1"># sécurité si longueurs diffèrent
</span>
<span class="k">for</span> <span class="n">t_idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">T_use</span><span class="p">):</span>
    <span class="n">tok_id</span> <span class="o">=</span> <span class="n">ids_single</span><span class="p">[</span><span class="n">t_idx</span><span class="p">]</span>
    <span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span> <span class="o">=</span> <span class="n">offsets</span><span class="p">[</span><span class="n">t_idx</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">mask_single</span><span class="p">[</span><span class="n">t_idx</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span> <span class="c1"># padding atteint
</span>        <span class="k">break</span>
    <span class="k">if</span> <span class="n">tok_id</span> <span class="ow">in</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">all_special_ids</span><span class="p">:</span> <span class="c1"># saute [CLS], [SEP], etc.
</span>        <span class="k">continue</span>
    <span class="k">if</span> <span class="n">e</span> <span class="o">&lt;=</span> <span class="n">s</span><span class="p">:</span>  <span class="c1"># parfois des offsets vides
</span>        <span class="k">continue</span>

    <span class="c1"># Ajoute tout "trou" éventuel non couvert par les offsets (espaces, etc.)
</span>    <span class="k">if</span> <span class="n">s</span> <span class="o">&gt;</span> <span class="n">cursor</span><span class="p">:</span>
        <span class="n">parts</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">text</span><span class="p">[</span><span class="n">cursor</span><span class="p">:</span><span class="n">s</span><span class="p">])</span>

    <span class="c1"># Couleur = rouge avec alpha = score normalisé
</span>    <span class="n">alpha</span> <span class="o">=</span> <span class="nf">float</span><span class="p">(</span><span class="n">scores_norm</span><span class="p">[</span><span class="n">t_idx</span><span class="p">])</span>
    <span class="n">bg</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">rgba(255, 0, 0, </span><span class="si">{</span><span class="n">alpha</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">)</span><span class="sh">"</span>

    <span class="n">seg</span> <span class="o">=</span> <span class="n">text</span><span class="p">[</span><span class="n">s</span><span class="p">:</span><span class="n">e</span><span class="p">]</span>
    <span class="n">parts</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span>
        <span class="sa">f</span><span class="sh">'</span><span class="s">&lt;span title=</span><span class="sh">"</span><span class="s">score=</span><span class="si">{</span><span class="nf">float</span><span class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="n">t_idx</span><span class="p">])</span><span class="si">:</span><span class="p">.</span><span class="mi">6</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="s"> </span><span class="sh">'</span>
        <span class="sa">f</span><span class="sh">'</span><span class="s">style=</span><span class="sh">"</span><span class="s">background-color:</span><span class="si">{</span><span class="n">bg</span><span class="si">}</span><span class="s">; border-radius:4px; padding:2px 1px;</span><span class="sh">"</span><span class="s">&gt;</span><span class="sh">'</span>
        <span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">seg</span><span class="si">}</span><span class="sh">'</span>
        <span class="sa">f</span><span class="sh">'</span><span class="s">&lt;/span&gt;</span><span class="sh">'</span>
    <span class="p">)</span>
    <span class="n">cursor</span> <span class="o">=</span> <span class="n">e</span>

<span class="c1"># Ajoute la fin de phrase si besoin
</span><span class="k">if</span> <span class="n">cursor</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">parts</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">text</span><span class="p">[</span><span class="n">cursor</span><span class="p">:])</span>

<span class="n">html</span> <span class="o">=</span> <span class="p">(</span>
    <span class="sh">'</span><span class="s">&lt;div style=</span><span class="sh">"</span><span class="s">font-family: ui-monospace, SFMono-Regular, Menlo, monospace; </span><span class="sh">'</span>
    <span class="sh">'</span><span class="s">line-height:1.8; font-size:14px;</span><span class="sh">"</span><span class="s">&gt;</span><span class="sh">'</span>
    <span class="o">+</span> <span class="sh">''</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">parts</span><span class="p">)</span> <span class="o">+</span>
    <span class="sh">'</span><span class="s">&lt;/div&gt;</span><span class="sh">'</span>
    <span class="sh">'</span><span class="s">&lt;div style=</span><span class="sh">"</span><span class="s">margin-top:8px; font-size:12px; color:#555;</span><span class="sh">"</span><span class="s">&gt;</span><span class="sh">'</span>
    <span class="sh">'</span><span class="s">Plus c’est &lt;b&gt;rouge&lt;/b&gt;, plus le token est important (Saliency). </span><span class="sh">'</span>
    <span class="sh">'</span><span class="s">On peut passer la souris pour voir le score de saillance de chaque token.</span><span class="sh">'</span>
    <span class="sh">'</span><span class="s">&lt;/div&gt;</span><span class="sh">'</span>
<span class="p">)</span>

<span class="nf">display</span><span class="p">(</span><span class="nc">HTML</span><span class="p">(</span><span class="n">html</span><span class="p">))</span>
</code></pre></div></div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainable_deeplearning/example_notam-480.webp 480w,/assets/img/explainable_deeplearning/example_notam-800.webp 800w,/assets/img/explainable_deeplearning/example_notam-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/explainable_deeplearning/example_notam.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h5 id="les-limites-de-cette-approche">Les limites de cette approche</h5> <ul> <li>La valeur absolue du gradient de la prédiction du modèle par rapport aux dimensions de l’embedding du token d’entrée: ça ne permet pas de distinguer les dimensions des tokens qui ont un impact négatifs de ceux qui ont un impact positif sur la prédiction modèle</li> <li>Norme L2 des gradients de chaque dimensions de l’embedding des tokens d’entrée: est-ce une bonne approche? <em>(on peut penser notamment à deux ou plusieurs dimensions qui sont très corrélées, alors leurs composantes de gradient pointent en général dans la même direction, et quand on fait la somme des carrés, ces contributions s’additionnent comme si c’étaient deux informations indépendantes, alors qu’elles portent le même signal, du coup la norme L2 des gradients par rapport à chaque dimension du modèle n’est pas forcément optimale)</em></li> <li>Le gradient de la prédiction par rapport aux tokens d’entrée n’a pas forcément de sens en termes absolu: un gradient petit n’implique pas forcément que le token n’est pas important: un token peut être très influent mais avoir un gradient (local) faible. $\nabla_{x_i}(f(x))$ approxime l’effet de micro-perturbations de l’embedding du token $i$, pas le remplacement de ce token (opération discrète et non-locale). Un token peut être crucial pour la classe, tout en ayant un gradient local faible. En effet, Le gradient est une pente locale. Si, autour du token d’entrée $i$, la fonction “s’aplatit”, alors la pente est proche de 0, donc le gradient local est faible, même si le token $i$ porte une info décisive pour la prédiction.</li> </ul> <p>D’autres approches permettent de combler l’effet “local” du gradient, notamment les approches d’Integrated Gradients, ou de Gradient x Input.</p> <h4 id="exemple-de-lapproche-des-gradients-intégrés-de-la-lib-xplain-de-deel-ai">Exemple de l’approche des <a href="https://github.com/deel-ai/xplique/blob/master/xplique/attributions/integrated_gradients.py">Gradients intégrés de la lib Xplain de deel-ai</a></h4> <p>Dans Xplique, IntegratedGradients accepte un argument baselines dans explain. Si on ne fournit rien, le comportement standard est d’utiliser un tenseur de zéros de même forme que l’entrée (baseline “no-signal”). Sauf que pour le NLP, un tenseur de 0 n’a pas forcément de sens. Une baseline “neutre” serait plutôt par exemple des [PAD] tokens.</p> <p>TODO: tester IG avec 0 et [PAD] tokens</p> <h3 id="classification-dimages">Classification d’images</h3> <h4 id="avec-un-réseau-à-base-de-convolutions-type-resnet">Avec un réseau à base de convolutions (type ResNet)</h4> <p>Resnet-18 entraîné sur http://cs231n.stanford.edu/tiny-imagenet-200.zip</p> <ul> <li>GradCam de la librairie xplique : https://github.com/deel-ai/xplique/blob/master/xplique/attributions/grad_cam.py</li> <li>SmoothGrad</li> </ul> <h4 id="avec-un-vit">Avec un ViT</h4> <h3 id="exemple-pour-une-entrée-multimodale-texte--image">Exemple pour une entrée multimodale (texte + image)</h3> <p>TODO - cas layoutlmv3</p> <h2 id="explication-de-la-génération-du-modèle-par-attribution-de-sa-génération-aux-données-dentraînements-quil-a-vu-qui-ont-positivement-ou-négativement-influencées-sa-prédiction">Explication de la génération du modèle par attribution de sa génération aux données d’entraînements qu’il a vu qui ont positivement ou négativement influencées sa prédiction</h2> <p>Si on cherche à estimer l’<mark>impact qu'aurait un exemple d'entraînement sur la perte d'un exemple de test (ou sur plusieurs résultats du modèle sur un jeu de données test) à un exemple d'entraînement</mark> (qu’il soit dans le jeu de données d’entraînement de base ou pas), on peut utiliser les fonctions d’influence.</p> \[\mathrm{Influence}\bigl(z_{\mathrm{train}}\to z_{\mathrm{test}}\bigr) = \frac{d}{d\varepsilon}\, \mathcal{L}\bigl(z_{\rm test},\,\theta_\varepsilon(z_\text{train})\bigr)\Big|_{\varepsilon=0} = -\nabla_\theta \,\mathcal{L}\bigl(z_{\mathrm{test}},\,\hat{\theta} \bigr)\,H_\theta^{-1}(\hat{\theta})\,\nabla_\theta \mathcal{L}(z_{\rm train},\hat{\theta})\] \[\mathrm{Influence}\bigl(z_{\mathrm{train}}\to f(x)\bigr) = \left.\frac{d}{d\varepsilon}\bigl(f_{\theta_{\varepsilon}(z_{\mathrm{train}})}(x)\bigr)\right|_{\varepsilon=0} = - \nabla_\theta f_{\hat{\theta}}(x)^\top \, H_\theta(\hat{\theta})^{-1} \, \nabla_\theta \mathcal{L}\bigl(x_{\mathrm{train}},\hat{\theta}\bigr)\] <p>Pour voir l’explication de la formule et plus de détails sur comment c’est utilisé dans les LLMs, vous pouvez aller voir <a href="camillebrl.github.io/blog/2025/influence_functions_applied_to_llms/">ce post</a>.</p> <p>Une des librairies possibles pour calculer l’influence est <a href="https://github.com/deel-ai/influenciae">Influenciae de Deel-AI</a>. TODO</p> <h2 id="explication-de-la-génération-du-modèle-par-la-façon-dont-le-modèle-couche-par-couche-neurone-par-neurone-etc-a-traité-lentrée-pour-effectuer-sa-prédiction-quels-concepts-intermédiaires-a-t-il-représenté-etc">Explication de la génération du modèle par la façon dont le modèle (couche par couche, neurone par neurone, etc) a traité l’entrée pour effectuer sa prédiction (quels concepts intermédiaires a-t-il représenté, etc)</h2> <p>Une des librairies qui explique les concepts d’un modèle de vision est <a href="https://github.com/deel-ai/Craft">Craft de Deel-AI</a> TODO</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="XAI,"/><category term="robustesse,"/><category term="attribution"/><summary type="html"><![CDATA[Explcabilité des modèles de deep learning]]></summary></entry><entry><title type="html">Tips mathématiques / points utiles en IA</title><link href="https://camillebrl.github.io/blog/2025/tips_mathematics_for_ai/" rel="alternate" type="text/html" title="Tips mathématiques / points utiles en IA"/><published>2025-07-25T07:00:00+00:00</published><updated>2025-07-25T07:00:00+00:00</updated><id>https://camillebrl.github.io/blog/2025/tips_mathematics_for_ai</id><content type="html" xml:base="https://camillebrl.github.io/blog/2025/tips_mathematics_for_ai/"><![CDATA[<h1 id="introduction">Introduction</h1> <blockquote> <p>Dans ce post, je vais mettre pas mal de <mark>rappels mathématiques</mark> (je reviens aux bases des bases, mais j’ai encore besoin de revenir à ces bases des choses pour comprendre certains concepts…) <mark>qui m'ont été utiles pour comprendre certains papiers de recherche ou débloquer certains points dans mes recherches</mark>.</p> </blockquote> <h1 id="entropie-information-mutuelle-cross-entropie">Entropie, information mutuelle, cross-entropie</h1> <ul> <li>L’entropie quantifie l’incertitude moyenne d’une variable aléatoire, ie la quantité d’information qu’elle contient: $H(X)=\mathbb{E}[-\log p(X)]$ (discret) et $h(X)=-\int f\log f$ (continu). Elle est maximale pour l’uniforme, nulle pour une variable déterministe ; le conditionnement ne l’augmente pas. Elle est au cœur du codage, de l’inférence statistique (cross-entropie, KL) et de l’information mutuelle. En continu, elle n’est pas invariante par changement d’échelle ; préférer souvent $I$, $D_{\mathrm{KL}}$ pour des comparaisons robustes.</li> <li>La cross-entropie entre une “vraie” loi $P$ et un modèle $Q$ est définie par $H(P,Q)=-\sum_x P(x)\log Q(x)$ (ou $\int P\log Q$ au continu). Elle mesure le coût moyen pour coder des échantillons tirés de $P$ avec un code optimal pour $Q$, et vérifie l’identité clé $H(P,Q)=H(P)+D_{\mathrm{KL}}(P\Vert Q)\ge H(P)$, avec minimum atteint quand $Q=P$. En apprentissage supervisé, la NLL (negative log-likelihood) utilisée pour la classification est exactement une cross-entropie entre les étiquettes (one-hot) et les probabilités du modèle (softmax).</li> <li>L’information mutuelle quantifie la dépendance entre deux variables : $I(X;Y)=H(X)-H(X\mid Y)=H(Y)-H(Y\mid X)=H(X)+H(Y)-H(X,Y)$. Elle s’écrit aussi comme une divergence de KL entre la conjointe et le produit des marginales : $I(X;Y)=D_{\mathrm{KL}}!\big(P_{XY}\Vert P_XP_Y\big)\ge 0$. Elle est nulle <strong>ssi</strong> $X$ et $Y$ sont indépendantes, est <strong>symétrique</strong>, et obéit à l’inégalité de traitement de l’information (si $X!\to!Y!\to!Z$, alors $I(X;Z)\le I(X;Y)$).</li> <li> <table> <tbody> <tr> <td>La Divergence de Kullback–Leibler (KL) est une métrique de “distance” directionnelle entre distributions non symétriques. Cette métrique permet de relier information mutuelle et cross-entropie ($\mathrm{D_{KL}}(P</td> <td>Q)=\sum_x P(x)\log\frac{P(x)}{Q(x)}\ \ (\ge 0)$ ; $=0$ ssi $P=Q$)</td> </tr> </tbody> </table> </li> </ul> <h2 id="lentropie">L’entropie</h2> <p>L’entropie mesure l’<strong>incertitude</strong> (ou le <strong>désordre</strong>) d’une variable aléatoire. Plus une variable est imprévisible, plus son entropie est grande. On peut aussi la voir comme la quantité moyenne d’<strong>information</strong> (ou de <strong>surprise</strong>) révélée par l’observation de la variable.</p> <h3 id="dans-le-cas-discret">Dans le cas discret</h3> <p>Soit $X$ une variable aléatoire discrète à valeurs dans un ensemble fini $\mathcal{X}$, de loi $p(x)=\Pr[X=x]$. Pour une base de logarithme fixée (base $2$ pour les <em>bits</em>, base $e$ pour les <em>nats</em>), l’entropie de $X$ est \(H(X) \;=\; - \sum_{x\in\mathcal{X}} p(x)\,\log p(x) \;=\; \mathbb{E}\!\left[-\log p(X)\right].\) Le terme $-\log p(x)$ est l’<strong>information de Shannon</strong> (ou <strong>surprise</strong>) de l’issue $x$.</p> <p><strong>Unités.</strong> Avec $\log_2$, $H(X)$ est en <strong>bits</strong> ; avec $\log$, en <strong>nats</strong> ; avec $\log_{10}$, en <strong>hartleys</strong>.</p> <h4 id="propriétés">Propriétés</h4> <ul> <li><strong>Bornes.</strong> $0 \le H(X) \le \log |\mathcal{X}|$.<br/> $H(X)=0$ ssi $X$ est déterministe (toute la masse sur une seule valeur).<br/> $H(X)$ est maximale et vaut $\log|\mathcal{X}|$ si $X$ est <strong>uniforme</strong> sur $\mathcal{X}$.</li> <li><strong>Concavité.</strong> $H(\cdot)$ est concave en la loi $p$ de $X$.</li> <li><strong>Transformation déterministe.</strong> Pour toute fonction déterministe $g$, $H(g(X)) \le H(X)$, avec égalité si $g$ est bijective sur le support.</li> <li><strong>Sous-additivité et indépendance.</strong> Pour $(X,Y)$ discrets, \(H(X,Y) = H(X\mid Y) + H(Y) = H(Y\mid X) + H(X).\) Si $X \perp Y$ (indépendants), alors $H(X,Y)=H(X)+H(Y)$.</li> <li><strong>Le conditionnement réduit l’entropie.</strong> $H(X\mid Y) \le H(X)$, avec égalité ssi $X$ et $Y$ sont indépendants.</li> </ul> <h4 id="exemples">Exemples</h4> <p><strong>Bernoulli.</strong> Si $X\sim\mathrm{Bernoulli}(p)$, \(H(X) = -\,p\log p - (1-p)\log(1-p),\) maximisée en $p=\tfrac12$ (vaut $1$ bit en base $2$) et nulle en $p\in{0,1}$.</p> <p><strong>Uniforme.</strong> Si $X$ est uniforme sur $n$ symboles, $H(X)=\log n$.</p> <h3 id="dans-le-cas-continu-entropie-différentielle">Dans le cas continu: entropie différentielle</h3> <p>Si $X$ est réelle (densité $f$), l’<strong>entropie différentielle</strong> est \(h(X) \;=\; - \int f(x)\,\log f(x)\,dx.\)</p> <p><strong>Attention :</strong> $h(X)$ peut être <strong>négative</strong> et <strong>n’est pas invariante par changement d’échelle</strong> : \(h(aX) = h(X) + \log|a| \quad (a\neq 0).\) En revanche, des quantités dérivées comme l’information mutuelle et la divergence KL restent bien définies et invariantes de coordonnées.</p> <p><strong>Exemple gaussien.</strong> Si $X\sim\mathcal{N}(\mu,\sigma^2)$, \(h(X) = \tfrac12 \log\!\big(2\pi e\,\sigma^2\big).\) En dimension $d$, pour $X\sim\mathcal{N}(\mu,\Sigma)$, \(h(X) = \tfrac12 \log\!\big((2\pi e)^d \det \Sigma\big).\)</p> <h3 id="estimer-lentropie">Estimer l’entropie</h3> <ul> <li><strong>Estimateur plug-in (discret).</strong> Remplacer $p(x)$ par la fréquence empirique $\hat{p}(x)$ : \(\widehat{H(X)} = -\sum_x \hat{p}(x)\log \hat{p}(x)\) (biais négatif pour petits échantillons ; corrections type Miller–Madow existent).</li> <li><strong>Continu.</strong> Estimateurs à noyau, $k$-plus proches voisins (Kozachenko–Leonenko), ou via modèles paramétriques/neuraux (flows, VAEs) pour approcher $f$ ou $D_{\mathrm{KL}}$.</li> </ul> <h2 id="principe-du-maximum-dentropie">Principe du maximum d’entropie</h2> <p>Parmi toutes les lois satisfaisant des contraintes (p. ex. support, moyenne, variance), la loi à entropie maximale est la moins informative (au sens de Shannon) :</p> <ul> <li>Support fini $\Rightarrow$ uniforme.</li> <li>Support $[0,\infty)$ et contrainte de moyenne $\Rightarrow$ exponentielle.</li> <li>Contrainte de moyenne et variance en $\mathbb{R}$ $\Rightarrow$ gaussienne.</li> </ul> <h2 id="lentropie-conditionnelle-et-linformation-mutuelle">L’Entropie conditionnelle et l’information mutuelle</h2> <p>L’entropie conditionnelle de $X$ sachant $Y$ est \(H(X\mid Y) \;=\; \mathbb{E}_Y\big[H(X\mid Y=y)\big] \;=\; -\,\sum_{x,y} p(x,y)\,\log p(x\mid y).\) L’<strong>information mutuelle</strong> mesure la réduction d’incertitude sur $X$ due à la connaissance de $Y$ : \(I(X;Y) \;=\; H(X) - H(X\mid Y) \;=\; H(Y) - H(Y\mid X) \;=\; H(X)+H(Y)-H(X,Y) \;\ge 0.\) <strong>Inégalité de traitement de l’information (data processing).</strong> Si $X \to Y \to Z$ forme une chaîne de Markov, alors $I(X;Z)\le I(X;Y)$.</p> <h2 id="la-cross-entropie-et-la-divergence-de-kullbackleibler">La Cross-entropie et la divergence de Kullback–Leibler</h2> <p>Pour deux lois $P$ et $Q$ sur le même alphabet,</p> <ul> <li><strong>Cross-entropie :</strong> $H(P,Q) = -\sum_x P(x)\log Q(x)$,</li> <li><strong>Divergence KL :</strong> $D_{\mathrm{KL}}(P\Vert Q) = \sum_x P(x)\log\frac{P(x)}{Q(x)} \ge 0$,</li> </ul> <p>et l’identité clé : \(H(P,Q) = H(P) + D_{\mathrm{KL}}(P\Vert Q).\) En apprentissage, minimiser la log-vraisemblance négative revient souvent à minimiser une cross-entropie (donc une KL) entre la vraie loi $P$ et le modèle $Q_\theta$.</p> <h2 id="lien-avec-le-codage-théorème-source-de-shannon">Lien avec le codage (théorème source de Shannon)</h2> <p>Pour une source i.i.d. $X$, la longueur moyenne minimale $\bar{\ell}^\star$ d’un code préfixe vérifie \(H(X) \;\le\; \bar{\ell}^\star \;&lt;\; H(X) + 1 \quad \text{(en bits/symbole)}.\) L’entropie est donc une borne fondamentale sur le <strong>taux de compression</strong>.</p> <h1 id="svd-acp">SVD, ACP</h1> <ul> <li>Données : matrice centrée $X \in \mathbb{R}^{n \times d}$, avec $n$ observations (lignes) et $d$ variables (colonnes).<br/> On centre toujours : $X = \tilde{X} - \mathbf{1}\mu^\top$, où $\mu \in \mathbb{R}^{d}$ est la moyenne colonne et $\mathbf{1}$ le vecteur de 1.</li> <li>Covariance empirique : $\displaystyle S = \frac{1}{n-1} X^\top X \in \mathbb{R}^{d \times d}$.</li> </ul> <h2 id="svd--décomposition-en-valeurs-singulières">SVD : décomposition en valeurs singulières</h2> <p>La décomposition en valeurs singulières (SVD) factorise toute matrice réelle $X \in \mathbb{R}^{n\times d}$ en $X = U\Sigma V^\top$, où $U \in \mathbb{R}^{n\times r}$ et $V \in \mathbb{R}^{d\times r}$ ont des colonnes orthonormées (vecteurs singuliers gauche et droit), $\Sigma = \mathrm{diag}(\sigma_1 \ge \cdots \ge \sigma_r &gt; 0)$ contient les valeurs singulières, et $r = \mathrm{rang}(X)$. On a $X^\top X = V\Sigma^2 V^\top$ et $X X^\top = U\Sigma^2 U^\top$, d’où le lien avec les décompositions spectrales. La SVD fournit la <strong>meilleure approximation de rang $k$</strong> au sens de la norme de Frobenius : $X_k = U_k \Sigma_k V_k^\top$ minimise $|X - Y|<em>F$ sur toutes les matrices $Y$ de rang $\le k$ (théorème d’Eckart–Young–Mirsky), avec erreur $|X - X_k|_F^2 = \sum</em>{i&gt;k} \sigma_i^2$. Elle est utilisée pour le débruitage, la compression et le calcul de l’ACP (où $V$ donne les chargements et $\sigma_i^2/(n-1)$ les variances expliquées), et reste numé</p> <p>La SVD (Singular Value Decomposition) de $X$ est : \(X = U \Sigma V^\top,\quad U \in \mathbb{R}^{n \times r},\; V \in \mathbb{R}^{d \times r},\; \Sigma=\mathrm{diag}(\sigma_1,\dots,\sigma_r),\; r=\mathrm{rang}(X).\)</p> <ul> <li>$U$ : vecteurs singuliers à gauche (orthonormés),</li> <li>$V$ : vecteurs singuliers à droite (orthonormés),</li> <li>$\sigma_1 \ge \sigma_2 \ge \cdots \ge \sigma_r &gt; 0$ : valeurs singulières.</li> </ul> <h3 id="lien-avec-la-covariance">Lien avec la covariance</h3> <p>\(X^\top X = V \Sigma^2 V^\top \quad\Rightarrow\quad S = \frac{1}{n-1} X^\top X = V \left(\frac{\Sigma^2}{\,n-1\,}\right) V^\top.\) Donc :</p> <ul> <li><strong>Vecteurs propres de $S$</strong> $=$ <strong>vecteurs singuliers droits</strong> $V$.</li> <li><strong>Valeurs propres</strong> $\lambda_k = \sigma_k^2/(n-1)$.</li> </ul> <h2 id="lacp">L’ACP</h2> <p>L’Analyse en Composantes Principales (ACP) est une méthode linéaire de réduction de dimension qui projette des données centrées $X$ sur des directions orthogonales — les <strong>composantes principales</strong> — choisies pour maximiser la variance expliquée. On diagonalise la matrice de covariance $S=\tfrac{1}{n-1}X^\top X$ (ou on calcule la SVD $X=U\Sigma V^\top$) : les vecteurs propres de $S$ (colonnes de $V$) sont les <strong>chargements</strong>, et les <strong>scores</strong> sont $T=XV=U\Sigma$. En ne conservant que $k$ composantes associées aux plus grandes valeurs propres $\lambda_i=\sigma_i^2/(n-1)$, on obtient une représentation à $k$ dimensions qui minimise l’erreur quadratique de reconstruction (théorème d’Eckart–Young) avec une <strong>variance expliquée</strong> cumulée $\sum_{i=1}^k \lambda_i \big/ \sum_j \lambda_j$. L’ACP sert à visualiser, compresser et débruiter des données et se pratique souvent après <strong>standardisation</strong> des variables si leurs échelles diffèrent fortement.</p> <p>Il y a 3 façons de voir l’ACP:</p> <h3 id="1-maximisation-de-variance-optimisation-quadratique">1) Maximisation de variance (optimisation quadratique)</h3> <p>La première composante principale est la direction unitaire $v_1 \in \mathbb{R}^{d}$ qui maximise la variance projetée : \(v_1 \;=\; \arg\max_{\|v\|_2=1}\; \mathrm{Var}(X v) \;=\; \arg\max_{\|v\|_2=1}\; v^\top S\, v.\) Par multiplicateurs de Lagrange, on obtient l’équation aux valeurs propres : \(S v_1 \;=\; \lambda_1 v_1,\quad \lambda_1=\max \text{ eigenvalue}.\) Les composantes suivantes $v_k$ se construisent de manière itérative sous contrainte d’orthogonalité $(v_k^\top v_j=0, j&lt;k)$, donnant les $d$ vecteurs propres de $S$ rangés par $\lambda_1 \ge \lambda_2 \ge \cdots \ge \lambda_d \ge 0$.</p> <p><strong>Scores (coordonnées factorielles)</strong> : $t_k = X v_k \in \mathbb{R}^{n}$.<br/> <strong>Chargements</strong> : colonnes de $V=[v_1,\dots,v_d]$.</p> <h3 id="2-meilleure-approximation-de-rang-k-moindres-carrés">2) Meilleure approximation de rang $k$ (moindres carrés)</h3> <p>L’ACP de rang $k$ cherche une approximation de $X$ par une matrice de rang $k$ : \(\min_{\substack{W \in \mathbb{R}^{d \times k},\, H \in \mathbb{R}^{n \times k}\\ W^\top W = I_k}} \; \|X - H W^\top\|_F^2.\) La solution optimale est obtenue en prenant $W=V_k$ (les $k$ premiers vecteurs propres de $S$) et $H = X V_k$.<br/> Cette vue mène au théorème d’Eckart–Young–Mirsky (voir SVD ci-dessous).</p> <h3 id="3-diagonalisation-de-la-covariance-vue-statistique">3) Diagonalisation de la covariance (vue statistique)</h3> <p>En décomposant $S$ : \(S = V \Lambda V^\top,\quad \Lambda=\mathrm{diag}(\lambda_1,\dots,\lambda_d),\) la projection $Z = X V$ a covariance diagonale : \(\frac{1}{n-1} Z^\top Z = \Lambda,\) c’est-à-dire des <strong>composantes non corrélées</strong> dont les variances expliquées sont $\lambda_k$.</p> <h3 id="choix-du-nombre-de-composantes-k">Choix du nombre de composantes $k$</h3> <ul> <li><strong>Cumul de variance expliquée</strong> : choisir le plus petit $k$ tel que $\sum_{i=1}^k \sigma_i^2 / \sum_{j=1}^r \sigma_j^2 \ge \tau$ (p.ex. $\tau=0{,}90$ ou $0{,}95$).</li> <li><strong>Coude</strong> (scree plot) sur $\sigma_i^2$.</li> <li><strong>Validation croisée</strong> sur l’erreur de reconstruction ou la performance aval.</li> </ul> <h3 id="reconstruction--projection">Reconstruction &amp; projection</h3> <ul> <li><strong>Projection (scores)</strong> : $T_k = X V_k$.</li> <li><strong>Reconstruction</strong> : $\widehat{X} = T_k V_k^\top = U_k \Sigma_k V_k^\top$.</li> <li><strong>Hors échantillon</strong> : pour un nouveau $x$, centrer $x_c=x-\mu$, puis $t = x_c^\top V_k$ et $\hat{x}= \mu + V_k t$.</li> </ul> <h3 id="centrage-standardisation-et-acp-sur-corrélations">Centrage, standardisation et ACP sur corrélations</h3> <ul> <li><strong>Centrage indispensable</strong> : sinon la 1ʳᵉ CP peut simplement « pointer » la moyenne.</li> <li><strong>Standardisation</strong> (PCA sur la matrice de corrélation) si les échelles des variables diffèrent fortement : travailler sur \(X_{\text{std}} = (X - \mathbf{1}\mu^\top) D^{-1},\) où $D$ contient les écarts-types des colonnes, puis appliquer ACP/SVD à $X_{\text{std}}$.</li> </ul> <h3 id="blanchiment-whitening">Blanchiment (whitening)</h3> <p>L’ACP fournit une base orthonormée où les variances sont $\lambda_k$. Le <strong>whitening</strong> met ces variances à 1 : \(X_{\text{white}} = X V \Lambda^{-1/2} = X V \left(\frac{\Sigma}{\sqrt{n-1}}\right)^{-1} = \sqrt{n-1}\, U.\) Ainsi, $\frac{1}{n-1} X_{\text{white}}^\top X_{\text{white}} = I$.</p> <h3 id="propriétés-et-détails-numériques">Propriétés et détails numériques</h3> <ul> <li><strong>Rang</strong> : $\mathrm{rang}(X) \le \min(n-1,d)$ (le centrage retire au plus un degré de liberté).</li> <li><strong>Stabilité</strong> : calculer l’ACP via la SVD de $X$ est numériquement plus stable que l’EVD de $S$.</li> <li><strong>Cas $n \ll d$ ou $d \ll n$</strong> : utiliser la SVD tronquée (méthodes itératives, p.ex. Lanczos/power method).</li> <li><strong>Indétermination de signe</strong> : $v_k$ et $-v_k$ sont équivalents (mêmes sous-espaces).</li> </ul> <h3 id="acp-noyau-kernel-pca--en-bref">ACP noyau (kernel PCA) — en bref</h3> <p>Remplace la projection linéaire par un <strong>noyau</strong> $K$ (centré en RKHS) et diagonalise $K$ (taille $n\times n$) pour capturer des <strong>variations non linéaires</strong>. Les scores sont obtenus via combinaisons des noyaux, sans construire explicitement la base de caractéristiques.</p> <h2 id="lien-acp--svd-formules-utiles">Lien ACP ↔ SVD (formules utiles)</h2> <p>Si $X$ est <strong>centrée</strong> :</p> <ul> <li><strong>Chargements ACP</strong> : $V = [v_1,\dots,v_r]$ (vecteurs singuliers droits).</li> <li><strong>Scores ACP</strong> : $T = X V = U \Sigma$ (chaque colonne $t_k = \sigma_k\, u_k$).</li> <li><strong>Variance expliquée</strong> : \(\text{EVR}_k = \frac{\lambda_k}{\sum_{j=1}^r \lambda_j} = \frac{\sigma_k^2}{\sum_{j=1}^r \sigma_j^2}.\)</li> <li><strong>Approximation de rang $k$</strong> (Eckart–Young–Mirsky, meilleure au sens $|\cdot|_F$) : \(X_k = U_k \Sigma_k V_k^\top \quad\text{et}\quad \|X - X_k\|_F^2 = \sum_{i=k+1}^{r} \sigma_i^2.\)</li> </ul> <h1 id="rang-dune-matrice-permet-de-déterminer-si-un-espace-est-sur--sous-dimensionné">Rang d’une matrice: permet de déterminer si un espace est sur / sous dimensionné</h1> <p>Soit $A \in \mathbb{R}^{n \times d}$ et $r=\mathrm{rang}(A)$.</p> <ul> <li><strong>Espace image (colonnes)</strong> : $ \mathrm{Im}(A)={A x : x\in\mathbb{R}^d} \subseteq \mathbb{R}^n$. \(\boxed{\;\mathrm{rang}(A)=\dim(\mathrm{Im}(A))=\dim(\mathrm{span}\{\text{colonnes de }A\})\;}\)</li> <li><strong>Rang des lignes</strong> : $ \mathrm{rang}(A)=\dim(\mathrm{span}{\text{lignes de }A})$ (rang des colonnes = rang des lignes).</li> <li><strong>Mineurs</strong> : $ \mathrm{rang}(A)$ est l’ordre maximal $k$ tel qu’il existe un mineur $k\times k$ de déterminant non nul.</li> <li><strong>SVD</strong> : si $A=U\Sigma V^\top$ avec $\Sigma=\mathrm{diag}(\sigma_1\ge\cdots\ge\sigma_r&gt;0)$, \(\boxed{\;\mathrm{rang}(A)=\#\{i:\sigma_i&gt;0\}\;}\)</li> </ul> <p>Bornes : $\;0 \le r \le \min(n,d)$.</p> <h2 id="théorème-du-rang-rang-noyau">Théorème du rang (rang-noyau)</h2> <p>Le <strong>noyau</strong> (espace des solutions homogènes) est $\ker(A)={x\in\mathbb{R}^d: Ax=0}$. \(\boxed{\; \dim(\ker(A)) + \mathrm{rang}(A) = d \;}\)</p> <ul> <li>$\dim(\ker(A)) = d-r$ s’appelle parfois la <strong>nullité</strong>.</li> <li>Interprétation : chaque dépendance linéaire entre colonnes ajoute une dimension au noyau.</li> </ul> <h2 id="indépendance-linéaire-et-base">Indépendance linéaire et base</h2> <p>Pour des vecteurs $a_1,\dots,a_m \in \mathbb{R}^d$, posons $A=[a_1~\cdots~a_m]\in\mathbb{R}^{d\times m}$.</p> <ul> <li>$ \mathrm{rang}(A) = $ <strong>nombre maximal de vecteurs linéairement indépendants</strong> parmi $a_1,\dots,a_m$.</li> <li>Si $ \mathrm{rang}(A)=m\le d$, la famille est libre.</li> <li>Si $ \mathrm{rang}(A)&lt;m$, la famille est redondante (sur-paramétrée) pour décrire son sous-espace vectoriel.</li> </ul> <h2 id="systèmes-linéaires-axb-et-sursous-détermination">Systèmes linéaires $Ax=b$ et (sur/sous)-détermination</h2> <ul> <li><strong>Existence (compatibilité)</strong> : le système $Ax=b$ a une solution ssi \(\mathrm{rang}(A) = \mathrm{rang}([A~|~b]).\)</li> <li><strong>Unicité</strong> : si une solution existe, elle est <strong>unique</strong> ssi $\ker(A)={0}$, i.e. $\mathrm{rang}(A)=d$ (colonnes libres).</li> </ul> <h3 id="cas-selon-la-forme-de-a-avec-ainmathbbrntimes-d">Cas selon la forme de $A$ (avec $A\in\mathbb{R}^{n\times d}$)</h3> <ol> <li><strong>Sur-déterminé (plus d’équations que d’inconnues)</strong> : $n&gt;d$. <ul> <li>Si $ \mathrm{rang}(A)=d$ (<strong>plein rang colonne</strong>), l’égalité exacte $Ax=b$ n’est pas garantie, mais la <strong>moindre carrés</strong> a solution unique : \(x^\star = (A^\top A)^{-1}A^\top b.\)</li> <li>Si $ \mathrm{rang}(A)&lt;d$, colonnes dépendantes $\Rightarrow$ solution LS <strong>non unique</strong> (ill-posée).</li> </ul> </li> <li><strong>Sous-déterminé (plus d’inconnues que d’équations)</strong> : $d&gt;n$. <ul> <li>Si $ \mathrm{rang}(A)=n$ (<strong>plein rang ligne</strong>), il y a soit <strong>infinité de solutions</strong> (si compatible), la solution <strong>de norme minimale</strong> est \(x^\star = A^\top (AA^\top)^{-1} b.\)</li> <li>Si $ \mathrm{rang}(A)&lt;n$, encore plus de liberté (noyau plus grand).</li> </ul> </li> <li><strong>Carré</strong> : $n=d$. <ul> <li>$ \mathrm{rang}(A)=n$ $\Leftrightarrow$ $A$ <strong>inversible</strong>, $\det(A)\neq 0$, $\sigma_i&gt;0$ $\forall i$.</li> <li>Sinon, $A$ est <strong>singulière</strong> : pas d’unicité (noyau non trivial) et potentielle incompatibilité.</li> </ul> </li> </ol> <h2 id="diagnostic--sursous-dimensionné--via-le-rang">Diagnostic « sur/sous-dimensionné » via le rang</h2> <h3 id="familles-de-vecteurs-dans-mathbbrd">Familles de vecteurs dans $\mathbb{R}^d$</h3> <ul> <li>Vous avez $m$ vecteurs. <ul> <li>Si $\mathrm{rang}(A)=m&lt;d$ : la famille est <strong>sous-dimensionnée pour $\mathbb{R}^d$</strong> (elle ne peut pas engendrer tout $\mathbb{R}^d$), mais <strong>bien dimensionnée</strong> pour son sous-espace de dimension $m$.</li> <li>Si $\mathrm{rang}(A)&lt;m$ : la famille est <strong>sur-dimensionnée</strong> pour son sous-espace (des vecteurs redondants).</li> <li>Pour obtenir une <strong>base</strong> de $\mathbb{R}^d$, il faut et il suffit d’avoir $\mathrm{rang}(A)=d$ avec $m\ge d$; une base minimale a $m=d$.</li> </ul> </li> </ul> <h3 id="données-n-times-d-et-colinéarités">Données $(n \times d)$ et colinéarités</h3> <p>Considérez une matrice de données $X\in\mathbb{R}^{n\times d}$ (lignes = observations, colonnes = variables).</p> <ul> <li>Si $\mathrm{rang}(X)=d$ et $n\ge d$ : les variables sont <strong>non redondantes</strong> (pas de colinéarité parfaite).</li> <li>Si $\mathrm{rang}(X)&lt;d$ : variables <strong>redondantes</strong> $\Rightarrow$ espace de caractéristiques <strong>sur-dimensionné</strong> par rapport à l’information disponible; on peut réduire la dimension (p.ex. ACP/SVD).</li> <li>Si $d \gg n$ : même avec $\mathrm{rang}(X)=n$, l’espace des variables est <strong>sous-contraint</strong> par les données (risque de sur-apprentissage) ; régularisation ou réduction de dimension sont recommandées.</li> </ul> <h2 id="outils-de-calcul-et-liens-utiles">Outils de calcul et liens utiles</h2> <ul> <li><strong>RREF / pivots</strong> : le nombre de pivots (après élimination de Gauss) = $ \mathrm{rang}(A)$.</li> <li><strong>SVD</strong> : $\mathrm{rang}(A)=#{ \sigma_i &gt; 0}$.</li> <li><strong>Pseudo-inverse de Moore–Penrose</strong> $A^+$ (via SVD) : \(A^+ = V\,\Sigma^+\,U^\top,\quad \Sigma^+=\mathrm{diag}\Big(\tfrac{1}{\sigma_i}\mathbf{1}_{\sigma_i&gt;0}\Big),\) donne les solutions LS minimales en norme : $x^\star=A^+ b$.</li> </ul> <h1 id="information-de-fisher-et-bornes-de-cramer-rao">Information de Fisher et Bornes de Cramer-Rao</h1> <p>L’information de Fisher d’un modèle paramétrique $p_\theta(y)$ est la matrice $I(\theta)=\mathbb{E}<em>{y\sim p</em>\theta}!\big[\nabla_\theta \log p_\theta(y)\,\nabla_\theta \log p_\theta(y)^\top\big]=-\mathbb{E}<em>{y\sim p</em>\theta}!\big[\nabla_\theta^2 \log p_\theta(y)\big]$ (sous conditions de régularité) ; elle quantifie la quantité d’information sur $\theta$ contenue dans une observation et mesure la courbure moyenne de la log-vraisemblance. Pour $n$ échantillons i.i.d., $I_n(\theta)=n\,I(\theta)$, ce qui induit la borne de Cramér–Rao $\mathrm{Cov}(\hat\theta)\succeq I_n(\theta)^{-1}$ pour tout estimateur sans biais et l’optimalité asymptotique du MLE : $\sqrt{n}(\hat\theta-\theta^\star)\Rightarrow \mathcal{N}!\big(0,\,I(\theta^\star)^{-1}\big)$. On distingue la <strong>Fisher observée</strong> $-\nabla_\theta^2 \log L(\theta)$ évaluée en $\hat\theta$ (avec $L$ la vraisemblance) et la <strong>Fisher attendue</strong> (son espérance). En apprentissage, on minimise la NLL $\mathcal{L}(\theta)=-\sum_i \log p_\theta(y_i)$, égale à la cross-entropie dans les modèles de langue ; au minimum $\hat\theta$, la Hessienne $\nabla_\theta^2 \mathcal{L}(\hat\theta)$ coïncide en moyenne avec $I_n(\hat\theta)$, d’où l’interprétation de la Fisher comme “courbure de la loss”. Enfin, l’approximation de second ordre de la divergence de KL donne $\mathrm{KL}\big(p_\theta\,|\,p_{\theta+\delta}\big)\approx \tfrac{1}{2}\,\delta^\top I(\theta)\,\delta$, plaçant $I(\theta)$ au cœur de la <strong>natural gradient</strong> et de la géométrie de l’information.</p> <p>A noter que dans les modèles de langue, c’est l’information de Fisher qui est utilisée comme loss (Trouver les $theta$ qui maximisent les probabilités des observations (log-likelihood), ou minimiser la negative log-likelihood).</p> <h1 id="jacobienne">Jacobienne</h1> <p>Soit $X\in\mathbb{R}^d$ à densité $f_X$ et une application $T:\mathbb{R}^d\to\mathbb{R}^d$ de classe $C^1$ inversible presque partout, avec matrice jacobienne $J_T(x)=\big[\partial T_i/\partial x_j\big]<em>{i,j}$ et déterminant $\det J_T(x)$. Pour $Y=T(X)$, la densité de $Y$ est le <strong>pushforward</strong> de $f_X$ par $T$ : $f_Y(y)=f_X!\big(T^{-1}(y)\big)\,\big|\det J</em>{T^{-1}}(y)\big|=f_X(x)\,/\,\big|\det J_T(x)\big|$ où $x=T^{-1}(y)$. Cette formule découle de la conservation de la masse $\int_{A} f_Y(y)\,dy=\int_{T^{-1}(A)} f_X(x)\,dx$ et du fait que $|\det J_T(x)|$ mesure la dilatation locale de volume (la valeur absolue assure des densités positives, l’orientation n’ayant pas d’effet probabiliste). En dimension 1, pour une transformation monotone $Y=g(X)$, $f_Y(y)=f_X!\big(g^{-1}(y)\big)\,\big|\tfrac{d}{dy}g^{-1}(y)\big|$; si $g$ est décroissante la dérivée est négative mais la valeur absolue corrige le signe. En cas de transformation <strong>non injective</strong> mais $C^1$ par morceaux (par ex. $T$ $k$-à-1 sur une partition), on somme sur les antécédents : $f_Y(y)=\sum_{x\in T^{-1}({y})} f_X(x)\,/\,\big|\det J_T(x)\big|$. Les espérances suivent la même règle : pour toute $\varphi$ intégrable, $\mathbb{E}[\varphi(Y)]=\int \varphi!\big(T(x)\big)f_X(x)\,dx=\int \varphi(y)\,f_Y(y)\,dy$. Si $\det J_T(x)=0$ sur un ensemble de mesure non négligeable, l’image peut être <strong>singulière</strong> (masse portée par une variété de dimension $&lt;d$) et $Y$ peut ne pas admettre de densité Lebesgue; sinon, dans le cas classique (diffeomorphisme a.e.), la jacobienne fournit la relation fondamentale de changement de variables en probabilité. <em>Ex. compact : coordonnées polaires</em> $T(r,\theta)=(r\cos\theta,r\sin\theta)$ sur $\mathbb{R}^2\setminus{0}$ ont $|\det J_T|=r$, d’où $f_{X,Y}(x,y)=f_{R,\Theta}(r,\theta)/r$ avec $r=\sqrt{x^2+y^2}$, $\theta=\mathrm{atan2}(y,x)$.</p> <h1 id="transformation-dun-problème-complexe-à-un-problème-convexe">Transformation d’un problème complexe à un problème convexe</h1> <p><strong>Transformer un problème complexe en problème convexe</strong> consiste à remplacer un programme non convexe $\min_{x\in\mathcal{X}} f(x)$ (où $f$ ou $\mathcal{X}$ est non convexe) par un <strong>relaxé convexe</strong> $\min_{x\in\mathcal{X}<em>{\mathrm{cvx}}} f</em>{\mathrm{cvx}}(x)$ résoluble globalement avec certificats d’optimalité (dualité forte, KKT). La démarche canonique combine : (i) <strong>relaxation convexe</strong> par enveloppe convexe et conv(hull) : on prend $f_{\mathrm{cvx}}=f^{<strong>}$ (biconjuguée de Fenchel, plus grand convexe l.s.c. majoré par $f$) et un sur-ensemble convexe $\mathcal{X}<em>{\mathrm{cvx}}\supseteq\mathcal{X}$, donnant un **borne inférieure** $p^\star</em>{\mathrm{cvx}}\le p^\star$ (ex.: $\ell_0\to\ell_1$, $\min_x \tfrac12|Ax-b|<em>2^2$ s.c. $|x|_0\le k$ $\leadsto$ $\min_x \tfrac12|Ax-b|_2^2+\lambda|x|_1$; **exactitude** si p.ex. $A$ vérifie une RIP/N.S.P., alors la solution $\ell_1$ coïncide avec $\ell_0$); (ii) **relaxation de rang** via norme nucléaire : $\min_X \operatorname{rank}(X)$ s.c. $\mathcal{A}(X)=b$ $\leadsto$ $\min_X |X|</em>\ast$ s.c. $\mathcal{A}(X)=b$ (exact sous incohérence/échantillonnage suffisant) ; (iii) **lifting/SDP</strong> pour bilinéaires/quadratiques : $x^\top Qx$ avec contraintes non convexes $\leadsto$ $X=xx^\top\succeq 0$ et contraintes linéaires sur $X$, en relâchant $\operatorname{rank}(X)=1$ ; (iv) <strong>épigraphe/perspective</strong> pour convexifier max/fractions : minimiser $f(x)$ $\equiv$ $\min_{t}{t: (x,t)\in\operatorname{epi} f}$, et pour un objectif fractionnel linéaire $\min \frac{c^\top x+d}{e^\top x+f}$ s.c. $Ax\le b$, la <strong>transformation de Charnes–Cooper</strong> $t=(e^\top x+f)^{-1}$, $y=xt$ donne $\min c^\top y+dt$ s.c. $Ay\le bt$, $e^\top y+ft=1$, $t&gt;0$ (LP) ; (v) <strong>changements de variables</strong> (mono. difféomorphes) pour rendre convexes des formes multiplicatives, p.ex. <strong>programmation géométrique</strong> : $\min \sum_k c_k \prod_i x_i^{a_{ik}}$ s.c. $\sum_k d_k \prod_i x_i^{b_{ik}}\le 1$ devient convex en posant $y_i=\log x_i$ et en log-transformant (somme de fonctions convexes log-somme-exp) ; (vi) <strong>prox/ pénalisation exacte</strong> pour absorber contraintes non convexes dans l’objectif, puis remplacer par une norme convexe (p.ex. pénalités SCAD/MCP $\to$ majorations convexes via MM). Sur le plan théorique, le relâché fournit un <strong>dualisme de Lagrange</strong> avec écart $p^\star-p^\star_{\mathrm{cvx}}\ge 0$ ; si la relaxation est <strong>serrée</strong> (p.ex. solution située à un <strong>point extrême</strong> de $\mathcal{X}<em>{\mathrm{cvx}}$ respectant les <strong>KKT</strong> du problème original), on a $p^\star=p^\star</em>{\mathrm{cvx}}$ (exactitude) et la solution du convexe résout l’original. Au final, on obtient un problème convexe de la forme standard $\min_x g(x)$ s.c. $h_i(x)\le 0,\;Ax=b$ (avec $g,h_i$ convexes), solvable en temps polynomial (méthodes de points intérieurs, descente proximale), garantissant un optimum <strong>global</strong> et des <strong>certificats</strong> via conditions KKT et <strong>dualité forte</strong> (écart nul).</p> <h1 id="transformation-dun-signal-du-domaine-temporel-au-domaine-fréquentiel">Transformation d’un signal du domaine temporel au domaine fréquentiel</h1> <p>Pour un signal continu $x(t)\in L^1(\mathbb{R})$ (ou $L^2$), sa transformée de Fourier est $X(f)=\mathcal{F}{x}(f)=\int_{-\infty}^{\infty} x(t)\,e^{-j2\pi f t}\,dt$ et l’inverse s’écrit $x(t)=\int_{-\infty}^{\infty} X(f)\,e^{j2\pi f t}\,df$; $X(f)$ encode l’<strong>amplitude</strong> et la <strong>phase</strong> des composantes sinusoïdales de fréquence $f$. La transformation est linéaire, convertit les <strong>décalages temporels</strong> en facteurs de phase $x(t-t_0)\;\leftrightarrow\;X(f)\,e^{-j2\pi f t_0}$, la <strong>modulation</strong> temporelle en <strong>translation fréquentielle</strong> $x(t)\,e^{j2\pi f_0 t}\;\leftrightarrow\;X(f-f_0)$, et surtout transforme la <strong>convolution temporelle</strong> en <strong>produit fréquentiel</strong> $(x*y)(t)\;\leftrightarrow\;X(f)Y(f)$ (et le produit temporel en convolution fréquentielle). L’<strong>identité de Parseval/Plancherel</strong> préserve l’énergie : $\int |x(t)|^2 dt=\int |X(f)|^2 df$. Pour un signal discret $x[n]=x(nT_s)$ échantillonné à $f_s=1/T_s$, la DTFT est $X(e^{j\omega})=\sum_{n=-\infty}^{\infty} x[n]\,e^{-j\omega n}$ ($\omega=2\pi f/f_s$) et, sur une fenêtre de longueur $N$, la <strong>DFT</strong> est $X[k]=\sum_{n=0}^{N-1} x[n]\,e^{-j2\pi nk/N}$, $k=0,\dots,N-1$, avec inverse $x[n]=\frac{1}{N}\sum_{k=0}^{N-1} X[k]\,e^{j2\pi nk/N}$; la résolution fréquentielle vaut $\Delta f=f_s/N$ et l’algorithme <strong>FFT</strong> calcule la DFT en $O(N\log N)$. L’<strong>échantillonnage</strong> d’un signal continu $x(t)$ produit un spectre périodisé : si $x_s(t)=\sum_{n} x(nT_s)\,\delta(t-nT_s)$, alors $X_s(f)=\frac{1}{T_s}\sum_{m\in\mathbb{Z}} X(f-mf_s)$; l’absence d’<strong>aliasing</strong> requiert un spectre borné $|f|\le B$ et $f_s&gt;2B$ (théorème de Shannon–Nyquist), avec reconstruction idéale $x(t)=\sum_{n\in\mathbb{Z}} x[n]\,\mathrm{sinc}!\big(\tfrac{t-nT_s}{T_s}\big)$. En pratique, l’analyse de signaux non stationnaires utilise des <strong>fenêtres</strong> (STFT) : $X(t_0,f)=\int x(t)\,w(t-t_0)\,e^{-j2\pi f t}\,dt$, où le choix de $w$ contrôle le compromis temps–fréquence (fuites spectrales et élargissement des pics). Ces outils fournissent une cartographie rigoureuse du contenu fréquentiel à partir du temps, ouvrant la voie au filtrage linéaire, à la débruitage, à la démodulation et à l’identification de systèmes via leurs réponses fréquentielles.</p> <h1 id="quand-on-a-linverse-dune-somme-on-peut-toujours-revenir-à-une-somme-des-puissances">Quand on a l’inverse d’une somme: on peut toujours revenir à une somme des puissances!</h1> <h2 id="démo">Démo</h2> <blockquote> <p>On va montrer que \(\colorbox{cyan}{$\displaystyle(1-x)^{-1} = 1 + x + x^2 + x^3 + x^4 + ...$}\)</p> </blockquote> <p>Prenons la somme $S = 1 + x + x^2 + x^3 + x^4 + …$</p> \[\begin{split} S - Sx &amp;= 1 + x + x^2 + x^3 + x^4 + ... - (x + x^2 + x^3 + x^4 + ...) \\ &amp;= 1 \end{split}\] <p>D’où:</p> \[\begin{split} S - Sx &amp;= 1 \\ S(1-x) &amp;= 1 \\ \colorbox{cyan}{$\displaystyle S = \frac{1}{1-x}$} \end{split}\] <p>A noter : quand on fait une approximation en $x$ proche de 0 (développement limité), on s’arrête à une certaine puissance $n$ (en fonction de l’approximation à l’ordre $n$).</p> <h2 id="utilité">Utilité</h2> <p>Dans le papier <a href="https://aclanthology.org/2024.emnlp-main.731.pdf">Chain and Causal Attention for Efficient Entity Tracking</a>, j’essayais de comprendre :</p> \[A + A^2 + A^3 + A^4 + ... = A(I - A)^{-1}\] <p>Et j’ai compris que ça venait de l’extension aux matrices de la formule précédente:</p> \[\begin{split} 1 + x + x^2 + x^3 + x^4 + ... &amp;= \frac{1}{1-x} \\ I + A + A^2 + A^3 + A^4 + ... &amp;= I(I - A)^{-1} \\ A + A^2 + A^3 + A^4 + ... &amp;= A(I - A)^{-1} \end{split}\] <h1 id="la-formule-de-taylor-dordre-n-pour-approximer-une-fonction-en-un-point-x_i">La formule de Taylor d’ordre n pour approximer une fonction en un point $x_i$</h1> <p>Pour une fonction $f(x)$ suffisamment dérivable au voisinage de $x_i$, la formule de Taylor d’ordre $n$ s’écrit :</p> \[f(x) \approx \sum_{k=0}^{n} \frac{f^{(k)}(x_i)}{k!}(x - x_i)^k + R_n(x)\] <p>où :</p> <ul> <li>$f^{(k)}(x_i)$ désigne la dérivée $k$-ième de $f$ évaluée en $x_i$</li> <li>$f^{(0)}(x_i) = f(x_i)$ par convention</li> <li>$R_n(x)$ est le reste de Taylor d’ordre $n$</li> </ul> <p>En développant explicitement :</p> \[f(x) \approx f(x_i) + f'(x_i)(x - x_i) + \frac{f''(x_i)}{2!}(x - x_i)^2 + \cdots + \frac{f^{(n)}(x_i)}{n!}(x - x_i)^n + R_n(x)\] <p>On a donc par exemple, l’approximation de Taylor à l’ordre 1: \(f(x) \approx f(x_i) + f'(x_i)\,(x - x_i)\)</p> <h2 id="démo-par-récurrence-et-intégrations-par-parties">Démo (par récurrence et intégrations par parties)</h2> <p><strong>Étape 1 : Cas de base (n = 0)</strong></p> <p>Partons du théorème fondamental du calcul intégral : \(f(x) - f(x_i) = \int_{x_i}^{x} f'(t) \, dt\)</p> <p>Donc : \(f(x) = f(x_i) + \int_{x_i}^{x} f'(t) \, dt\)</p> <p>C’est la formule de Taylor d’ordre 0 avec $R_0(x) = \int_{x_i}^{x} f’(t) \, dt$.</p> <p><strong>Étape 2 : Passage de l’ordre k à l’ordre k+1</strong></p> <p>Supposons que nous ayons établi la formule à l’ordre $k$ : \(f(x) = \sum_{j=0}^{k} \frac{f^{(j)}(x_i)}{j!}(x - x_i)^j + R_k(x)\)</p> <p>avec $R_k(x) = \int_{x_i}^{x} \frac{f^{(k+1)}(t)}{k!}(x - t)^k \, dt$.</p> <p>Appliquons une intégration par parties à $R_k(x)$ :</p> <p>Posons :</p> <ul> <li>$u = f^{(k+1)}(t)$, donc $du = f^{(k+2)}(t) \, dt$</li> <li>$dv = \frac{(x - t)^k}{k!} \, dt$, donc $v = -\frac{(x - t)^{k+1}}{(k+1)!}$</li> </ul> <p>L’intégration par parties donne : \(R_k(x) = \left[ f^{(k+1)}(t) \cdot \left(-\frac{(x - t)^{k+1}}{(k+1)!}\right) \right]_{x_i}^{x} + \int_{x_i}^{x} \frac{(x - t)^{k+1}}{(k+1)!} f^{(k+2)}(t) \, dt\)</p> <p>Le terme entre crochets devient : \(\left[ -\frac{f^{(k+1)}(t)(x - t)^{k+1}}{(k+1)!} \right]_{x_i}^{x} = 0 - \left(-\frac{f^{(k+1)}(x_i)(x - x_i)^{k+1}}{(k+1)!}\right) = \frac{f^{(k+1)}(x_i)}{(k+1)!}(x - x_i)^{k+1}\)</p> <p>Donc : \(R_k(x) = \frac{f^{(k+1)}(x_i)}{(k+1)!}(x - x_i)^{k+1} + \int_{x_i}^{x} \frac{f^{(k+2)}(t)}{(k+1)!}(x - t)^{k+1} \, dt\)</p> <p>En substituant dans l’expression de $f(x)$ : \(f(x) = \sum_{j=0}^{k} \frac{f^{(j)}(x_i)}{j!}(x - x_i)^j + \frac{f^{(k+1)}(x_i)}{(k+1)!}(x - x_i)^{k+1} + R_{k+1}(x)\)</p> <p>où $R_{k+1}(x) = \int_{x_i}^{x} \frac{f^{(k+2)}(t)}{(k+1)!}(x - t)^{k+1} \, dt$.</p> <p>Ceci établit la formule à l’ordre $k+1$ : \(f(x) = \sum_{j=0}^{k+1} \frac{f^{(j)}(x_i)}{j!}(x - x_i)^j + R_{k+1}(x)\)</p> <p><strong>Conclusion</strong></p> <p>Par récurrence, la formule de Taylor d’ordre $n$ est démontrée pour tout $n \geq 0$.</p> <h2 id="utilité-1">Utilité</h2> <p>Si on cherche à approximer les nouveaux poids du modèle $\theta_\varepsilon$ qui sont les poids du modèle modifié (avec retrait d’un exemple d’entraînement), on a la nouvelle loss finale du modèle $R_\varepsilon(\theta, z_\text{train})$ qui est:</p> \[R_\varepsilon(\theta, z_\text{train}) = \frac{1}{n}\sum_{i=1}^n \mathcal{L}(z_i,\theta) \;+\;\varepsilon\,\mathcal{L}(z_{\rm train},\theta).\] <p>Et donc les nouveaux poids $\theta_\varepsilon$ qui minimisent cette loss:</p> \[\theta_\varepsilon = \arg\min_{\theta}\;R_\varepsilon(\theta, z_\text{train})\] <p>D’où:</p> \[\begin{split} R_\varepsilon(\theta_\varepsilon, z_\text{train}) &amp;= \frac{1}{n}\sum_{i=1}^n \mathcal{L}(z_i,\theta_\varepsilon) \;+\;\varepsilon\,\mathcal{L}(z_{\rm train},\theta_\varepsilon) \\ &amp;= 0 \end{split}\] <p>Maintenant, on peut utiliser une approximation de Taylor à $\varepsilon$ proche de 0 de \(\frac{1}{n}\sum_{i=1}^n \mathcal{L}(z_i,\theta_\varepsilon) \;+\;\varepsilon\,\mathcal{L}(z_{\rm train},\theta_\varepsilon)\), car on connait $\theta$ et donc on peut peut-être arriver à quelque chose!</p> <p>Pour rappel, l’approximation de Taylor de $f$ à l’ordre 1 en $\varepsilon$ proche de 0 donne: \(f(\theta_\varepsilon) \approx f(\hat{\theta}) + f'(\hat{\theta})\,(\theta_\varepsilon - \hat{\theta})\)</p> \[\begin{split} \underbrace{\frac{1}{n}\sum_{i=1}^n \nabla_\theta \mathcal{L}(z_i,\theta_\varepsilon) \;+\;\varepsilon\,\nabla_\theta \mathcal{L}(z_{\rm train},\theta_\varepsilon)}_{f(\theta_\varepsilon)} \approx \underbrace{[ \frac{1}{n}\sum_{i=1}^n \nabla_\theta \mathcal{L}(z_i,\hat{\theta}) \;+\;\varepsilon\,\nabla_\theta \mathcal{L}(z_{\rm train},\hat{\theta}) ]}_{f(\hat{\theta})} \;\; &amp;+ \\ \;\; \underbrace{[ \frac{1}{n}\sum_{i=1}^n \nabla^2_\theta \mathcal{L}(z_i,\hat{\theta}) \;+\;\varepsilon\,\nabla^2_\theta \mathcal{L}(z_{\rm train},\hat{\theta}) ]}_{f'(\hat{\theta})} \;\;(\theta_\varepsilon - \hat{\theta}) \end{split}\] <p>ça nous permet d’isoler $\theta_\varepsilon$ et donc d’écrire $\theta_\varepsilon$ en fonction de $\theta$!</p> \[\begin{split} \theta_\varepsilon \approx \hat{\theta} - \left[ \frac{1}{n}\sum_{i=1}^n \nabla^2_\theta \mathcal{L}(z_i,\hat{\theta}) + \varepsilon\,\nabla^2_\theta \mathcal{L}(z_{\rm train},\hat{\theta}(z_\text{train})) \right]^{-1} \;\; &amp;\times \\ \;\; \left[ \frac{1}{n}\sum_{i=1}^n \nabla_\theta \mathcal{L}(z_i,\hat{\theta}) + \varepsilon\,\nabla_\theta \mathcal{L}(z_{\rm train},\hat{\theta}) \right] \end{split}\] <p>Et ça c’est cool parce qu’on sait le simplifier puisqu’on connait $\hat{\theta}$! Notamment $\hat{\theta}$ sont les poids optimaux pour le modèle de base, donc $\frac{1}{n}\sum_{i=1}^n \nabla_\theta \mathcal{L}(z_i,\hat{\theta}) = 0$. D’autres simplifications peuvent ensuite être faites. Et donc on arrive à approximer les nouveaux poids du modèle!</p> <h1 id="penser-à-la-chain-rule-exemple-pour-calculer-fracpartial-mathcallpartial-theta">Penser à la chain rule: exemple pour calculer $\frac{\partial \mathcal{L}}{\partial \theta}$</h1> <p>On a par exemple $\mathcal{L}$ est une fonction de perte (loss) qui dépend de la sortie du modèle $y$, $y$ qui est la sortie du modèle, qui dépend des paramètres du modèle $\theta$, d’où on a: $\frac{\partial \mathcal{L}}{\partial \theta} = \frac{\partial \mathcal{L(y(\theta))}}{\partial \theta}$. ça doit nous faire penser à la chain rule! \(\frac{\partial}{\partial x} f(g(x))\) avec $f = \mathcal{L}$, ou $f(y) = \mathcal{L(y)}$ et $g(\theta) = y(\theta)$.</p> <h2 id="rappel-de-la-formule">Rappel de la formule…</h2> <p>Il faut donc ici penser à la chain rule! Un petit rappel…</p> \[\frac{\partial}{\partial x} f(g(x)) = f'(g(x)) \cdot g'(x)\] <h2 id="application-concrète-pour-calculer-fracpartial-mathcallpartial-theta">Application concrète pour calculer $\frac{\partial \mathcal{L}}{\partial \theta}$</h2> \[\frac{\partial \mathcal{L(y(\theta))}}{\partial \theta} = \frac{\partial \mathcal{L}}{\partial y} \cdot \frac{\partial y}{\partial \theta}\] <h1 id="optimisation-sous-contrainte-en-ia">Optimisation sous contrainte en IA</h1> <p>TODO</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="maths"/><summary type="html"><![CDATA[Tips mathématiques / points utiles en IA]]></summary></entry><entry><title type="html">Les fonctions d’influence appliquées aux LLMs</title><link href="https://camillebrl.github.io/blog/2025/influence_functions_applied_to_llms/" rel="alternate" type="text/html" title="Les fonctions d’influence appliquées aux LLMs"/><published>2025-07-20T07:00:00+00:00</published><updated>2025-07-20T07:00:00+00:00</updated><id>https://camillebrl.github.io/blog/2025/influence_functions_applied_to_llms</id><content type="html" xml:base="https://camillebrl.github.io/blog/2025/influence_functions_applied_to_llms/"><![CDATA[<blockquote> <p>J’ai eu l’opportunité au sein d’Orange d’explorer les fonctions d’influences appliquées au deep learning, un concept introduit notamment dans le papier <a href="https://arxiv.org/pdf/1703.04730">Understanding Black-box Predictions via Influence Functions</a>. Ces fonctions permettent de quantifier l’impact d’une donnée d’entraînement sur une prédiction du modèle. Elles approximent en réalité la méthode “Leave-one-out” qui compare la prédiction du modèle avec et sans cet échantillon dans le dataset d’entraînement (en entraînant le modèle sans et avec). Cette approche m’a particulièrement intéressée dans le contexte des modèles de langage (LLMs). En travaillant sur l’adaptation d’un LLM à un domaine spécifique, j’ai été confrontée à une question cruciale : comment sélectionner les données de fine-tuning pour garantir que le modèle puisse répondre efficacement à des questions spécifiques? On s’était confronté à plusieurs défis dans le cadre de ce projet sur l’adaptation des LLMs à un domaine métier: Comment identifier quelles données permettent réellement d’améliorer les performances sur des tâches ciblées? Comment évaluer l’impact du “continual pretraining” de mon LLM sur un texte précis sur la performance du modèle final sur la tâche cible? Ainsi, j’ai décidé d’étudier un peu plus en détail les fonctions d’influence appliquées aux LLMs et j’ai trouvé ça facinant (et mathématiquement parlant très complexe). Donc je me suis dit que ça serait utile d’en faire un post, et de mettre un peu les différents papiers que j’ai lu (en diagonale ou de manière plus approfondie), ce que j’ai compris des formules, des approximations, des approches pour adaptées ces méthodes aux LLMs, etc. Ce post est un condensé de tout cela!</p> </blockquote> <h1 id="introduction-à-ce-post">Introduction à ce post</h1> <p>Comme expliqué précédemment, l’influence cherche à approximer le Leave-one-out, c’est à dire cherche à estimer l’<mark>impact qu'aurait un exemple d'entraînement sur la perte d'un exemple de test (ou sur plusieurs résultats du modèle sur un jeu de données test)</mark>.</p> <p>Dans l’ensemble de ce post, on va montrer d’où sort la formule suivante, comment elle est adaptée pour son application aux LLMs, et des cas concrets de son application:</p> \[\begin{split} \mathrm{Influence}\bigl(z_{\mathrm{train}}\to z_{\mathrm{test}}\bigr) &amp;= \frac{d}{d\varepsilon}\, \mathcal{L}\bigl(z_{\rm test},\,\theta_\varepsilon(z_\text{train})\bigr)\Big|_{\varepsilon=0} \\ &amp;\approx -\nabla_\theta \,\mathcal{L}\bigl(z_{\mathrm{test}},\,\hat{\theta} \bigr)\,H_\theta^{-1}(\hat{\theta})\,\nabla_\theta \mathcal{L}(z_{\rm train},\hat{\theta}) \\ &amp;\approx -\nabla_\theta \,\mathcal{L}\bigl(z_{\mathrm{test}},\,\hat{\theta} \bigr)\,(G_\theta(\hat{\theta}) + \lambda I)^{-1}\,\nabla_\theta \mathcal{L}(z_{\rm train},\hat{\theta}) \end{split}\] <p>Où:</p> <ul> <li>\(\hat{\theta}\) sont les poids du modèle préentraîné (poids supposés optimiaux pour le jeu de pretraining du LLM)</li> <li>\(\theta_\varepsilon(z_\text{train})\) sont les poids du modèle “modifié”, ie lorsqu’on upweight de $\varepsilon$ l’exemple d’entraînement $z_\text{train}$</li> <li>\(\theta\) est une variable muette (eg pour le gradient, on dérive par rapport aux paramètres du modèle et on applique en un point)</li> <li>Tous les $\theta$ sont des vecteurs colonne des poids du modèle</li> <li>$\mathcal{L}$ est la loss du modèle, donc ici, vu qu’on est dans le cas des LLMs, c’est la cross-entropy loss (negative log-likelihood), ie pour une séquence de taille $T$ ($x_1$, $x_2$, …, $x_T$): \(\mathcal{L} = -\frac{1}{T}\sum_{t=1}^{T} \log P(x_t \mid x_1, x_2, \ldots, x_{t-1})\)</li> <li>$f$ est une fonction (moyenne, ou autre) sur plusieurs résultats du modèle sur un jeu de données test noté $x$. Il nous permet de ne pas évaluer l’impact de l’upweight d’un $z_\text{train}$ sur la loss d’un $z_\text{test}$, mais sur un ensemble de type de données $x$ (car parfois, on ne veut pas calculer l’influence d’une donnée d’entraînement sur la loss d’un prompt, mais sur la performance (pas forcément la loss d’ailleurs) sur un certain type de prompt)</li> <li>$H_\theta$ est la hessienne (dérivée seconde par rapport aux paramètres du modèle)</li> <li>$G_\theta$ est la Hessienne de Gauss-Newton qu’on va voir plus loin dans ce post, qui est en fait une approximation de la hessienne</li> <li>$\lambda$ est un terme dit de “damping”, dû au fait que la loss n’est pas convexe, qu’on verra plus loin dans le post</li> </ul> <p>Note que dans tout ce post, on va utiliser ces notations!</p> <p>La librairie <a href="https://github.com/pomonam/kronfluence">Kronfluence</a> permet de calculer l’influence dans le cas des LLMs. Des repo github proposent des tutos pour plusieurs de libs de calcul de l’influence dans les modèles de deep learning, comme <a href="https://github.com/deel-ai/influenciae">Influenciae</a> (btw, c’est une lib d’un labo de Toulouse!). Les fonctions d’influence permettent de répondre à ce genre de questions:</p> <ul> <li>Est-ce que je devrais <mark>ajouter cet exemple dans mon set d'apprentissage pour améliorer les performances de cette prédiction?</mark></li> <li>Quels <mark>exemples d'entraînement ont été utiles à la prédiction</mark> de mon modèle?</li> <li>Le modèle s’est trompé: sur quels exemples d’entraînement s’est-il basé pour cette mauvaise prédiction?</li> </ul> <h2 id="11-introduction-sur-les-fonctions-dinfluence">1.1 Introduction sur les fonctions d’influence</h2> <p>Pour introduire les fonctions d’influence appliquées au deep learning, nous nous basons sur le le papier <a href="https://arxiv.org/pdf/1703.04730">Understanding Black-box Predictions via Influence Functions</a>, et notamment sur l’annexe A pour expliquer les différentes formules.</p> <p><strong>L’influence de</strong> $z_{\rm train}$ <strong>sur</strong> $z_{\rm test}$ ($\mathrm{Influence}(z_{\rm train} \to z_{\rm test})$) <strong>se définit comme</strong></p> \[\mathrm{Influence}(z_{\rm train}\to z_{\rm test})\;=\; \left.\frac{d}{d\varepsilon}\,\mathcal{L}\bigl(z_{\rm test},\,\theta_\varepsilon(z_\text{train})\bigr)\right|_{\varepsilon=0}\] <p><strong>L’influence de</strong> $z_{\rm train}$ <strong>sur</strong> $f(x)$ ($\mathrm{Influence}(z_{\rm train} \to f_{\theta_\varepsilon(z_\text{train})}(x))$) <strong>se définit comme</strong></p> \[\mathrm{Influence}(z_{\rm train}\to f(x))\;=\; \left.\frac{d}{d\varepsilon}\,(f_{\theta_\varepsilon(z_\text{train})}(x)\bigr)\right|_{\varepsilon=0}\] <p>En d’autres termes, elle <mark>mesure la sensibilité de la loss de $z_{\rm test}$ (ou de toute fonction $f(x)$) à un « up-weight » infinitésimal de la loss de $z_{\rm train}$.</mark></p> <p>A noter:</p> <ul> <li>C’est l’<strong>impact de l”up-weight” de la loss sur $z_\text{train}$ sur qqch qu’on mesure avec les fonctions d’influence</strong>. En fait pour mesurer l’impact de l”up-weight” de $z_{\text{train}}$ dans la loss globale, on se pose la question: “on se pose la question : <strong>“<mark>si je donnais un peu plus de poids à ce terme de loss dans l’objectif global, comment cela ferait-il bouger mes paramètres et, avec ces nouveaux paramètres, ma performance sur un point de test, ou sur une fonction?</mark>“</strong> . En effet, en deep learning, modifier le poids d’une donnée dans l’entraînement, c’est modifier le poids qu’on donne à sa loss dans l’apprentissage.</li> <li><mark>$f_{\theta_\varepsilon(z_\text{train})}(x)$ peut être n'importe quelle fonction (exemple: la moyenne des prédictions sur un ensemble de données types $x$</mark> (cf le papier <a href="https://arxiv.org/pdf/2505.19949">Which Data Attributes Stimulate Math and Code Reasoning? An Investigation via Influence Functions</a> qui cherche à calculer l’influence des textes d’entraînement sur la génération de code (moyenne de des log probabilité de la génération de chaque token de code générés dans un benchmark sachant un problème de code en langage naturel à résoudre)), la différence entre 2 prédictions du modèle, …)</li> </ul> <blockquote> <p><mark>Nous allons, dans ce post, entrer dans le détail de comment on calcule cette influence:</mark></p> \[\begin{split} \mathrm{Influence}\bigl(z_{\mathrm{train}}\to z_{\mathrm{test}}\bigr) &amp;= I_{z_\text{test}}\bigl(z_{\mathrm{train}}\bigr) \\ &amp;= \frac{d}{d\varepsilon}\, \mathcal{L}\bigl(z_{\rm test},\ \theta_\varepsilon(z_\text{train})\bigr) \Big|_{\varepsilon=0} \\ &amp;= -\nabla_\theta \,\mathcal{L}\bigl(z_{\mathrm{test}},\,\hat{\theta}\bigr)\,H_\theta^{-1}(\hat{\theta})\,\nabla_\theta \mathcal{L}(z_{\rm train},\hat{\theta}) \end{split}\] <p>Ou bien, si on veut l’influence sur $f(x)$:</p> \[\begin{split} \mathrm{Influence}\bigl(z_{\mathrm{train}}\to f(x)\bigr) &amp;= I_{f(x)}\bigl(z_{\mathrm{train}}\bigr) \\ &amp;= \left.\frac{d}{d\varepsilon}\bigl(f_{\theta_{\varepsilon}(z_{\mathrm{train}})}(x)\bigr)\right|_{\varepsilon=0} \\ &amp;= - \nabla_\theta f_{\hat{\theta}}(x)^\top \, H_\theta(\hat{\theta})^{-1} \, \nabla_\theta \mathcal{L}\bigl(x_{\mathrm{train}},\hat{\theta}\bigr) \end{split}\] </blockquote> <blockquote> <p>On cherche donc à calculer</p> \[\frac{d}{d\varepsilon}\, \mathcal{L}\bigl(z_{\rm test},\ \theta_\varepsilon(z_\text{train})\bigr) \Big|_{\varepsilon=0}\] </blockquote> <h3 id="111-décomposition-avec-chain-rule">1.1.1. Décomposition avec chain rule</h3> <p>On peut voir \(\frac{d}{d\varepsilon}\, \mathcal{L}\bigl(z_{\rm test},\ \theta_\varepsilon(z_\text{train})\bigr) \Big|_{\varepsilon=0}\) comme : \(\frac{d}{d\varepsilon} f(g(\varepsilon))\)</p> <p>avec :</p> <ul> <li>$g(\varepsilon) = \theta_\varepsilon(z_\text{train})$</li> <li>$f(\theta_\varepsilon(z_\text{train})) = \mathcal{L}(z_{\rm test},\theta_\varepsilon(z_\text{train}))$</li> </ul> <p>d’où, <mark>par chain rule</mark> ( \(\frac{d}{d\varepsilon} f \circ g(\varepsilon) = \nabla_\theta f(g(\varepsilon)) \times \frac{d}{d\varepsilon} g(\varepsilon)\) ), on a :</p> \[\colorbox{yellow}{ $\displaystyle \frac{d}{d\varepsilon}\, \mathcal{L}\bigl(z_{\rm test},\ \theta_\varepsilon(z_\text{train})\bigr) \Big|_{\varepsilon=0} = \nabla_\theta \mathcal{L}(z_{\rm test}, \hat{\theta}) \times \frac{d}{d\varepsilon} \theta_\varepsilon(z_\text{train}) \Big|_{\varepsilon=0} $}\] <p>Du coup, <mark>dans un premier temps, il nous faut calculer</mark></p> \[\colorbox{yellow}{ $\displaystyle \frac{d}{d\varepsilon} \theta_\varepsilon(z_\text{train}) \Big|_{\varepsilon=0} $}\] <h3 id="112-calcul-de-fracddvarepsilon-theta_varepsilonz_texttrain-big_varepsilon0">1.1.2. Calcul de $\frac{d}{d\varepsilon} \theta_\varepsilon(z_\text{train}) \Big|_{\varepsilon=0}$</h3> <p><strong>Calculer cela revient à se demander : <mark>comment $\theta_\varepsilon(z_\text{train})$ varie autour de $\theta$ quand on up-weight très légèrement (voisinage de 0) la loss de notre exemple $z_\text{train}$</mark>?</strong></p> <blockquote> <p>Le but ici est de calculer la dérivée de $\theta_\varepsilon(z_\text{train})$ par rapport à $\varepsilon$ pris en $\varepsilon = 0$.</p> </blockquote> <p>Essayons d’abord de voir ce qu’est ce $\theta_\varepsilon(z_\text{train})$.</p> <p><mark>Pour faire cet "up-weight" de la loss de $z_{\text{train}}$ d'un tout petit $\varepsilon$, on perturbe la fonction de perte en ajoutant un petit coefficient $\varepsilon$ sur la perte de $z_{\rm train}$ et on voit comment les paramètres optimaux $\theta$ évoluent avec $\varepsilon$.</mark></p> <p>On repart de ce que ça veut dire de “perturber la fonction de perte en ajoutant un petit coefficient $\varepsilon$ sur la perte de $z_{\rm train}$”: on obtient une nouvelle la loss totale du modèle ($R_\varepsilon(\theta, z_\text{train})$) avec les nouveau poid $\varepsilon$ donné à la loss de $z_{\text{train}}$:</p> <h4 id="1121-theta_varepsilonz_texttrain-sont-les-poids-qui-minimisent-cette-nouvelle-loss-du-modèle-après-lupweight-de-varepsilon-de-z_texttrain">1.1.2.1. $\theta_\varepsilon(z_\text{train})$ sont les poids qui minimisent cette nouvelle loss du modèle après l’upweight de $\varepsilon$ de $z_\text{train}$</h4> <p>Le modèle réentraîné avec cet “upweight” de $\varepsilon$ de $z_\text{train}$ a donc une nouvelle loss de:</p> \[R_\varepsilon(\theta, z_\text{train}) = \frac{1}{n}\sum_{i=1}^n \mathcal{L}(z_i,\theta) \;+\;\varepsilon\,\mathcal{L}(z_{\rm train},\theta).\] <p>Puis, <mark>on cherche les poids $\theta_\varepsilon(z_\text{train})$ qui minimisent cette nouvelle loss</mark>:</p> \[\theta_\varepsilon(z_\text{train}) \;=\; \arg\min_{\theta}\;R_\varepsilon(\theta, z_\text{train})\] <p><mark>Ce qui revient à chercher les $\theta_\varepsilon(z_\text{train})$ dont le gradient de cette nouvelle loss en $\theta$ est nul</mark> , car $\theta_\varepsilon(z_\text{train})$ est un minimum local de $R_\varepsilon$ si et seulement si (si la fonction est convexe: mais pas le cas pour les réseaux de neurone, cf partie 1.3.2) sa dérivée première (gradient) s’annule:</p> \[\nabla_\theta R_\varepsilon\bigl(\theta_\varepsilon(z_\text{train})\bigr) = 0.\] <h4 id="1122-approximation-de-taylor-à-lordre-1-en-varepsilon--0-de-theta_varepsilon">1.1.2.2. Approximation de Taylor à l’ordre 1 en $\varepsilon = 0$ de $\theta_\varepsilon$</h4> <p>Pour cette partie, on écrit pour simplifier $\theta_\varepsilon$ au lieu de $\theta_\varepsilon(z_\text{train})$</p> <p><mark>En développant à l'aide de l'approximation de Taylor en $\hat{\theta}$ pour un tout petit $\varepsilon$, ie proche de 0, donc pour un $\theta_\varepsilon$ proche de $\hat{\theta}$, cette formule</mark>:</p> \[\frac{1}{n}\sum_{i=1}^n \nabla_\theta \mathcal{L}(z_i,\theta_\varepsilon) \;+\;\varepsilon\,\nabla_\theta \mathcal{L}(z_{\rm train},\theta_\varepsilon)\] <blockquote> <p>Rappel de la formule de Taylor à l’ordre 1 en $\varepsilon = 0$:</p> \[f(\theta_\varepsilon) \approx f(\hat{\theta}) + f'(\hat{\theta})\,(\theta_\varepsilon - \hat{\theta})\] </blockquote> <p>On obtient :</p> \[\begin{split} \nabla_\theta R_\varepsilon\bigl(\theta_\varepsilon\bigr) &amp;= \underbrace{\frac{1}{n}\sum_{i=1}^n \nabla_\theta \mathcal{L}(z_i,\theta_\varepsilon) \;+\;\varepsilon\,\nabla_\theta \mathcal{L}(z_{\rm train},\theta_\varepsilon)}_{f(\theta_\varepsilon)} \\ &amp;\approx \underbrace{[ \frac{1}{n}\sum_{i=1}^n \nabla_\theta \mathcal{L}(z_i,\hat{\theta}) \;+\;\varepsilon\,\nabla_\theta \mathcal{L}(z_{\rm train},\hat{\theta}) ]}_{f(\hat{\theta})} \;\; + \;\; \underbrace{[ \frac{1}{n}\sum_{i=1}^n \nabla^2_\theta \mathcal{L}(z_i,\hat{\theta}) \;+\;\varepsilon\,\nabla^2_\theta \mathcal{L}(z_{\rm train},\hat{\theta}) ]}_{f'(\hat{\theta}) = \nabla_\theta f(\hat{\theta})} \;\;(\theta_\varepsilon - \hat{\theta}) \end{split}\] <p>\(\nabla_\theta R_\varepsilon\bigl(\theta_\varepsilon\bigr) = 0\) nous ramène donc à la formule:</p> \[\left[ \frac{1}{n}\sum_{i=1}^n \nabla_\theta \mathcal{L}(z_i,\hat{\theta}) + \varepsilon\,\nabla_\theta \mathcal{L}(z_{\rm train},\hat{\theta}) \right] + \left[ \frac{1}{n}\sum_{i=1}^n \nabla^2_\theta \mathcal{L}(z_i,\hat{\theta}) + \varepsilon\,\nabla^2_\theta \mathcal{L}(z_{\rm train},\hat{\theta}) \right] \; \; (\theta_\varepsilon - \hat{\theta}) \; \; \approx 0\] <p>D’où, en isolant $(\theta_\varepsilon - \theta)$ dans l’équation:</p> \[\theta_\varepsilon - \hat{\theta} \approx - \left[ \frac{1}{n}\sum_{i=1}^n \nabla^2_\theta \mathcal{L}(z_i,\hat{\theta}) + \varepsilon\,\nabla^2_\theta \mathcal{L}(z_{\rm train},\hat{\theta}(z_\text{train})) \right]^{-1} \;\; \times \;\; \left[ \frac{1}{n}\sum_{i=1}^n \nabla_\theta \mathcal{L}(z_i,\hat{\theta}) + \varepsilon\,\nabla_\theta \mathcal{L}(z_{\rm train},\hat{\theta}) \right]\] <p>Or, <mark>puisque $\hat{\theta}$ sont les poids optimaux pour le modèle de base</mark>, $\frac{1}{n}\sum_{i=1}^n \nabla_\theta \mathcal{L}(z_i,\hat{\theta}) = 0$:</p> \[\theta_\varepsilon - \hat{\theta} \approx - \left[ \frac{1}{n}\sum_{i=1}^n \nabla^2_\theta \mathcal{L}(z_i,\hat{\theta}) + \varepsilon\,\nabla^2_\theta \mathcal{L}(z_{\rm train},\hat{\theta}(z_\text{train})) \right]^{-1} \;\; \times \;\; \varepsilon\,\nabla_\theta \mathcal{L}(z_{\rm train},\hat{\theta})\] <h4 id="1123-theta_varepsilon---theta-donne-un-terme-en-fonction-de-varepsilon-x-une-série-en-varepsilon-on-va-montrer-quon-ne-peut-garder-que-la-constante-indépendante-de-varepsilon-de-la-série-à-lordre-1">1.1.2.3. $\theta_\varepsilon - \theta$ donne un terme en fonction de $\varepsilon$ x une série en $\varepsilon$: on va montrer qu’on ne peut garder que la constante (indépendante de $\varepsilon$) de la série à l’ordre 1</h4> <p>Ici, on a :</p> \[\theta_\varepsilon - \hat{\theta} \approx - \left[ \underbrace{\frac{1}{n}\sum_{i=1}^n \nabla^2_\theta \mathcal{L}(z_i,\hat{\theta})}_{= a} + \varepsilon\,\underbrace{\nabla^2_\theta \mathcal{L}(z_{\rm train},\hat{\theta}(z_\text{train}))}_{= b} \right]^{-1} \;\; \times \;\; \varepsilon\,\underbrace{\nabla_\theta \mathcal{L}(z_{\rm train},\hat{\theta})}_{= \beta}\] <p>On sait que $A(I-A)^{-1}$ peut se ramener à une somme : $A(I-A)^{-1} = \sum_{i=0}^{\inf}{A^i}$ =&gt; cf ce <a href="camillebrl.github.io/blog/2025/tips_mathematics_for_ai/">post de blog</a> ou cf les <a href="https://fr.wikipedia.org/wiki/S%C3%A9rie_de_Neumann">séries de Neumann</a>.</p> <p>Du coup on a $(a + \varepsilon b)^{-1}$ qui peut se ramener à quelque chose de la forme :</p> <p>A COMPLETER!!!</p> <p>En gros, l’idée c’est qu’on obtient qqch du genre: \(\begin{split} (a + \varepsilon b) \times \varepsilon \beta &amp;= \varepsilon \beta a + \varepsilon^2 b \beta \\ &amp;\underbrace{\approx}_{\text{à l'ordre 1}} \varepsilon \beta a \end{split}\)</p> <p>Et que les termes en $\varepsilon ^2$ sont négligeables puisqu’on fait une approximation pour $\varepsilon$ proche de 0 à l’ordre 1.</p> <p>En gros, on se retrouve avec:</p> \[\colorbox{orange}{$\displaystyle(\theta_\varepsilon - \theta)$} \approx - [\frac{1}{n}\sum_{i=1}^n \nabla^2_\theta \mathcal{L}(z_i,\hat{\theta})]^{-1} \times \colorbox{cyan}{$\displaystyle \varepsilon $} \,\colorbox{pink}{$\displaystyle \nabla_\theta \mathcal{L}(z_{\rm train},\hat{\theta}) $}\] <p>Et si on <mark>dérive par rapport à $\varepsilon$</mark> on obtient:</p> \[\begin{split} \frac{d}{d \colorbox{cyan}{$\displaystyle \varepsilon $}}\colorbox{orange}{$\displaystyle(\theta_\varepsilon - \theta)$} &amp;= \colorbox{yellow}{$\displaystyle \frac{d}{d\varepsilon} \theta_\varepsilon $} \\ &amp;\approx - [\underbrace{\frac{1}{n}\sum_{i=1}^n \nabla^2_\theta \mathcal{L}(z_i,\hat{\theta})}_{\colorbox{red}{$\displaystyle H_\theta (\hat{\theta}) $}}]^{-1} \times \colorbox{pink}{$\displaystyle \nabla_\theta \mathcal{L}(z_{\rm train},\hat{\theta})$} \\ &amp;\approx - \colorbox{red}{$\displaystyle H_\theta (\hat{\theta}) $}^{-1} \nabla_\theta \mathcal{L}(z_{\rm train},\hat{\theta}) \end{split}\] <p>D’où :</p> \[\colorbox{yellow}{ $\displaystyle \frac{d}{d\varepsilon} \theta_\varepsilon(z_\text{train}) = - H_\theta^{-1}(\hat{\theta}) \nabla_\theta \mathcal{L}(z_{\rm train},\hat{\theta}) $}\] <p>Ainsi, quand on multiplie par $\nabla_\theta \,\mathcal{L}\bigl(z_{\mathrm{test}},\,\hat{\theta}\bigr)$ on obtient a la formule de l’influence:</p> \[\begin{split} \mathrm{Influence}\bigl(z_{\mathrm{train}}\to z_{\mathrm{test}}\bigr) &amp;= I_{z_\text{test}}\bigl(z_{\mathrm{train}}\bigr) \\ &amp;= \frac{d}{d\varepsilon}\, \mathcal{L}\bigl(z_{\rm test},\ \theta_\varepsilon(z_\text{train})\bigr) \Big|_{\varepsilon=0} \\ &amp;= -\nabla_\theta \,\mathcal{L}\bigl(z_{\mathrm{test}},\,\hat{\theta}\bigr)H_\theta^{-1}(\hat{\theta})\nabla_\theta \mathcal{L}(z_{\rm train},\hat{\theta}) \end{split}\] <p>ou:</p> \[\begin{split} \mathrm{Influence}\bigl(z_{\mathrm{train}}\to f(x)\bigr) &amp;= I_{f(x)}(z_{\mathrm{train}}) \\ &amp;= \left.\frac{d}{d\varepsilon}\bigl(f_{\theta_{\varepsilon}(z_{\mathrm{train}})}(x)\bigr)\right|_{\varepsilon=0} \\ &amp;= - \nabla_\theta f_{\hat{\theta}}(x)^\top \, H_\theta(\hat{\theta})^{-1} \, \nabla_\theta \mathcal{L}\bigl(z_{\mathrm{train}},\hat{\theta}\bigr) \end{split}\] <h2 id="12-corrections-des-fonctions-dinfluence">1.2 “Corrections” des fonctions d’influence</h2> <h3 id="121-données-dentraînement-mal-apprises-prédominantes-comment-corriger">1.2.1 Données d’entraînement mal apprises prédominantes: comment corriger?</h3> <p>A noter que le papier <a href="https://arxiv.org/pdf/2006.14651">Influence Functions in Deep Learning Are Fragile</a> indique que les <mark>fonctions d'influence sont biaisées vers les exemples à forte perte</mark>. en effet, le <mark>gradient de la loss des points d'entraînement est plus élevé pour les exemples mal appris</mark>, entraînant un <mark>biais systématique vers ces exemples dans l’attribution d’influence, indépendamment de leur véritable effet sur la fonction qu'on veut mesurer ou sur la loss sur notre $z_\text{test}$</mark>. Le papier <a href="https://arxiv.org/pdf/2003.11630">RelatIF: Identifying Explanatory Training Examples via Relative Influence</a> propose de recalculer les scores d’influence en <mark>normalisant l'influence par l'inverse de la hessienne x le gradient de la loss du training datapoint</mark>, éliminant ainsi les gradients élevés pour des exemples mal appris par le modèle (sans lien avec la fonction à maximiser).</p> \[\begin{split} \mathrm{Influence}\bigl(z_{\mathrm{train}}\to z_{\mathrm{test}}\bigr) &amp;= \frac{\frac{d}{d\varepsilon} \mathcal{L}\bigl(z_{\rm test},\theta_\varepsilon(z_\text{train})\bigr)}{||H_\theta^{-1}(\hat{\theta}) \;\; \nabla_\theta \mathcal{L}(z_\text{test}, \hat{\theta})||} \Big|_{\varepsilon=0} \\ &amp;= \frac{-\nabla_\theta \,f(\theta_\varepsilon(z_\text{train}))H_\theta^{-1}(\hat{\theta})\nabla_\theta \mathcal{L}(z_{\rm train},\theta_\varepsilon(z_\text{train}))}{||H_\theta^{-1}(\hat{\theta}) \;\; \nabla_\theta \mathcal{L}(z_\text{test}, \hat{\theta})||} \end{split}\] <p>Il faut que je revois un peu plus le papier pour comprendre d’où sort cette formule.</p> <h2 id="13-les-limites-de-linfluence-appliquée-au-deep-learning-et-comment-contrer-cela">1.3 Les limites de l’influence appliquée au deep learning et comment contrer cela</h2> <p>Le papier <a href="https://arxiv.org/pdf/2209.05364">If Influence Functions are the Answer, Then What is the Question?</a> explique que les <strong>fonctions d’influence</strong> n’approximent pas fidèlement le retraining « leave-one-out », mais qu’elles approximent en fait à la <strong>fonction de réponse de Bregman proximale</strong> (PBRF), une formulation plus locale autour des paramètres entraînés.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/pbrf-480.webp 480w,/assets/img/explainability_llms/pbrf-800.webp 800w,/assets/img/explainability_llms/pbrf-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/explainability_llms/pbrf.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>En effet:</p> <p>Dans les LLMs, souvent surparamétrés, les optima peuvent être non uniques. La matrice hessienne $H_\theta$ devient alors parfois singulière, empêchant l’existence d’une fonction de réponse unique. De plus, on n’entraîne généralement pas un réseau jusqu’à convergence totale, d’une part pour limiter le coût de calcul, d’autre part pour éviter le surapprentissage. Hors optimum, l’interprétation de la formule de l’influence n’est plus claire et la hessienne peut présenter des valeurs propres négatives.</p> <p><mark>La fonction de réponse de Bregman proximale offre une meilleure approximation des fonctions d’influence dans le contexte du deep learning : elle ajoute un terme d’amortissement $\lambda$ et utilise un linéarisé de Gauss–Newton $G$ pour corriger les problèmes de singularité, de non-convergence et de non-convexité</mark>. Concrètement, la <mark>PBRF repose sur une hessienne de Gauss–Newton amortie $G + \lambda I$, toujours définie positive, garantissant une réponse bien définie</mark>.</p> <p>Note: je dois revoir plus en détail le PBRF pour mieux le comprendre.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/pbrf2-480.webp 480w,/assets/img/explainability_llms/pbrf2-800.webp 800w,/assets/img/explainability_llms/pbrf2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/explainability_llms/pbrf2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Les fonctions d’influence (trait noir pointillé) et la PBRF (trait rouge) appréhendent différemment la modification locale du paysage de perte :</p> <ul> <li>En réglant la pondération d’un exemple $z_m$, la PBRF suit la trajectoire qui minimise/maximise la perte tout en restant proche de $\theta_\varepsilon(z_\text{train})$.</li> <li>Les <mark>fonctions d’influence classiques se bornent à une expansion de Taylor d’ordre 1 autour de $\epsilon=0$, valable seulement en présence d’une fonction strictement convexe et d’un optimum unique</mark>.</li> </ul> <p>On va voir en quoi ça consiste, cet “amortissement” ($\lambda$) (1.3.1) et ce “gauss-newton” ($G$) (1.3.2):</p> <h3 id="131-hesienne-pas-forcément-inversible">1.3.1 Hesienne pas forcément inversible…</h3> <p>Dans les réseaux de neurones, la loss d’entraînement n’est pas fortement convexe (le minimum local n’est pas forcément un minimum global…) donc la hessienne peut être non inversible. Donc, des approches ont été étudiées pour garantir l’inversibilité de la hessienne, en ajoutant notamment un terme dit de “damping” $\lambda &gt;0$.</p> <h3 id="132-hessienne-par-rapport-aux-paramètres-du-réseau-compliquée-à-calculer-pour-des-réseaux-avec-un-grand-nombre-de-paramètres">1.3.2 Hessienne par rapport aux paramètres du réseau compliquée à calculer pour des réseaux avec un grand nombre de paramètres…</h3> <p>Le papier <a href="https://arxiv.org/pdf/2209.05364">If Influence Functions are the Answer, Then What is the Question?</a> propose d’approximer la Hessienne par la Hessienne de Gauss–Newton (GNH), notée $G_\theta$ :</p> \[G_\theta = J_{y\theta}^T \, H_y \, J_{y\theta}\] <p>où :</p> <ul> <li>$J_{y\theta}$ est la matrice Jacobienne des sorties du réseau par rapport aux paramètres ;</li> <li>$H_y = \nabla^2_y \mathcal{L}(y, \theta)$ est la Hessienne de la fonction de coût par rapport aux sorties du réseau. propose d’approximer la Hessienne par la matrice d’information de Fisher (équivalente à la Hessienne de Gauss-Newton).</li> </ul> <p>En fait, on a:</p> \[\underbrace{H_\theta}_{\text{Très difficile}} =\;\;\; \underbrace{J_{y\theta}^T \, H_y \, J_{y\theta}}_{\text{Facile (GNH)}} \;\;\;+ \underbrace{\sum_i \frac{\partial \mathcal{L}}{\partial y_i} \,\nabla_\theta^2 y_i}_{\text{Cauchemar computationnel}}\] <p>avec:</p> <ul> <li>$J_{y\theta}$ : Déjà calculé par backpropagation standard</li> <li>$H_y$ : Matrice $k \times k$ avec $k =$ le nombre de tokens possibles (ex : 151936 pour Qwen)</li> </ul> <p>On va essayer de décortiquer la hessienne pour retrouver cette formule:</p> \[H_\theta = \frac{\partial ^2 \mathcal{L}}{\partial \theta}\] <p>Or, par chain rule, avec $\mathcal{L}$ qui est une fonction de perte (loss) qui dépend de la sortie du modèle $y$, $y$ qui est la sortie du modèle, qui dépend des paramètres du modèle $\theta$, d’où on a: \(\mathcal{L} = \mathcal{L}(y(\theta))\)</p> <p><strong>Rappel: \(\frac{\partial}{\partial x} f(g(x)) = f'(g(x)) \cdot g'(x)\)</strong></p> <p>D’où:</p> \[\frac{\partial \mathcal{L}}{\partial \theta} = \frac{\partial \mathcal{L}}{\partial y} \cdot \frac{\partial y}{\partial \theta}\] <p>Ainsi:</p> \[\begin{split} \frac{\partial ^2 \mathcal{L}}{\partial \theta} &amp;= \frac{\partial}{\partial \theta} (\frac{\partial \mathcal{L}}{\partial \theta}) \\ &amp;= \frac{\partial}{\partial \theta} (\frac{\partial \mathcal{L}}{\partial y} \cdot \frac{\partial y}{\partial \theta}) \end{split}\] <p>Par dérivée d’une multiplication de fonctions : <strong>Rappel: \((f \times g)' = f'g + g'f\)</strong></p> <p>Avec</p> <ul> <li> \[f = \frac{\partial \mathcal{L}}{\partial y}\] </li> <li> \[g = \frac{\partial y}{\partial \theta}\] </li> </ul> <p>On a:</p> \[\begin{split} H_\theta &amp;= \frac{\partial ^2 \mathcal{L}}{\partial \theta} \\ &amp;= \frac{\partial}{\partial \theta} (\frac{\partial \mathcal{L}}{\partial y} \times \frac{\partial y}{\partial \theta}) \\ &amp;= \colorbox{lime}{$\displaystyle \underbrace{\frac{\partial}{\partial \theta} (\frac{\partial \mathcal{L}}{\partial y})}_{= f'}$} \times \colorbox{pink}{$\displaystyle \underbrace{\frac{\partial y}{\partial \theta}}_{= g} $} \;\; + \;\; \colorbox{brown}{$\displaystyle \underbrace{\frac{\partial}{\partial \theta} (\frac{\partial y}{\partial \theta})}_{= g' = \frac{\partial ^2 y}{\partial \theta ^2} = \nabla ^2_\theta y} \times \underbrace{\frac{\partial \mathcal{L}}{\partial y}}_{= f}$} \end{split}\] <p>Concentrons-nous sur \(\frac{\partial}{\partial \theta} (\underbrace{\frac{\partial \mathcal{L}}{\partial y}}_{= h})\):</p> <p>Par chain rule (car la dérivée de $h$ qu’on cherche dépend d’une variable intermédiaire (ici $y$), qui dépend elle-même de $\theta$)</p> \[\begin{split} \colorbox{lime}{$\displaystyle \frac{\partial}{\partial \theta} \left( \frac{\partial \mathcal{L}}{\partial y} \right) $} &amp;= \frac{\partial h}{\partial y} \cdot \frac{\partial y}{\partial \theta} \\ &amp;= \frac{\partial}{\partial y} (\frac{\partial \mathcal{L}}{\partial y}) \cdot \frac{\partial y}{\partial \theta} \\ &amp;= \underbrace{\frac{\partial^2 \mathcal{L}}{\partial y^2}}_{= H_y} \cdot \underbrace{\frac{\partial y}{\partial \theta}}_{= J_{y \theta}} \\ \end{split}\] <p>On se retrouve donc avec: \(H_\theta = \colorbox{pink}{$\displaystyle J_{y\theta}^T $} \, \colorbox{lime}{$\displaystyle H_y \, J_{y\theta} $} + \colorbox{brown}{$\displaystyle \nabla_\theta^2 y \cdot \frac{\partial \mathcal{L}}{\partial y}$}\)</p> <p>Ainsi, afin de ne pas calculer les dérivées secondes à travers tout le réseau (ce qui est très coûteux quand on a beaucoup de paramètres), on utilise, pour l’ensemble des calculs d’influence (surtout pour les LLMs), ce résultat:</p> \[H_\theta^{-1}(\hat{\theta})\approx \bigl(G_\theta + \lambda I\bigr)^{-1}.\] <h3 id="134-factorisation-par-blocs-de-g_theta-et-factorisation-en-produit-de-kronecker-pour-pourvoir-stocker-cette-matrice--paralléliser-les-calculs-entre-couches">1.3.4 Factorisation par blocs de $G_\theta$ et factorisation en produit de Kronecker pour pourvoir stocker cette matrice &amp; paralléliser les calculs entre couches</h3> <p>Au lieu d’inverser directement la grande matrice $\,G_\theta+\lambda I$, le papier <a href="https://arxiv.org/pdf/2505.05017">Scalable Multi-Stage Influence Function for Large Language Models via Eigenvalue-Corrected Kronecker-Factored Parameterization</a> exploite sa structure en blocs correspondant à chaque couche du réseau.</p> \[G = \begin{bmatrix} G_{1,1} &amp; G_{1,2} &amp; \cdots\\ G_{2,1} &amp; G_{2,2} &amp; \\ \vdots &amp; &amp; \ddots \end{bmatrix},\] <p>Pour un réseau à $L$ couches, on a donc \(G = [G_{i,j}]_{1 \leq i, j \leq L}\), avec \(G_{i,j}\) qui représente le bloc entre les paramètres de la couche $i$ et de la couche $j$.</p> <p>Cette séparation en bloc permet au papier de simplifier $G$ en ne gardant que les blocs diagonaux :</p> \[G \approx \tilde{G} = \mathrm{diag}(G_{1,1}, G_{2,2}, \dots, G_{L,L})\] <p>Cela signifie qu’on ignore les interactions entre différentes couches et qu’on ne considère que les blocs $G_{l,l}$ pour chaque couche $l$.</p> <p>Cette approche par blocs permet :</p> <ul> <li>De traiter chaque couche indépendamment</li> <li>D’éviter de stocker/calculer la matrice complète de taille $p \times p$ (où $p$ est nombre total de paramètres)</li> <li>De paralléliser les calculs entre couches</li> </ul> <p>C’est ce qui rend la méthode scalable pour les grands modèles comme les LLMs avec des milliards (plutôt même billions…) de paramètres.</p> <h2 id="14-le-cas-des-llms-besoin-dune-influence-token-wise-ou-sentence-wise">1.4 Le cas des LLMs: besoin d’une influence token-wise ou sentence-wise</h2> <p>Le papier <a href="https://arxiv.org/pdf/2308.03296">Studying Large Language Model Generalization with Influence Functions</a> présente l’application des fonctions d’influence aux LLMs. Dans le cas des LLMs, la loss est la negative log-vraisemblance. La première <mark>particularité d'un LLM, c'est le fait qu'un datapoint est un peu compliqué à définir. On peut supposer qu'il s'agit d'une phrase (et son label, le token suivant la phrase), ou on peut considérer le token lui-même</mark> (l’input étant la phrase le précédent, le label le token en question, par exemple). Mais il est important de bien définir de quoi on parle quand on parle de “datapoint”.</p> <h3 id="141-linfluence-à-léchelle-de-la-phrase-z_m">1.4.1 L’influence à l’échelle de la phrase $z_m$</h3> <p>Supposons que $z_m$ soit cette phrase “le chat est gris”, soit ce datapoint:</p> \[[\text{BOS, le, chat, est, gris}] \rightarrow \text{[EOS]}\] <p>Vu qu’on est dans un cas autorégressif (c’est-à-dire que les tokens sont prédits à partir des tokens précédents de la séquence) :</p> <p>Si \(z_m\) de taille \(T\) :</p> \[\nabla_\theta L(z_m, \theta) = \sum_{t=1}^T(-\nabla_\theta \log p(z_{m,t} \mid z_{m, \lt t}, \theta))\] <p>Nous, \(z_m\) est de taille 6 :</p> \[\begin{split} \nabla_\theta L(z_m, \theta) &amp;= -\nabla_\theta \log p(\text{le} \mid \text{[BOS]}, \theta) \\ &amp;\quad - \nabla_\theta \log p(\text{chat} \mid \text{[[BOS], le]}, \theta) \\ &amp;\quad - \nabla_\theta \log p(\text{est} \mid \text{[BOS], le, chat}, \theta) \\ &amp;\quad - \nabla_\theta \log p(\text{gris} \mid \text{[BOS], le, chat, est}, \theta) \\ &amp;\quad - \nabla_\theta \log p(\text{[EOS]} \mid \text{[BOS], le, chat, est, gris}, \theta) \end{split}\] <h3 id="142-linfluence-à-léchelle-des-tokens-t-dans-la-phrase-z_m">1.4.2 L’influence à l’échelle des tokens $t\;\;$ dans la phrase $z_m$</h3> <p>On peut aussi considérer l’échelle du token, comme on l’a mis plus haut, en considérant l’input comme étant la phrase précédant ce token, et le label ce token en question.</p> <p>On a ici $\nabla_\theta L(z_m, \theta)$ qui est la somme des gradients de la loss au niveau de chaque token. Du coup, il suffit de prendre $- \nabla_\theta \log p(\text{gris} \mid \text{[BOS], le, chat, est}, \theta)$ pour avoir l’influence du token gris dans la séquence par exemple. On peut ainsi avoir l’information token par token.</p> <p>Et ça c’est ce qu’on obtient ici (cf <a href="https://arxiv.org/pdf/2308.03296">Studying Large Language Model Generalization with Influence Functions, Grosse 2023</a>) :</p> \[\nabla_\theta L(\text{token t dans } z_m, \theta) = \nabla_\theta \log p(\text{token t} \mid \text{ce qui est avant token t dans } z_m, \theta)\] <p>On obtient donc la formule:</p> \[I_f(z_{m,t}) = \nabla_{\theta}f_{\theta_\varepsilon(z_\text{train})}(x)^{T} H^{-1} \nabla_{\theta}\log p(z_{m,t}\mid z_{m,\lt t}, \theta)\] <p>Prenons l’exemple suivant: on prend $f = \log p(\text{“hydrogen and oxygen”} \mid \text{“Water is composed of”})$ et $z_m$ qui est le texte ci-dessous. On peut afficher l’influence token par token dans le texte:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/image-480.webp 480w,/assets/img/explainability_llms/image-800.webp 800w,/assets/img/explainability_llms/image-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/explainability_llms/image.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="15-le-cas-des-llms-beaucoup-de-données-dentraînement-eg-36-trillions-de-tokens-pour-qwen3--query-batching-ou-semantic-matching-pour-ne-pas-calculer-linfluence-sur-toutes-les-données-trop-coûteux">1.5 Le cas des LLMs: beaucoup de données d’entraînement (eg. 36 trillions de tokens pour Qwen3) =&gt; query batching ou semantic matching pour ne pas calculer l’influence sur toutes les données (trop coûteux)</h2> <p>Le papier <a href="https://arxiv.org/pdf/2308.03296">Studying Large Language Model Generalization with Influence Functions</a> propose une approche pour éviter de calculer les gradients de tous les exemples d’entraînement candidats pour chaque requête d’influence. Pour cela, ils “filtrent” les données d’entraînement par rapport à la phrase test via un filtrage TF-IDF et une approche qu’ils introduisent de “query batching”.</p> <h3 id="151-le-filtrage-tf-idf">1.5.1 Le filtrage TF-IDF</h3> <p>Le filtrage TF-IDF utilise une technique classique de recherche d’information pour présélectionner les séquences d’entraînement les plus susceptibles d’être influentes. L’intuition derrière est que les séquences pertinentes devraient avoir au moins un certain chevauchement de tokens avec la requête.</p> <p>Ils retiennent les top 10,000 séquences selon le score TF-IDF Calcul d’influence et calculent les influences uniquement sur ces séquences présélectionnées.</p> <h3 id="152-le-query-batching">1.5.2 Le Query-Batching</h3> <p>Dans un LLM, on a beaucoup d’exemples $z_m$ d’entraînement. Donc, on calcule séparemment $\nabla_\theta\mathcal{L}(z_m, \theta_\varepsilon(z_\text{train}))$ et $\nabla_{\theta} f(\theta_\varepsilon(z_\text{train}))^\top \, H^{-1}$ qui se calcule en une fois.</p> <p>Pour stocker de nombreux gradients de requêtes en mémoire ($\nabla_\theta\mathcal{L}(z_m, \theta_\varepsilon(z_\text{train})) \; \forall \; z_m$), ils approximent chaque matrice de gradient préconditionné comme étant de rang faible (rank-32 dans leurs expériences).</p> <p>Ainsi, pour chaque requête, ils n’ont pas à refaire les calculs! Ils ont juste à calculer $\nabla_{\theta} f(\theta_\varepsilon(z_\text{train}))$.</p> <h2 id="16-le-cas-des-llms-plusieurs-couches-dentraînement-pretraining-fine-tuning-alignement---multi-stage-influence-functions">1.6 Le cas des LLMs: plusieurs couches d’entraînement (pretraining, fine-tuning, alignement, …) =&gt; multi-stage influence functions</h2> <p>Le papier <a href="https://arxiv.org/pdf/2505.05017">Scalable Multi-Stage Influence Function for Large Language Models via Eigenvalue-Corrected Kronecker-Factored Parameterization</a> explique que la fonction d’influence classique $I_f(z_{m,t}) = \nabla_{\theta}f_{\theta_\varepsilon(z_\text{train})}(x)^{T} H^{-1} \sum_{t=1}^T(-\nabla_\theta \log p(z_{m,t} \mid z_{m, &lt;t}, \theta))$ permet de quantifier l’impact d’une phrase d’entraînement sur les prédictions du modèle. Cependant, on a des modèles qui sont passés par plusieurs phases d’entraînement pour les LLMs (avec plusieurs données différentes). En effet, les LLMs sont pré-entraînés (modèles “base”), puis instruct-tuné (modèles “chat”), puis passent par du reinforcement learning (ou du “faux” réinforcement learning (DPO, …)) pour la phase d’alignement. Donc notre formule ne marche plus si on prend un modèle “chat” par exemple (les 3/4 des modèles qu’on trouve sur huggingface) et qu’on veut calculer l’influence d’une phrase du jeu de pre-entraînement par exemple. Or, ce sont ces données de pré-entraînement qui nous intéressent puisque la majorité des connaissances d’un LLM sont acquises pendant le pré-entraînement. Sans pouvoir les tracer, on ne peut pas expliquer d’où viennent les réponses du modèle.</p> <p>Ainsi, les auteurs du papier proposent une connexion entre l’espace des paramètres du modèle pré-entraîné et celui du modèle fine-tuné. L’intuition est que le fine-tuning ne devrait pas trop éloigner les paramètres de leur état pré-entraîné. On reformule donc l’objectif de fine-tuning avec une contrainte de proximité euclidienne :</p> \[\theta^{ft} = \arg\min_\theta \mathcal{L}_{ft}(\theta) + \frac{\alpha}{2}||\theta - \theta^{pt}||_2^2\] <p>où :</p> <ul> <li>$\mathcal{L}_{ft}(\theta)$ est la loss de fine-tuning</li> <li>$\alpha \in \mathbb{R}^+$ est un hyperparamètre contrôlant la proximité</li> <li>\(\|\theta - \theta^{pt}\|_2^2\) est la distance euclidienne entre les paramètres du modèle pré-entraîné avec le modèle fine-tuné (final)</li> </ul> <p>Avec cette reformulation, on peut dériver la fonction d’influence multi-étapes :</p> \[I_f(z_m) = \nabla_\theta f(\theta^{ft})^T \left(\nabla^2_\theta \mathcal{L}_{ft}(\theta^{ft}) + \alpha I\right)^{-1} \left(\nabla^2_\theta \mathcal{L}_{pt}(\theta^{pt})\right)^{-1} \nabla_\theta \mathcal{L}(z_m, \theta^{pt})\] <p>Ainsi, on a 2 hessiennes:</p> <ul> <li><strong>Hessienne du pré-entraînement</strong> : \(\left(\nabla^2_\theta \mathcal{L}_{pt}(\theta^{pt})\right)^{-1}\) <ul> <li>Calculée aux paramètres $\theta^{pt}$ (modèle pré-entraîné)</li> <li>Capture la courbure de la loss de pré-entraînement</li> </ul> </li> <li><strong>Hessienne du fine-tuning</strong> : \(\left(\nabla^2_\theta \mathcal{L}_{ft}(\theta^{ft}) + \alpha I\right)^{-1}\) <ul> <li>Calculée aux paramètres $\theta^{ft}$ (modèle fine-tuné)</li> <li>Inclut le terme de régularisation $\alpha I$ qui encode la contrainte de proximité</li> </ul> </li> </ul> <p>Cette double inversion de Hessienne permet de :</p> <ul> <li><strong>Première inversion</strong> : Transformer le gradient de l’exemple de pré-entraînement en changement de paramètres</li> <li><strong>Seconde inversion</strong> : Propager ce changement à travers le fine-tuning pour voir son impact final</li> </ul> <p>C’est comme si on “remontait” l’influence à travers deux étapes d’entraînement successives.</p> <h2 id="17-beaucoup-de-sujets-récents-de-recherche-utilisent-les-fonctions-dinfluence-pour-déterminer-les-données-utiles-quon-peut-utiliser-pour-fine-tuner-le-modèle-pour-améliorer-la-génération-dun-llm-ou-ajouter-une-connaissance-au-modèle-par-exemple">1.7 Beaucoup de sujets récents de recherche utilisent les fonctions d’influence pour déterminer les données utiles (qu’on peut utiliser pour fine-tuner le modèle) pour améliorer la génération d’un LLM, ou ajouter une “connaissance” au modèle par exemple</h2> <p>Les fonctions d’influence sont un bon moyen d’évaluer l’impact de chaque exemple de données sur les performances d’un LLM dans un domaine donné. Pour l’instruction-tuning, elles permettent de mesurer précisément quels couples question-réponse contribuent le plus à la qualité des réponses générées dans un domaine donné. Pour le continual pretraining, elles identifient les phrases dont l’ajout ou la suppression modifie le plus la capacité du modèle à maîtriser un vocabulaire ou un style spécifique. Les fonctions d’influence sont de plus en plus utilisées pour affiner le corpus d’apprentissage et maximiser le gain de performance du LLM sur un domaine cible. Voici quelques exemples de papier faisant cela:</p> <p><a href="https://arxiv.org/pdf/2505.12762">IDEAL: Data Equilibrium Adaptation for Multi-Capability Language Model Alignment</a></p> <details> <summary style="cursor: pointer;">Cliquez pour voir l’illustration IDEAL</summary> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/IDEAL-480.webp 480w,/assets/img/explainability_llms/IDEAL-800.webp 800w,/assets/img/explainability_llms/IDEAL-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/explainability_llms/IDEAL.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </details> <p><a href="https://arxiv.org/pdf/2505.12250">Not All Documents Are What You Need for Extracting Instruction Tuning Data</a></p> <details> <summary style="cursor: pointer;">Cliquez pour voir l’illustration de EQUAL</summary> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/EQUAL-480.webp 480w,/assets/img/explainability_llms/EQUAL-800.webp 800w,/assets/img/explainability_llms/EQUAL-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/explainability_llms/EQUAL.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </details>]]></content><author><name></name></author><category term="sample-posts"/><category term="XAI,"/><category term="influence"/><summary type="html"><![CDATA[Détail de ma compréhension des fonctions d'influence appliquées aux LLMs]]></summary></entry><entry><title type="html">Les approches d’explicabilité des LLMs</title><link href="https://camillebrl.github.io/blog/2025/explainability_llm_generation/" rel="alternate" type="text/html" title="Les approches d’explicabilité des LLMs"/><published>2025-07-02T22:00:00+00:00</published><updated>2025-07-02T22:00:00+00:00</updated><id>https://camillebrl.github.io/blog/2025/explainability_llm_generation</id><content type="html" xml:base="https://camillebrl.github.io/blog/2025/explainability_llm_generation/"><![CDATA[<p>Un LLM génère des tokens à partir d’autres tokens selon un processus probabiliste. <mark>Pour chaque séquence de tokens fournie en entrée, le modèle retourne une distribution de probabilité sur l'ensemble des tokens suivants possibles</mark>. Un token représente un élément du vocabulaire du modèle, dont la taille est fixe et définie dans la configuration du tokenizer ou du modèle lui-même. Prenons l’exemple des modèles Qwen avec un <mark>vocabulaire de 151 936 tokens : à chaque étape de génération, le modèle calcule une distribution de probabilité sur ces 151 936 possibilités</mark>. De manière autorégressive, chaque nouveau token généré est ajouté à la séquence d’entrée pour prédire le suivant, et ainsi de suite. <mark>Comprendre pourquoi un modèle génère certains tokens plutôt que d'autres soulève plusieurs questions cruciales</mark>. La séquence d’entrée (prompt) peut contenir des éléments variés, notamment des chunks de texte dans le cas du RAG (Retrieval-Augmented Generation), qui servent de contexte pour répondre aux questions. Pour véritablement comprendre le processus de génération, nous devons répondre à trois interrogations principales :</p> <ul> <li><mark>Quelles parties spécifiques du prompt influencent la génération de quels tokens ?</mark> Cette question est particulièrement pertinente dans le contexte du RAG où différents passages peuvent avoir des impacts variés sur la réponse finale.</li> <li><mark>Sur quelles connaissances acquises pendant l'entraînement le modèle s'appuie-t-il pour générer chaque token ?</mark> Comment distinguer ce qui provient du prompt de ce qui provient de la mémoire du modèle ?</li> <li><mark>Quelles parties du modèle (couches, têtes d'attention, neurones) sont responsables de quelles décisions ?</mark> Pourquoi des modèles de tailles différentes, même issus de la même famille, produisent-ils des réponses différentes ? Pour répondre à ces questions, nous devons nous tourner vers les approches d’explicabilité. Nous pouvons classifier les approches d’explicabilité des LLMs en 4 “familles”, comme présentées dans le papier <a href="https://arxiv.org/pdf/2506.05451">Interpretation Meets Safety: A Survey on Interpretation Methods and Tools for Improving LLM Safety</a>:</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/survey-480.webp 480w,/assets/img/explainability_llms/survey-800.webp 800w,/assets/img/explainability_llms/survey-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/explainability_llms/survey.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/slide1-480.webp 480w,/assets/img/explainability_llms/slide1-800.webp 800w,/assets/img/explainability_llms/slide1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/explainability_llms/slide1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>celles de <mark>"training data attribution" (ou "training" dans l'image), identifiant les données d'entraînement qui ont un fort impact (positif ou négatif) sur la génération du LLM</mark> , notamment à l’aides des fonctions d’influence,</li> <li>celles de <mark>"context attribution" (ou "input" dans l'image), identifiant quelles parties des données d'entrée (input) a un impact sur quelle partie de la génération du LLM</mark> (à l’aide des cartes de saillance et des cartes d’attention notamment),</li> <li>celles d’<mark>explicabilité mécanistique (ou "inference" dans l'image), consistant à trouver les circuits / éléments du LLM qui capturent certains concepts</mark>, avec des circuits identifiés par approches d’observation de l’espace latent (ACP sur les états cachés du LLM à différents niveaux du réseau), de dictionnary learning pour visualiser quels concepts sont capturés par chaque neurones / groupes de neurones, ou par “patching”, en modifiant certains états cachés pour voir l’effet sur le modèle.</li> <li>Celles <mark>étudiant la génération des modèles (ou "generation" dans l'image), notamment des modèles dits de "raisonnement" (thinking) qui détaillent leur raisonnement dans la réponse qu'ils fournissent</mark> : cette approche est d’autant plus utile aujourd’hui que les agents consistent en plusieurs appels du LLMs (un appel qui retourne un appel à un outil (tool), un autre qui retourne l’appel à un autre outil, etc).</li> </ul> <p>Nous allons <mark>d'abord observer les approches d'explicabilité par les données, que ce soit par les données d'entraînement ou par les données d'input (du prompt)</mark>. L’objectif est d’établir des liens causaux clairs entre ces sources de données et les tokens générés, qu’ils soient pris individuellement ou en groupes de tokens.</p> <h1 id="i-approches-tda-training-data-attribution-pour-comprendre-quelles-connaissances-apprises-dans-lentraînement-ont-été-influentes-dans-la-génération-de-quels-tokens">I/ Approches TDA (training data attribution) pour comprendre quelles connaissances apprises dans l’entraînement ont été influentes dans la génération de quels tokens</h1> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/slide2.PNG" sizes="95vw"/> <img src="/assets/img/explainability_llms/slide2.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/slide3-480.webp 480w,/assets/img/explainability_llms/slide3-800.webp 800w,/assets/img/explainability_llms/slide3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/explainability_llms/slide3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/slide4-480.webp 480w,/assets/img/explainability_llms/slide4-800.webp 800w,/assets/img/explainability_llms/slide4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/explainability_llms/slide4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/slide5-480.webp 480w,/assets/img/explainability_llms/slide5-800.webp 800w,/assets/img/explainability_llms/slide5-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/explainability_llms/slide5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="1-les-fonctions-dinfluence-pour-estimer-limpact-dun-exemple-dentraînement-sur-la-prédiction-dun-exemple-de-test">1. Les fonctions d’influence pour estimer l’impact d’un exemple d’entraînement sur la prédiction d’un exemple de test</h2> <p>Si on cherche à estimer l’<mark>impact qu'aurait un exemple d'entraînement sur la perte d'un exemple de test (ou sur plusieurs résultats du modèle sur un jeu de données test) à un exemple d'entraînement</mark> (qu’il soit dans le jeu de données d’entraînement de base ou pas), on peut utiliser les fonctions d’influence.</p> \[\mathrm{Influence}\bigl(z_{\mathrm{train}}\to z_{\mathrm{test}}\bigr) = \frac{d}{d\varepsilon}\, \mathcal{L}\bigl(z_{\rm test},\,\theta_\varepsilon(z_\text{train})\bigr)\Big|_{\varepsilon=0} = -\nabla_\theta \,\mathcal{L}\bigl(z_{\mathrm{test}},\,\hat{\theta} \bigr)\,H_\theta^{-1}(\hat{\theta})\,\nabla_\theta \mathcal{L}(z_{\rm train},\hat{\theta})\] \[\mathrm{Influence}\bigl(z_{\mathrm{train}}\to f(x)\bigr) = \left.\frac{d}{d\varepsilon}\bigl(f_{\theta_{\varepsilon}(z_{\mathrm{train}})}(x)\bigr)\right|_{\varepsilon=0} = - \nabla_\theta f_{\hat{\theta}}(x)^\top \, H_\theta(\hat{\theta})^{-1} \, \nabla_\theta \mathcal{L}\bigl(x_{\mathrm{train}},\hat{\theta}\bigr)\] <p>Pour voir l’explication de la formule et plus de détails sur comment c’est utilisé dans les LLMs, vous pouvez aller voir <a href="camillebrl.github.io/blog/2025/influence_functions_applied_to_llms/">ce post</a>.</p> <p>La librairie <a href="https://github.com/pomonam/kronfluence">Kronfluence</a> permet de calculer l’influence dans le cas des LLMs. Des repo github proposent des tutos pour plusieurs de libs de calcul de l’influence dans les modèles de deep learning, comme <a href="https://github.com/deel-ai/influenciae">Influenciae</a> (btw, c’est une lib d’un labo de Toulouse!). Les fonctions d’influence permettent de répondre à ce genre de questions:</p> <ul> <li>Est-ce que je devrais <mark>ajouter cet exemple dans mon set d'apprentissage pour améliorer les performances de cette prédiction?</mark></li> <li>Quels <mark>exemples d'entraînement ont été utiles à la prédiction</mark> de mon modèle?</li> <li>Le modèle s’est trompé: sur quels exemples d’entraînement s’est-il basé pour cette mauvaise prédiction?</li> </ul> <h2 id="2-les-approches-dattribution-aux-données-dentraînement-par-similarité-sémantique">2. Les approches d’attribution aux données d’entraînement par similarité sémantique</h2> <p>Pour trouver les données d’entraînement influentes pour la génération du LLM, on peut également simplement étudier la similarité sémantique entre la génération du LLM et les données d’entraînement.</p> <p>Le papier <a href="https://aclanthology.org/2024.blackboxnlp-1.33.pdf">Wrapper Boxes: Faithful Attribution of Model Predictions to Training Data</a> compare l’embedding de la génération du LLM à celui de la penultième couche, et entraîne ensuite trois types de « wrapper boxes » sur ces embeddings:</p> <ul> <li>un k-nearest neighbors (kNN) pour retrouver, à chaque inférence, les k exemples d’entraînement les plus proches ;</li> <li>un clustering k-means pour assigner l’input à un cluster dont on peut exposer le centroïde et les exemples qui le composent ;</li> <li>un arbre de décision dont chaque feuille correspond à un sous-ensemble d’exemples d’entraînement utilisés pour la classification.</li> </ul> <details> <summary style="cursor: pointer;">Cliquez pour voir l’illustration de Wrapper Boxes</summary> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/wrapper_boxes-480.webp 480w,/assets/img/explainability_llms/wrapper_boxes-800.webp 800w,/assets/img/explainability_llms/wrapper_boxes-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/explainability_llms/wrapper_boxes.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </details> <h2 id="3-les-approches-dattribution-aux-données-dentraînement-basées-sur-le-gradient">3. Les approches d’attribution aux données d’entraînement basées sur le gradient</h2> <p>Ces approches sont <mark>ressemblantes mais différentes des fonctions d'influence classiques, qu'on a vu plus tôt</mark>. En effet, ces fonctions d’influence calculent comment la perte sur un échantillon test, ou comment une fonction du modèle sur un échantillon donné changerait si on retirait / up-weightait un échantillon d’entraînement. Ici, les approches basées sur le gradient utilisent une notion plus simple: on <mark>calcule la distance entre 2 gradients</mark>: celui de la loss sur l’échantillon d’entraînement dont on veut voir l’influence avec celui de la loss sur l’exemple cible sur lequel on veut mesurer l’influence. Il y a plusieurs façons de calculer la distance entre les gradients: par produit scalaire, distance cosinus, …</p> <p>Le papier <a href="https://arxiv.org/pdf/2502.11411">Detecting and Filtering Unsafe Training Data via Data Attribution</a> identifie les données d’entraînement “unsafe” à l’aide d’une distance cosinus entre les gradients d’un exemple “unsafe” et des exemples du jeu d’entraînement:</p> \[\begin{split} \text{DABUF-inf}(z_\text{train}, z\text{target}) &amp;= \eta \cdot \cos(\nabla L(z_\text{target}; \theta), \nabla L(z_\text{train}; \theta)) \\ &amp;= \eta \cdot \frac{\nabla L(z_\text{target}; \theta) \cdot \nabla L(z_\text{train}; \theta)}{||\nabla L(z_\text{target}; \theta)|| \cdot ||\nabla L(z_\text{train}; \theta)||} \end{split}\] <p>On a aussi <a href="https://github.com/frederick0329/TracIn">TracIn</a>, qui lui ne calcule pas la distance cosinus entre les gradient mais effectue un produit scalaire entre eux.</p> <h2 id="4-les-approches-dattribution-aux-données-dentraînement-basées-sur-les-valeurs-de-shapley">4. Les approches d’attribution aux données d’entraînement basées sur les valeurs de Shapley</h2> <p>les valeurs de shapley estiment l’effet moyen de sous-ensembles d’un dataset en supprimant certaines variables explicatives (ici, certains datapoints du dataset) et en mesurant la différence de prédiction du modèle, puis en moyennant ces effets marginaux sur toutes les permutations possibles.</p> <p>L’utilisation des valeurs de Shapley pour déterminer la contribution de datapoints à la prédiction du modèle peut être vue comme une évaluation des contributions marginales <a href="https://arxiv.org/pdf/2110.14049">Beta Shapley: a Unified and Noise-reduced Data Valuation Framework for Machine Learning</a>. La contribution marginale d’un point de données $z_i$ à un sous-ensemble de taille $k$ de l’ensemble d’entraînement est définie comme:</p> \[\Delta^{D_N}_{z_i}(k, D_T) := \frac{1}{\binom{n-1}{k}} \sum_{\substack{S \subseteq N \setminus \{i\} \\ |S|=k}} \bigl(U(S\cup\{i\},D_T) - U(S,D_T)\bigr).\] <p>L’approche Leave-one-out (cf fonctions d’influences plus haut) considère la variation de la précision du modèle lorsqu’on retire le point cible de l’ensemble d’entraînement. Cette LOO peut être interprétée comme la contribution marginale de $z_i$ à $D_N\setminus{z_i}$. Les approches “leave-one-out” ou ses approximations (comme les fonctions d’influence, cf plus haut), ne considèrent que la contribution marginale de $z_i$ à un unique sous-ensemble de taille $n-1$ (avec $n$ étant la taille du jeu d’entraînement du modèle). Ainsi, le score d’instance d’un exemple chute fortement lorsqu’un autre exemple similaire apparaît dans l’ensemble d’entraînement (cf papier <a href="https://arxiv.org/pdf/1904.02868">Data Shapley: Equitable Valuation of Data for Machine Learning</a>), ce qui réduit la fiabilité et la robustesse de la mesure.</p> <p>Au lieu de ne considérer qu’un sous-ensemble, la valeur de Shapley la généralise en tenant compte de l’impact de $z_i$ sur tous les sous-ensembles. En gros, au lieu de considérer: \(g^{\mathrm{LOO}}(z_i, D_T, D_N) = \Delta^{D_N}_{z_i}(n-1, D_T).\) On considère: \(g^{\mathrm{Shap}}(z_i, D_T, D_N) = n^{-1} \sum_{k=0}^{n-1} \Delta^{D_N}_{z_i}(k, D_T).\) Ceci peut être vu comme la moyenne des contributions marginales de $z_i$ à des sous-ensembles de toutes tailles possibles.</p> <p>Cependant, calculer la valeur de Shapley sur un ensemble d’entraînement de taille $n$ exige un nombre exponentiel d’ajustements du modèle (soit $n!$), ce qui nécessite ajustements lorsque les datasets sont très grands (comme pour les LLMs).</p> <p>Le papier <a href="https://openreview.net/pdf?id=WSpPC1Jm0p">Helpful or Harmful Data? Fine-tuning-free Shapley Attribution for Explaining Language Model Predictions</a> propose une méthode d’attribution aux données d’entraînement basée sur les valeurs de Shapley, appelée FreeShap (Fine-tuning-free Shapley Attribution), basée sur la théorie du noyau tangent neuronal (NTK) pour éviter de multiples réentraînements du modèle.</p> <p>En effet, la théorie du noyau tangent neuronal (NTK) a été proposée pour étudier la dynamique d’entraînement des réseaux de neurone. Les travaux <a href="https://arxiv.org/pdf/1904.11955">On Exact Computation with an Infinitely Wide Neural Net</a> et <a href="https://www.researchgate.net/publication/339642345_Toward_a_theory_of_optimization_for_over-parameterized_systems_of_non-linear_equations_the_lessons_of_deep_learning">Toward a theory of optimization for over-parameterized systems of non-linear equations: the lessons of deep learning</a> montrent que l’entraînement d’un réseau entièrement suffisamment large est équivalent à la résolution d’une régression par noyau avec le NTK à l’initialisation aléatoire. Cependant, deux défis se posent lorsque l’on applique la théorie NTK au fine-tuning des LMs : (1) l’utilisation quasi systématique de poids préentraînés au lieu d’une initialisation aléatoire ; (2) l’emploi de prompts. Le papier de 2023 <a href="https://arxiv.org/pdf/2210.05643">A Kernel-Based View of Language Model Fine-Tuning</a> étend l’analyse pour montrer que résoudre la régression par noyau avec le NTK empirique (eNTK) calculé à partir des poids préentraînés peut reproduire le fine-tuning basé sur des prompts. De plus, la régression par noyau utilisant l’eNTK obtient des performances comparables au fine-tuning en computer vision (cf le papier <a href="https://arxiv.org/pdf/2203.06176">More Than a Toy: Random Matrix Models Predict How Real-World Neural Representations Generalize</a>) et en NLP (cf papier <a href="https://arxiv.org/pdf/2210.05643">A Kernel-Based View of Language Model Fine-Tuning</a>), ce qui en fait un substitut prometteur.</p> <p>L’eNTK se calcule à partir du Jacobien de la sortie du modèle pour un point de données $x_i$ : \(\psi(x_i)\;:=\;\frac{\partial f(x_i;\theta_0)}{\partial \theta_0} \;\in\;\mathbb{R}^{C\times P},\) où $\theta_0\in\mathbb{R}^P$ désigne les poids préentraînés.<br/> Pour un ensemble d’entraînement $D_S$ d’indices $S:={1,\dots,k}$, on pose \(X_S \;:=\; \begin{bmatrix}x_1 \\ \vdots \\ x_k\end{bmatrix} \in\mathbb{R}^{k\times d}, \quad Y_S \;:=\; \begin{bmatrix} y^1_1 &amp; \cdots &amp; y^C_1 \\ \vdots &amp; &amp; \vdots \\ y^1_k &amp; \cdots &amp; y^C_k \end{bmatrix} \;\in\;\{0,1\}^{k\times C},\) où $y^p_j=1$ si la classe réelle de $x_j$ est $p$.<br/> Une instance de test $x_t$ est alors prédite par le modèle de régression eNTK : \(f_S^{\mathrm{eNTK}}(x_t) \;=\; K(x_t, X_S)^\top \,K(X_S, X_S)^{-1}\, Y_S, \tag{5}\) avec \(K(x_t, X_S) =\bigl[\psi(x_t)\,\psi(x_i)^\top\bigr]_{i=1}^k \;\in\;\mathbb{R}^{C\times k}, \quad K(X_S, X_S) =\bigl[\psi(x_i)\,\psi(x_j)^\top\bigr]_{i,j=1}^k \;\in\;\mathbb{R}^{kC\times kC}.\)</p> <h1 id="ii-approche-input-prompt-attribution-mesurer-sur-quelles-parties-de-linput-le-modèle-sest-basé-pour-faire-sa-prédiction">II/ Approche Input (prompt) attribution: mesurer sur quelles parties de l’input le modèle s’est basé pour faire sa prédiction</h1> <p>Ces approches consistent à attributer la génération du LLM à des tokens du prompt pour comprendre quel(s) token(s) est/sont responsable(s) de quelle(s) parties de la génération du LLM.</p> <p>Dans ce <a href="https://arxiv.org/pdf/2502.15886">papier: A Close Look at Decomposition-based XAI-Methods for Transformer Language Models, 2025</a>, ils comparent plusieurs approches (qu’on va détailler ci-dessous) de context / input attribution:</p> <details> <summary style="cursor: pointer;">Cliquez la comparaison des tokens du prompts influents sur la génération du LLM par différentes approches comme présentées dans le papier</summary> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/comparison-480.webp 480w,/assets/img/explainability_llms/comparison-800.webp 800w,/assets/img/explainability_llms/comparison-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/explainability_llms/comparison.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </details> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/slide6.PNG" sizes="95vw"/> <img src="/assets/img/explainability_llms/slide6.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/slide7-480.webp 480w,/assets/img/explainability_llms/slide7-800.webp 800w,/assets/img/explainability_llms/slide7-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/explainability_llms/slide7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/slide8-480.webp 480w,/assets/img/explainability_llms/slide8-800.webp 800w,/assets/img/explainability_llms/slide8-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/explainability_llms/slide8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/slide9-480.webp 480w,/assets/img/explainability_llms/slide9-800.webp 800w,/assets/img/explainability_llms/slide9-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/explainability_llms/slide9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="1-les-approches-basées-sur-lattention">1. Les approches basées sur l’attention</h2> <p>Ces approches affichent les poids d’attention de chaque token générés par rapport aux tokens du prompt (cas auto-regressif). Des poids d’attention élevés signifient que le modèle a donné plus d’importance à ce token pour la génération du token en question.</p> <p>Cependant, les poids d’attention sont donnés dans le réseau au niveau de chaque tête d’attention. Il est compliqué d’obtenir une attention “globale” que le modèle donne aux tokens. En effet, les scores d’attention ne montrent que les connexions locales (au sein de chaque tête de chaque couche du réseau), et surtout, ces patterns d’attention ne donnent tel quel aucune information : il s’agit de patterns uniformes. Plusieurs approches visent donc à modéliser ce flux d’attention global à travers le réseau pour tracer l’influence de chaque token d’entrée sur les tokens prédits.</p> <p>Tout l’enjeu est d’arriver à mesurer, de manière la plus fidèle et informative possible, le flux d’attention au sein du réseau pour donner une vue globale de l’influence de chaque token d’entrée sur un token généré.</p> <h3 id="11-visualisation-du-llm-comme-un-graphe-dattentions-pour-obtenir-la-contribution-finale-du-token">1.1 Visualisation du LLM comme un graphe d’attentions pour obtenir la contribution finale du token</h3> <p>Le papier <a href="https://aclanthology.org/2020.acl-main.385.pdf">Quantifying Attention Flow in Transformers</a> propose deux approches: une d’attention rollout et une d’attention flow. Ces 2 approches modélisent le LLM comme un graphe d’attention et utilisent chacune une méthode différente pour calculer l’attention globale d’un token sur le token prédit.</p> <p>La première approche proposée est dite d’“attention rollout”. Elle consiste à multiplier récursivement les matrices d’attention à travers toutes les couches. L’idée est de représenter le LLM comme un graphe d’attentions: Si chaque arête représente la proportion d’information transférée, multiplier les poids le long d’un chemin donne le flux total.</p> <p>La seconde approche est dite d’“attention flow”, avec dans l’idée de traiter le graphe d’attention comme un réseau de flux (flow network). Il utilise un algorithme de flux maximum où les capacités des arêtes sont les poids d’attention. Dans cette approche, le poids d’un chemin est le minimum des poids (pas le produit).</p> <details> <summary style="cursor: pointer;">Cliquez pour voir l’illustration des poids d'attention selon ces 2 méthodes</summary> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/vis_attn-480.webp 480w,/assets/img/explainability_llms/vis_attn-800.webp 800w,/assets/img/explainability_llms/vis_attn-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/explainability_llms/vis_attn.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </details> <h3 id="12-la-contribution-globale-dun-token-associée-aux-logits-au-sein-des-différents-blocs-dattention-et-mlp">1.2 La contribution globale d’un token associée aux logits au sein des différents blocs d’attention (et MLP)</h3> <p>ALTI-Logit <a href="https://aclanthology.org/2023.acl-long.301.pdf">Explaining How Transformers Use Context to Build Predictions</a> suit comment chaque token d’entrée contribue à la prédiction finale en traversant le réseau, en prenant en compte la contribution de chaque élément au logit, ainsi que l’ensemble des transformations (notamment au niveau du MLP) réalisées sur les scores d’attention pour évaluer cette contribution. En gros, pour calculer la contribution finale d’un token, ALTI-logit trace comment ce token (au logit du mot évalué) se propage à travers les connexions d’attention et les connexions résiduelles de toutes les couches, en tenant compte du mélange d’information entre tokens à chaque étape.</p> <details> <summary style="cursor: pointer;">Cliquez pour voir l’illustration la composition d'un block transformers</summary> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/transformer_layer-480.webp 480w,/assets/img/explainability_llms/transformer_layer-800.webp 800w,/assets/img/explainability_llms/transformer_layer-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/explainability_llms/transformer_layer.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </details> \[\begin{aligned} x^L_T &amp;= x^0 + \sum_{l=1}^L \Bigl(\Delta_{\mathrm{MHA}}^l + \Delta_{\mathrm{MLP}}^l\Bigr),\\ \Delta_{\mathrm{MHA}}^l &amp;= \text{contribution du bloc MHA de la couche }l,\\ \Delta_{\mathrm{MLP}}^l &amp;= \text{contribution du bloc MLP de la couche }l. \end{aligned}\] <p>Pour chaque couche $l$ : \(\begin{aligned} \Delta_{\mathrm{MLP}}^l &amp;= \mathrm{sortie}^l_{\mathrm{MLP}} \;-\; \mathrm{entrée}^l_{\mathrm{MLP}},\\ \mathrm{relevance}_{\mathrm{MLP}}^l &amp;= \Delta_{\mathrm{MLP}}^l \;\cdot\; U_w, \end{aligned}\) où $U_w$ est le vecteur d’unembedding de sortie pour le token suivant.</p> <p>\textbf{Exemple (couche 10)} : \(\begin{aligned} \mathrm{entrée}_{\mathrm{MLP}}^{10} &amp;= [0.1,\;0.2,\;\dots],&amp; \mathrm{sortie}_{\mathrm{MLP}}^{10} &amp;= [0.3,\;0.5,\;\dots],\\ \Delta_{\mathrm{MLP}}^{10} &amp;= [0.2,\;0.3,\;\dots],&amp; \Delta_{\mathrm{MLP}}^{10} \cdot \mathrm{embedding}_{\text{dort}} &amp;= 1.5. \end{aligned}\)</p> <details> <summary style="cursor: pointer;">Cliquez pour voir l’illustration de ALTI-logit</summary> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/alti_logit-480.webp 480w,/assets/img/explainability_llms/alti_logit-800.webp 800w,/assets/img/explainability_llms/alti_logit-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/explainability_llms/alti_logit.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </details> <h3 id="13-visualisation-de-lattention-sa-propagation-dans-le-réseau-comme-un-graphe-à-plusieurs-niveaux">1.3 Visualisation de l’attention (sa propagation dans le réseau) comme un graphe à plusieurs niveaux</h3> <p>On peut voir l’attention comme un graphe à plusieurs niveaux: en effet, il y a d’abord les tokens (premier niveau, groupe A) sur lequels le token prédit par le LLM porte son attention. Mais il y a ensuite les tokens sur lesquels les tokens du groupe A portent leur attention (second niveau, groupe B), et ainsi dessuite. On peut remonter ainsi l’attention à plusieurs niveaux.</p> <p>Afin d’identifier comment les tokens du prompt sont traitées par le LLM dans ses différentes têtes d’attention pour amener à l’output, le papier <a href="https://arxiv.org/pdf/2402.14811">Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking</a> teste d’abord toutes les têtes d’attention pour voir lesquelles, quand patchées (modification par une version bruitée de la tête d’attention: ici obtenue à l’aide de 2 fine-tuning différents: l’un bruité et l’autre d’origine), affectent le plus le logit final. Les têtes qui causent la plus grande chute de performance sont celles qui “regardent” directement la réponse correcte (Ces têtes forment le Groupe A). Le papier cherche ensuite quelles têtes d’attention ont un effet direct important sur les têtes du Groupe A, en patchant les chemins allant de candidats potentiels vers le Groupe A.</p> <details> <summary style="cursor: pointer;">Cliquez pour voir l’illustration de l'approche de la propagation de l'attention par groupes</summary> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/path_patching2-480.webp 480w,/assets/img/explainability_llms/path_patching2-800.webp 800w,/assets/img/explainability_llms/path_patching2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/explainability_llms/path_patching2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </details> <p>Le papier <a href="https://arxiv.org/pdf/2502.01951">On the Emergence of Position Bias in Transformers</a> représente en effet le masque d’attention comme un graphe $G$ dirigé où:</p> <ul> <li>Les nœuds représentent les tokens de la séquence</li> <li>une arrête $(j,i) \in E(G)$ signifie que l’attention du token $i$ est portée sur le token $j$</li> </ul> <p>La propagation de l’information au sein des différentes couches du réseau se fait comme cela:</p> <p>Pour une couche unique: \(X^{(1)} = A^{(0)}X^{(0)}W_V^{(0)}\), Pour 2 couches: \(X^{(2)} = A^{(1)}A^{(0)}X^{(0)}W_V^{(0)}W_V^{(1)}\) Pour t couches: \(X^{(t+1)}_{i,:} = \sum_{j=1}^{N} \underbrace{(A^{(t)} \cdots A^{(0)})_{ij}}_{P^{(t)}(z_i=j|X^{(0)})} \cdot \underbrace{X^{(0)}_{j,:} W_V^{(0)} \cdots W_V^{(t)}}_{f^{(t)}(X^{(0)}_{z_i,:})}\)</p> <p>Le produit cumulatif $P^{(t)} = A^{(t)} \cdots A^{(0)}$ représente la probabilité cumulative que le token $i$ sélectionne le token $j$ comme contexte après $t$ couches. Cela capture tous les chemins possibles de longueur $t+1$ entre les tokens $j$ et $i$ dans le graphe.</p> <p>Le papier <a href="https://aclanthology.org/2024.emnlp-main.731.pdf">Chain and Causal Attention for Efficient Entity Tracking</a> généralise ce pattern d’attention “multi-niveau” en proposant un nouveau mécanisme d’attention qui, en une seule couche, permet toutes les connexions d’attention précédemment faites entre les couches. Pour cela, il utilise une représentation formelle avec le produit cumulatif des matrices d’attention qui permet de capturer toutes les longueurs de chemins en une seule couche. En effet, le papier interprète la matrice d’attention $A$ comme une matrice d’adjacence d’un graphe dirigé pour capturer toutes les dépendances possibles : \(A + A^2 + A^3 + \cdots = A(I - A)^{-1}\), avec $A$ connexions directes (chemins de longueur 1), $A^2$ chemins de longueur 2, $A^3$ chemins de longueur 3, etc. L’attention qu’ils proposent est calculée comme :</p> \[\boxed{Y = (1 - \gamma) \cdot A(I - \gamma A)^{-1}V}\] <p>où $\gamma \in [0, 1)$ est un hyperparamètre qui assure la convergence de la série $A + \gamma A^2 + \gamma^2 A^3 + \cdots$ et permet l’interpolation entre attention standard ($\gamma = 0$) et ChaCAL ($\gamma \approx 1$).</p> <h3 id="14-intérêt-des-approches-basées-sur-lattention-facilement-manipulables-pour-forcer-le-modèle-à-porter--ne-pas-porter-son-attention-sur-certains-tokens">1.4 Intérêt des approches basées sur l’attention: “facilement” manipulables pour “forcer” le modèle à porter / ne pas porter son attention sur certains tokens</h3> <p>Le papier <a href="https://openreview.net/pdf?id=xZDWO0oejD">Tell Your Model Where to Attend: Post-hoc Attention Steering for LLMs</a> force l’attention du LLM vers des parties spécifiques du prompt désignées par l’utilisateur, à la manière dont nous utilisons le gras ou l’italique dans les textes humains pour diriger l’attention du lecteur.</p> <details> <summary style="cursor: pointer;">Cliquez pour voir l’illustration de l'approche PASTA</summary> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/pasta-480.webp 480w,/assets/img/explainability_llms/pasta-800.webp 800w,/assets/img/explainability_llms/pasta-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/explainability_llms/pasta.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </details> <p>Le papier <a href="https://arxiv.org/pdf/2502.09674">The Hidden Dimensions of LLM Alignment: A Multi-Dimensional Analysis of Orthogonal Safety Directions</a> propose une approche plus complexe, pour neutraliser l’attention sur les tokens déclencheurs de jailbreak, qu’il identifie en exploitant la structure multi-dimensionnelle de l’espace résiduel de sécurité, construit comme la transformation linéaire des activations pendant le safety fine-tuning. Les tokens déclencheurs sont identifiés non pas par une seule direction, mais par l’interaction entre une direction dominante qui prédit le comportement de refus global, et plusieurs directions non-dominantes qui capturent des patterns spécifiques de jailbreak. Ces directions sont extraites via SVD de la matrice représentant les changements d’activation.</p> <details> <summary style="cursor: pointer;">Cliquez pour voir l’illustration du papier du "safety residual space"</summary> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/safe_direction-480.webp 480w,/assets/img/explainability_llms/safe_direction-800.webp 800w,/assets/img/explainability_llms/safe_direction-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/explainability_llms/safe_direction.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </details> <p>Cependant, ces approches basées sur l’attention pour mesurer l’importance d’un token du prompt sur la génération du modèle sont limitéesn puisque même si le modèle porte une attention plus forte à un token donné, cela ne veut pas nécessairement dire que ce token exerce une influence plus importante sur les logits. C’est pour ça que les apprioches basées sur le gradient ont plus de sens pour mesurer l’impact des tokens du prompt sur la génération du modèle:</p> <h2 id="2-approches-basées-sur-le-gradient-les-cartes-de-saillance-saliency-maps">2. Approches basées sur le gradient: les cartes de saillance (saliency maps)</h2> <p>Ces approches mesurent comment la sortie du modèle $f(x)$ varie si on modifie chaque token $x_i$ de l’entrée.</p> <p>Plus précisément, les tokens étant représentés par des embeddings de dimension $d_{\text{model}}$, <mark>on calcule le gradient de $f(x)$ par rapport à chaque dimension de l'embedding de chaque token $x_i$</mark>. On obtient donc, pour chaque token, un vecteur gradient de dimension $d_{\text{model}}$. Pour obtenir un score de saillance scalaire par token, on <mark>agrège ces $d_{\text{model}}$ gradients à l'aide de différentes méthodes</mark> :</p> <ul> <li>Norme L2 : $|\nabla_{x_i} f(x)|_2$</li> <li>Norme L1 (somme des valeurs absolues) : $|\nabla_{x_i} f(x)|_1$</li> <li>Produit scalaire avec l’embedding : $\nabla_{x_i} f(x) \cdot x_i$</li> </ul> <p>Pour se faire, des libraires comme <a href="https://github.com/pytorch/captum">Captum</a> ou <a href="https://github.com/inseq-team/inseq">Inseq</a> ou <a href="https://github.com/jacobgil/pytorch-grad-cam">Grad-cam</a> ou <a href="https://github.com/albermax/innvestigate">Investigate</a> ou <a href="https://github.com/SeldonIO/alibi">Alibi</a> existent.</p> <p>J’ai créé un <a href="https://github.com/camillebrl/mirage_ui">repo github</a> qui reproduit un <a href="https://aclanthology.org/2024.emnlp-main.347.pdf">papier (MIRAGE)</a> d’explication de la génération du modèle à partir des éléments du prompt, découpés document par document (cas du RAG).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/mirage_illustration-480.webp 480w,/assets/img/explainability_llms/mirage_illustration-800.webp 800w,/assets/img/explainability_llms/mirage_illustration-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/explainability_llms/mirage_illustration.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Ici, on parle d’attribution de l’importance des tokens du prompt dans la génération du LLM par calcul de gradient, c’est à dire simplement en dérivant la sortie du LLM par rapport aux tokens du prompt. Le problème, c’est que quand la fonction est localement plate (gradient nul), l’attribution est nulle même si l’input est important. L’approche des gradients intégrés (IG) (introduits dans le papier <a href="https://arxiv.org/pdf/1703.01365">Axiomatic Attribution for Deep Networks</a>), résout ces problèmes en intégrant les gradients le long d’un chemin entre une baseline neutre et l’input réel.</p> <p>Soit $x = (x_1, x_2, …, x_n)$ le vecteur d’embeddings correspondant aux tokens du prompt, et $x’ = (x’_1, x’_2, …, x’_n)$ une baseline neutre (typiquement des embeddings nuls ou des tokens de padding). Pour un modèle $F$ produisant une sortie $y = F(x)$, l’importance du $i$-ème token est calculée par :</p> \[IG_i(x) = (x_i - x'_i) \times \int_{\alpha=0}^{1} \frac{\partial F(x' + \alpha \times (x - x'))}{\partial x_i} d\alpha\] <p>Cette formule capture l’accumulation des gradients lorsqu’on interpole linéairement entre la baseline $x’$ et l’entrée réelle $x$. Le terme $(x_i - x’_i)$ représente la différence entre l’embedding du token réel et celui de la baseline, tandis que l’intégrale accumule les sensibilités du modèle tout au long du chemin d’interpolation.</p> <p>En pratique, l’intégrale est approximée par une somme de Riemann (cas discret) :</p> \[IG_i(x) \approx (x_i - x'_i) \times \sum_{k=1}^{m} \frac{\partial F(x' + \frac{k}{m} \times (x - x'))}{\partial x_i} \times \frac{1}{m}\] <p>où $m$ est le nombre de pas d’intégration. Cette méthode garantit deux propriétés importantes : la sensibilité (un token n’ayant aucun impact aura une attribution nulle) et la complétude (la somme des attributions égale la différence entre les sorties du modèle pour l’entrée réelle et la baseline).</p> <p>Le papier <a href="https://aclanthology.org/2024.findings-emnlp.551.pdf">Barkan et al., Improving LLM Attributions with Randomized Path-Integration, 2024</a> améliore la méthode des gradients intégrés (IG) en introduisant de la randomisation dans le processus d’intégration pour générer de meilleures attributions. En effet, contrairement à IG qui intègre les gradients par rapport aux embeddings d’entrée, RPI intègre les gradients par rapport aux scores d’attention du modèle. Ainsi, pour chaque couche, il calcule les gradients de la prédiction par rapport au tenseur d’attention interpolé le long d’un chemin randomisé entre une baseline aléatoire et les scores d’attention réels.</p> <h2 id="3-approches-basées-sur-les-vecteurs-décomposition-des-représentations-intermédiaires-faites-par-le-llm-de-chaque-token-du-prompt-avec-acp">3. Approches basées sur les vecteurs: décomposition des représentations intermédiaires faites par le LLM de chaque token du prompt avec ACP</h2> <p>Une autre approche consiste à étudier l’espace latent (c’est à dire les représentations intermédiaires (à différents niveaux du réseau) du LLM) des tokens d’entrée pour étudier cet espace : comment une perturbation de cet espace modifie la génération du LLM? Comment est structuré cet espace? Est-ce qu’en modifiant / étudiant les directions de cet espace je peux en déduire quelque chose sur la sortie du LLM?</p> <p>Le papier <a href="https://arxiv.org/pdf/2310.17191">How do Language Models Bind Entities in Context?</a> par exemple utilise des perturbations sur les représentations intermédiaires des tokens du prompt pour voir leur impact sur la génération du LLM.</p> <details> <summary style="cursor: pointer;">Cliquez pour voir l’illustration du papier binding ids, perturbant les représentations intermédiaires des tokens du prompt pour voir l'impact sur la génération du LLM</summary> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/binding_ids-480.webp 480w,/assets/img/explainability_llms/binding_ids-800.webp 800w,/assets/img/explainability_llms/binding_ids-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/explainability_llms/binding_ids.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </details> <p>Le papier <a href="https://arxiv.org/pdf/2407.12831">Truth is Universal: Robust Detection of Lies in LLMs</a> effectue quand à lui une ACP sur les représentations intermédiaires du dernier token de chaque phrase (le “.”) - le modèle étudié étant autorégressif, le dernier token encode la globalité de la phrase. Ils effectuent cette ACP sur les représentations intermédiaires d’un ensemble de données comprenant des phrases vraies (triangles orange dans les figures) et fausses (carrés violets), qu’ils avaient annotées au préalable. Ils observent deux directions principales qui émergent de cette analyse : une direction “générale de vérité” (tG) qui sépare efficacement les déclarations vraies des fausses indépendamment de leur polarité (affirmative ou négative), et une direction “sensible à la polarité” (tP) qui distingue les phrases affirmatives des phrases négatives. Dans l’espace d’activation du modèle, tG pointe systématiquement des déclarations fausses vers les vraies, indépendamment de la polarité grammaticale (affirmative/négative). On peut donc prédire si la génération du LLM va être une hallucination ou non à l’aide de cet espace.</p> <details> <summary style="cursor: pointer;">Cliquez pour voir l’illustration du papier qui fait l'ACP sur les représentations intermédiaires d'un ensemble de prompts vrai et faux</summary> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/acp_true_false_statements.PNG" sizes="95vw"/> <img src="/assets/img/explainability_llms/acp_true_false_statements.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </details> <h1 id="iii-les-approches-dexplicabilité-mécanistique-inférence-comprendre-quelle-partie-du-llm-est-responsable-de-quel-concept">III/ Les approches d’explicabilité mécanistique (inférence): comprendre quelle partie du LLM est responsable de quel concept</h1> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/slide10-480.webp 480w,/assets/img/explainability_llms/slide10-800.webp 800w,/assets/img/explainability_llms/slide10-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/explainability_llms/slide10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/slide11-480.webp 480w,/assets/img/explainability_llms/slide11-800.webp 800w,/assets/img/explainability_llms/slide11-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/explainability_llms/slide11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/slide12-480.webp 480w,/assets/img/explainability_llms/slide12-800.webp 800w,/assets/img/explainability_llms/slide12-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/explainability_llms/slide12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Le papier <a href="https://arxiv.org/pdf/2501.16496">Open Problems in Mechanistic Interpretability</a> décrit tous les défis / enjeux d’explicabilité mécanistique.</p> <h2 id="1-les-sondes-sur-lespace-latent">1. Les sondes sur l’espace latent</h2> <p>Ces approches consistent à étudier l’espace latent du modèle en lui donnant différents concepts en entrée (les “sondes”).</p> <h3 id="11-les-approches-de-sonde-par-ablation-de-lespace-latent">1.1 Les approches de sonde par ablation de l’espace latent</h3> <p>L’ablation de l’espace latent consiste à une mise à zéro ou suppression d’une activation.</p> <ul> <li>Ablation de couches, têtes d’attention ou paramètres</li> <li>Localisation de composants responsables :</li> <li>Hallucinations : Jin et al. (2024), Li et al. (2024a)</li> <li>Jailbreaks : Zhao et al. (2024d), Wei et al. (2024)</li> <li>Biais : Yang et al. (2023b), Ma et al. (2023)</li> </ul> <h3 id="12-les-approches-de-sonde-par-patching-de-lespace-latent">1.2 Les approches de sonde par patching de l’espace latent</h3> <p>Le patching consiste à remplacer une activation par une autre.</p> <ul> <li>Inspiré de l’analyse de médiation causale (Pearl, 2001)</li> <li>Remplacement d’activations intermédiaires</li> <li>Applications : Hallucinations (Monea et al., 2024), biais (Vig et al., 2020)</li> </ul> <p>Pour étudier les circuits:</p> <ul> <li>Path patching : Wang et al. (2023), Goldowsky-Dill et al. (2023)</li> </ul> <h3 id="13-les-approches-de-sonde-par-observation-réduction-de-dimension-et-autre-de-lespace-latent">1.3 Les approches de sonde par observation (réduction de dimension et autre) de l’espace latent</h3> <p>Il s’agit de projections de l’espace latent calculé sur différents concepts. On affiche donc les représentations des tokens / phrases avec et sans un concept spécifique, pour essayer de trouver un élément de l’espace latent qui les sépare.</p> <p>Pour se faire, on peut effectuer une moyenne des vecteurs appartenant à un certain concept (hallucinations (<a href="https://aclanthology.org/2024.emnlp-main.1012.pdf">Liu et al., On the Universal Truthfulness Hyperplane Inside LLMs</a>, 2024), jailbreaks (<a href="https://arxiv.org/pdf/2406.11717">Arditi et al. Refusal in Language Models Is Mediated by a Single Direction, 2024</a>)), ou encore des méthodes de réduction de dimension de l’espace latent, comme l’ACP (<a href="https://arxiv.org/pdf/2402.09733">Duan et al., Do LLMs Know about Hallucination? An Empirical Investigation of LLM’s Hidden States</a>). D’autres approches entraînent un classifier sur les représentations de l’espace latent (Détection d’hallucinations <a href="https://arxiv.org/pdf/2212.03827">Burns et al., Discovering Latent Knowledge in Language Models Without Supervision, 2022</a>, <a href="https://arxiv.org/pdf/2407.12831">Truth is Universal: Robust Detection of Lies in LLMs, Bürger et al., 2024</a>, jailbreaks <a href="https://aclanthology.org/2024.findings-emnlp.139.pdf">Zhou et al., How Alignment and Jailbreak Work: Explain LLM Safety through Intermediate Hidden States, 2024</a>).</p> <p>Cependant, ces différents approches supposent que ces concepts sont encodés comme directions linéaires, alors que ce n’est pas vraiment le cas dans les LLMs.</p> <h3 id="14-ces-approches-permettent-de-corriger-le-modèle-en-dirigeant-dune-certaine-manière-les-vecteurs-latents">1.4 Ces approches permettent de “corriger” le modèle en dirigeant d’une certaine manière les vecteurs latents</h3> <p>cf: hallucinations <a href="https://arxiv.org/pdf/2306.03341">Li et al., Inference-Time Intervention: Eliciting Truthful Answers from a Language Model, 2023</a>, jailbreaks <a href="https://arxiv.org/pdf/2308.10248">Turner et al.,Steering Language Models With Activation Engineering, 2023</a></p> <p>Exemple des figures de ces papiers pour “corriger” le modèle selon ces direction:</p> <p>Le papier <a href="https://arxiv.org/pdf/2306.03341">Inference-Time Intervention: Eliciting Truthful Answers from a Language Model</a> propose cette approche:</p> <details> <summary style="cursor: pointer;">Cliquez pour voir l’illustration de l'intervention sur les activations du modèle</summary> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/intervention-480.webp 480w,/assets/img/explainability_llms/intervention-800.webp 800w,/assets/img/explainability_llms/intervention-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/explainability_llms/intervention.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </details> <p>Le papier <a href="https://arxiv.org/pdf/2308.10248">Steering Language Models With Activation Engineering</a> propose cette approche:</p> <details> <summary style="cursor: pointer;">Cliquez pour voir une autre illustration d'une intervention sur les activations du modèle</summary> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/intervention2-480.webp 480w,/assets/img/explainability_llms/intervention2-800.webp 800w,/assets/img/explainability_llms/intervention2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/explainability_llms/intervention2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </details> <p>Le papier <a href="https://arxiv.org/pdf/2406.11717">Refusal in Language Models Is Mediated by a Single Direction</a> qui corrige les poids du modèle de la direction $r$ identifiée comme générant des jailbreaks : $x’ = x - rr^Tx$. L’<strong>ablation directionnelle</strong> « met à zéro » la composante suivant $r$ pour chaque activation du flux résiduel $x \in \mathbb{R}^{d_{\text{model}}}$.</p> <h2 id="12-les-auto-encodeurs-pour-apprendre-un-dictionnaire-des-éléments-du-réseau-sparse-dictionnary-learning">1.2 Les auto-encodeurs pour apprendre un “dictionnaire” des éléments du réseau (Sparse Dictionnary Learning)</h2> <p><strong>Analyse des neurones individuels</strong></p> <p><strong>Autoencodeurs épars (SAE)</strong></p> <ul> <li>Objectif : Désentrelement des concepts superposés</li> <li>Architecture : Encodeur → vecteur épars de concepts → Décodeur</li> <li><strong>Références clés</strong> :</li> <li>Fondamentaux : Sharkey et al. (2022), Bricken et al. (2023)</li> <li>Améliorations : Rajamanoharan et al. (2024a), Templeton et al. (2024)</li> <li><strong>Applications sécurité</strong> :</li> <li>Hallucinations : Ferrando et al. (2025), Theodorus et al. (2025)</li> <li>Jailbreaks : Härle et al. (2024), Muhamed et al. (2025)</li> <li>Biais : Hegde (2024), Zhou et al. (2025a)</li> </ul> <p><strong>Logit lens</strong></p> <ul> <li>Projection des vecteurs latents intermédiaires sur l’espace vocabulaire</li> <li>Origine : nostalgebraist (2020), Elhage et al. (2021)</li> <li>Améliorations : Belrose et al. (2023), Din et al. (2023)</li> <li>Applications : Mécanismes de stockage/rappel (Yu et al., 2023), hallucinations (Yu et al., 2024b)</li> </ul> <h3 id="121-le-principe-de-superposition--analyse-des-neurones-individuels">1.2.1 Le principe de superposition : Analyse des neurones individuels</h3> <p>Selon l’hypothèse de superposition, un réseau peut représenter plus de caractéristiques qu’il n’a de dimensions, à condition que chaque feature s’active de manière parcimonieuse. (chaque caractéristique (feature) d’un réseau de neurones ne doit pas être activée tout le temps, mais seulement dans des cas spécifiques et rares)</p> <p>Ces approches de SDL sont plus sophistiquées et “state of the art” que les approches de réduction de dimension avec ACP ou autre des activations, justement dû au fait de la polysémanticité des neurones. Si un neurone encode à la fois : “nature”, “vieux”, “lumière” dans un même sous-espace de représentation, l’ACP ne pourra pas dissocier ces trois concepts si leurs activations sont mélangées dans les mêmes dimensions.</p> <p>En gros, les activations sont traitées par un petit réseau neuronal à deux couches, correspondant respectivement à un encodeur et un décodeur, avec un espace latent large: L’encodeur encode l’activation de chaque variable latente, et le décodeur est une matrice qui sert de dictionnaire des directions latentes.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/superposition-480.webp 480w,/assets/img/explainability_llms/superposition-800.webp 800w,/assets/img/explainability_llms/superposition-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/explainability_llms/superposition.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>Identification des entrées activant fortement un neurone</li> <li>Références : Geva et al. (2021), Foote et al. (2023)</li> <li>Défi : Polysémantique des neurones (Arora et al., 2018)</li> <li>Fondamentaux : Sharkey et al. (2022), Bricken et al. (2023)</li> <li>Améliorations : Rajamanoharan et al. (2024a), Templeton et al. (2024)</li> <li>Hallucinations : Ferrando et al. (2025), Theodorus et al. (2025)</li> <li>Jailbreaks : Härle et al. (2024), Muhamed et al. (2025)</li> <li>Biais : Hegde (2024), Zhou et al. (2025a)</li> </ul> <p>Il existe plusieurs approches de SDL, qui traitent des activations différentes:</p> <ul> <li>Les autoencodeurs parcimonieux (SAE) (classique)</li> <li>Les transcodeurs (cf papier <a href="https://openreview.net/pdf?id=J6zHcScAo0">Transcoders Find Interpretable LLM Feature Circuits</a>)</li> <li>Les crosscodeurs (cf article <a href="https://transformer-circuits.pub/2024/crosscoders/index.html">Sparse Crosscoders for Cross-Layer Features and Model Diffing</a>)</li> </ul> <h3 id="122-analyse-de-groupes-de-neurones-à-différents-niveaux-dans-le-réseau">1.2.2 Analyse de groupes de neurones à différents niveaux dans le réseau</h3> <p>Les concepts sont ici représentées non par par neurone mais sur plusieurs layers. Du coup, il est naturel d’appliquer l’apprentissage de dictionnaires de manière conjointe entre les layers. Les crosscodeurs permettent de gérer la persistance d’une feature sur plusieurs couches.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/residual_stream-480.webp 480w,/assets/img/explainability_llms/residual_stream-800.webp 800w,/assets/img/explainability_llms/residual_stream-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/explainability_llms/residual_stream.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/superposition2-480.webp 480w,/assets/img/explainability_llms/superposition2-800.webp 800w,/assets/img/explainability_llms/superposition2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/explainability_llms/superposition2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/crosscoder-480.webp 480w,/assets/img/explainability_llms/crosscoder-800.webp 800w,/assets/img/explainability_llms/crosscoder-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/explainability_llms/crosscoder.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Les crosscodeurs peuvent nous aider en cas de superposition entre couches, mais ils peuvent également être utiles lorsqu’une feature calculée reste dans le flux résiduel pendant de nombreuses couches. Considérons le cycle de vie hypothétique suivant d’une feature à travers le flux résiduel :</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/crosscoder2-480.webp 480w,/assets/img/explainability_llms/crosscoder2-800.webp 800w,/assets/img/explainability_llms/crosscoder2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/explainability_llms/crosscoder2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h1 id="iv-les-approches-dexplicabilité-par-génération-de-raisonnement-du-llm">IV/ Les approches d’explicabilité par Génération de Raisonnement du LLM</h1> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/slide13-480.webp 480w,/assets/img/explainability_llms/slide13-800.webp 800w,/assets/img/explainability_llms/slide13-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/explainability_llms/slide13.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/slide14-480.webp 480w,/assets/img/explainability_llms/slide14-800.webp 800w,/assets/img/explainability_llms/slide14-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/explainability_llms/slide14.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Explorer comment les LLM peuvent interpréter leurs propres sorties en exprimant le raisonnement sous-jacent.</p> <h2 id="raisonnement-en-génération">Raisonnement en génération</h2> <h3 id="chain-of-thought-cot">Chain-of-Thought (CoT)</h3> <p>La Chain of Thought consiste à fournir, dans le prompt, quelques exemples de raisonnements détaillés pas-à-pas (“intermediate steps”), incitant le modèle à « penser à voix haute » avant de donner sa réponse.</p> <p>Plusieurs approches “améliorées” du CoT ont émergées:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/cot_sota-480.webp 480w,/assets/img/explainability_llms/cot_sota-800.webp 800w,/assets/img/explainability_llms/cot_sota-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/explainability_llms/cot_sota.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Il y a notamment le CoT-SC (Chain of Thought Self Consistency) qui consiste à choisir la réponse la plus fréquente ou la plus cohérente en agrégeant ces chemins de raisonnement issu du CoT, comme présenté dans le papier <a href="https://arxiv.org/pdf/2203.11171">Self-Consistency Improves Chain of Thought Reasoning in Language Models</a></p> <details> <summary style="cursor: pointer;">Cliquez pour voir une illustration du CoT-SC</summary> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/explainability_llms/cot_sc-480.webp 480w,/assets/img/explainability_llms/cot_sc-800.webp 800w,/assets/img/explainability_llms/cot_sc-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/explainability_llms/cot_sc.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </details> <h4 id="extension-via-le-tree-of-thoughts-tot">Extension via le Tree of Thoughts (ToT)</h4> <p>Tree of Thoughts étend la CoT en explorant un arbre de pensées : à chaque nœud, le modèle génère plusieurs « thoughts » candidates, peut les évalue globalement, … <a href="https://arxiv.org/pdf/2305.10601">Tree of Thoughts: Deliberate Problem Solving with Large Language Models</a></p> <h4 id="combinaison-du-cot-et-de-lappel-de-tool-des-llms-cas-des-agents--react">Combinaison du CoT et de l’appel de tool des LLMs (cas des agents) : ReAct</h4> <p>Le paradigme ReAct combine raisonnement (traces de CoT) et actions (interrogation d’API, interaction avec un environnement) de façon intercalée, permettant de corriger en temps réel et de réduire l’hallucination.</p> <p><a href="https://arxiv.org/pdf/2210.03629">ReAct: Synergizing Reasoning and Acting in Language Models</a></p> <h4 id="autres-papiers-sur-le-sujet">Autres papiers sur le sujet</h4> <p>Quelques papiers intéressants sur le sujet: <a href="https://arxiv.org/pdf/2502.03671">Advancing Reasoning in Large Language Models: Promising Methods and Approaches</a> <a href="https://arxiv.org/pdf/2502.17419">From System 1 to System 2: A Survey of Reasoning Large Language Models</a> <a href="https://arxiv.org/pdf/2507.06203">A Survey on Latent Reasoning</a> <a href="https://arxiv.org/pdf/2505.16782">Reasoning Beyond Language: A Comprehensive Survey on Latent Chain-of-Thought Reasoning</a></p>]]></content><author><name></name></author><category term="sample-posts"/><category term="XAI,"/><category term="influence,"/><category term="mechanistic,"/><category term="saliency,"/><category term="attention,"/><category term="transformers"/><summary type="html"><![CDATA[Méthodes d'explicabilité de la génération de texte par les LLMs]]></summary></entry><entry><title type="html">Biais Positionnels dans les transformers auto-régressifs</title><link href="https://camillebrl.github.io/blog/2025/positional_biais/" rel="alternate" type="text/html" title="Biais Positionnels dans les transformers auto-régressifs"/><published>2025-06-16T22:00:00+00:00</published><updated>2025-06-16T22:00:00+00:00</updated><id>https://camillebrl.github.io/blog/2025/positional_biais</id><content type="html" xml:base="https://camillebrl.github.io/blog/2025/positional_biais/"><![CDATA[<h1 id="introduction-sur-le-biais-positionnel">Introduction sur le biais positionnel</h1> <h2 id="le-biais-positionnel--cest-quoi-">Le biais positionnel : c’est quoi ?</h2> <div class="row"> <div class="col-md-8"> Le papier [Lost in the Middle: How Language Models Use Long Contexts](https://arxiv.org/pdf/2307.03172) décrit comment, pour des contextes très longs, les LLMs se concentrent surtout sur les débuts et la fin du prompt. Le biais positionnel dans les LLMs, c'est la tendance du modèle à se concentrer excessivement sur certaines parties de l'entrée, qu'importe la sémantique. Et cette concentration influence significativement les performances et la fiabilité des transformers: en fonction de l'ordre des éléments dans le prompt, le modèle va générer des réponses différentes. Notamment, plus on a un contexte long, plus le modèle a tendance à se concentrer sur les éléments au début et à la fin du prompt (effet "lost in the middle"). </div> <div class="col-md-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/positional_biais/u-shape.PNG" sizes="95vw"/> <img src="/assets/img/positional_biais/u-shape.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="le-biais-positionnel--ça-vient-doù-">Le biais positionnel : ça vient d’où ?</h2> <ul> <li><strong>Le masque causal</strong> <blockquote> <p>Avec un masque causal, chaque token ne peut voir que les tokens qui le précèdent. Cela signifie que les tokens situés au début sont accessibles à presque tous les calculs d’attention ultérieurs, alors que ceux situés plus tard ne le sont qu’à partir d’un certain point.</p> </blockquote> </li> <li><strong>L’encodage de position</strong> (relatif, RoPE…) <blockquote> <p>Ce type d’encodage de position favorisent la proximité avec le token courant. Ainsi, les jetons situés près de la fin de la séquence bénéficient d’une attention renforcée, car leur représentation est plus fortement influencée par la similarité de position avec le token en cours de génération.</p> </blockquote> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/positional_biais/relative_pos_encoding-480.webp 480w,/assets/img/positional_biais/relative_pos_encoding-800.webp 800w,/assets/img/positional_biais/relative_pos_encoding-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/positional_biais/relative_pos_encoding.png" class="img-fluid rounded z-depth-1 float-right ml-3" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="quest-ce-que-lencodage-de-position-">Qu’est-ce que l’encodage de position ?</h2> <blockquote> <p>L’encodage de position est nécessaire dans les transformers à cause du calcul de l’attention qui prend en compte tous les azutres tokens de la séquence. Sans encodage positionnel, chaque token identique aurait la même influence, peu importe l’endroit dans la séquence :<br/> “Le chat mange la souris” et “La souris mange le chat” seraient équivalents. Avec l’encodage de position, deux tokens identiques mais à des positions différentes auront des vecteurs différents.</p> </blockquote> <ol> <li> <p><strong>Encodage absolu</strong> (sinusoïdal ou appris)<br/> Ajouté aux embeddings initiaux. Limité pour extrapoler à des longueurs supérieures à celles vues à l’entraînement.</p> </li> <li><strong>Encodages relatifs</strong> <ul> <li>T5 : biais appris pour chaque distance relative.</li> <li>Alibi : biais fonctionnel selon la distance.</li> </ul> </li> <li><strong>RoPE</strong> (Rotary Positional Encoding)<br/> rotation appliqué à la représentation intermédiaire de chaque token dépendante de la position du token au niveau du calcul du score d’attention (chaque paire de dimensions du vecteur est tournée dans le plan d’un angle qui dépend de sa position dans la séquence). Mais du coup, quand on applique cette rotation à query et key, et qu’on fait un produit scalaire, le dot product dépend de la position relative $i−j$, même si on encode chaque position de manière absolue.</li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/positional_biais/periodic_attn.PNG" sizes="95vw"/> <img src="/assets/img/positional_biais/periodic_attn.PNG" class="rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h1 id="sota-des-approches-pour-corriger-le-biais-positionnel">SOTA des approches pour corriger le biais positionnel</h1> <h2 id="1-modifier-le-mécanisme-dattention">1. Modifier le mécanisme d’attention</h2> <ul> <li> <p><strong>Stable Mask</strong> (<a href="https://arxiv.org/pdf/2402.04779">2402.04779</a>)<br/> Ajout d’un “pseudo-score” $-\gamma\times(j-1)$ présente une approche de “compensation” du score d’attention excessif sur les premiers tokens en ajoutant un “pseudo-score”: $-\gamma \times (j-1)$ pour la j-ième position, qui diminue au fur et à mesure de la séquence.</p> </li> <li> <p><strong>Calibration du score d’attention</strong> (<a href="https://arxiv.org/pdf/2406.16008">2406.16008</a>) présente une approche dans laquelle on a le score d’attention de la query avec un doc à la position k qui est fonction ($f$) de la pertinence du doc \(\text{rel}(\text{doc}_k)\) et du biais positionnel à la position k $b_k$. En simplifiant la fonction $f$ (rasoir d’Occam) par une fonction linéaire, on a alors \(\text{Attn}(\text{query}, doc_k) = \text{rel}(\text{doc}_k) + b_k + \epsilon\) <strong>Donc en gros ils supposent que le score d’attention entre le document et la query est une combinaison linéaire du biais de position et de la pertinence réelle du document avec la query</strong>. Pour isoler $\text{rel}(\text{doc}_k)$, ils <strong>introduisent un document “dummy” à la même position $k$ que le document</strong>: \(\text{doc}_{\text{k;dum}}$, on a alors $\text{rel}(\text{doc}_k) = \text{Attn}(\text{query}, doc_k) - \text{Attn}(\text{query}, doc_{k;dum}) + \epsilon\) Grâce à la pertinence “effective” de chaque document, ils calculent, $\forall k$, un coefficient de rééchelonnement $\alpha_k$, qui est une fonction Softmax appliquée sur le score de pertinence du document $\text{doc}_k$.</p> </li> <li> <p><strong>Attention bidirectionnelle entre documents</strong> (PCW <a href="https://arxiv.org/pdf/2212.10947">2212.10947</a>) propose une modification de l’attention entre documents pour un traitement “indépendant”: Au lieu d’utiliser l’attention causale (unidirectionnelle) qui impose un ordre strict. A contrario, le papier <a href="https://openreview.net/attachment?id=fvkElsJOsN&amp;name=pdf">l’approche d’attention bidirectionnelle entre documents</a> propose une approche d’attention bidirectionnelle qui permet à chaque document d’interagir équitablement avec tous les autres.</p> <div class="row"> <div class="col-md-6"> </div></div> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/positional_biais/bidirectional_attn.PNG" sizes="95vw"/> <img src="/assets/img/positional_biais/bidirectional_attn.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;/div&gt;
&lt;div class="col-md-6"&gt;
</code></pre></div></div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/positional_biais/cropped_attn.PNG" sizes="95vw"/> <img src="/assets/img/positional_biais/cropped_attn.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;/div&gt;
</code></pre></div></div> <p>&lt;/div&gt;</p> <ul> <li> <p><strong>Diminuer le biais positionnel dû à RoPE</strong>: L’objectif principal de <a href="https://arxiv.org/pdf/2104.09864">ROPE</a> est d’encoder l’information positionnelle de sorte que le produit scalaire des embeddings de requête et de clé contienne intrinsèquement l’information relative à la position, c’est-à-dire $f(q_m, m)^T f(k_n, n) = f(q_m, k_n, m - n)$. Ici, $f$ est la fonction d’encodage positionnel appliquée aux embeddings de requête et de clé aux positions $m$ et $n$, respectivement. Pour satisfaire cette condition, la fonction $f$ est définie comme une fonction complexe vectorielle :</p> \[f(x, m) = x e^{im\theta} = \left[(x_1 + i x_2)e^{im\theta_1};(x_3 + i x_4)e^{im\theta_2};\ldots;(x_{l-1} + i x_l)e^{im\theta_{l/2}}\right]^T\] <p>Dans cette équation, $l$ représente la dimension des embeddings, $\theta_k = 10000^{-2k/l}$, et $i$ est l’unité imaginaire. Pour le calcul du score d’attention, RoPE considère la partie réelle du produit, spécifiquement $\operatorname{Re}\left(f(q_m, m)^T f(k_n, n)\right)$. La fonction trigonométrique $\cos((m-n) \theta)$ est périodique, d’où la waveform qu’on obtient. pour certaines valeurs de $(m−n)$, le cosinus (et le sinus) prend des valeurs élevées (les “pics”), tandis que pour d’autres il prend des valeurs faibles (les “creux”). Ainsi, si une information cruciale se trouve à une position qui correspond à un trough de l’oscillation, son score d’attention sera relativement bas. La fréquence des oscillations est modulée par rapport à $\theta_k$: Donc ce qui se fait couramment pour contrebalancer ça, c’est de prendre plusieurs bases de $\theta$, qui fixe l’échelle exponentielle à laquelle les fréquences décroissent.</p> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/positional_biais/periodic_attn.PNG" sizes="95vw"/> <img src="/assets/img/positional_biais/periodic_attn.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Ainsi, plusieurs approches visent à contre-balancer le biais positionnel induit par RoPE:</p> <ul> <li> <p><strong>Attention Bucket</strong> (<a href="https://arxiv.org/pdf/2312.04455">2312.04455</a>)<br/> Plusieurs bases $\theta$ traitées en parallèle.</p> </li> <li> <p><strong>MS-PoE</strong> (<a href="https://arxiv.org/pdf/2403.04797">2403.04797</a>)<br/> C’est une approche qui applique un facteur $r$ à la position des tokens par tête, modifiant du coup l’oscillation par tête $m/r \theta$ afin de garder la base RoPE sur laquelle a été entraîné le modèle (dans une approche de correction du biais à l’inférence)</p> <div class="col-md-6"> </div> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/positional_biais/ms_poe.PNG" sizes="95vw"/> <img src="/assets/img/positional_biais/ms_poe.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>&lt;/div&gt;</p> <div class="col-md-6"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/positional_biais/attn_bucket.PNG" sizes="95vw"/> <img src="/assets/img/positional_biais/attn_bucket.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <h2 id="2-jouer-sur-les-données">2. Jouer sur les données</h2> <ul> <li> <p><strong>IN2 training</strong> (<a href="https://arxiv.org/pdf/2404.16811">2404.16811</a>)<br/> Fine-tuning sur des QA avec contextes longs formés par concaténation aléatoire de segments courts.</p> </li> <li> <p><strong>Réordonnancement des documents</strong> (<a href="https://aclanthology.org/2024.acl-long.91.pdf">ACL-Long’24</a>)<br/> Placer les plus pertinents en début ou fin de prompt.</p> </li> </ul> <h1 id="introduction-du-papier--mitigate-positional-biais-via-scaling-a-single-dimension-">Introduction du papier « Mitigate Positional Biais via Scaling a Single Dimension »</h1> <ul> <li>Le biais positionnel provient des patterns d’attention (focus sur début/fin).</li> <li>Causal mask + encodage positionnel génèrent des “dimensions positionnelles” dans les hidden-states.</li> <li><strong>Objectifs</strong> : <ol> <li>Identifier ces dimensions.</li> <li>Les modifier (scaling) pour diminuer le biais.</li> </ol> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/positional_biais/pos_hidden_state.PNG" sizes="95vw"/> <img src="/assets/img/positional_biais/pos_hidden_state.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="tâche-de-retrieval-utilisée">Tâche de retrieval utilisée</h2> <ul> <li><strong>Clés/valeurs aléatoires</strong> pour isoler le pure retrieval.</li> <li> <p>On mesure</p> \[A_G = \frac{1}{|G|} \sum_{j\in G} a_{l,j}\] <p>où $l$=dernier token (interrogateur), $G$=positions de la clé, $a_{l,j}$=poids d’attention.</p> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/positional_biais/task.PNG" sizes="95vw"/> <img src="/assets/img/positional_biais/task.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="identification-des-dimensions-positionnelles">Identification des dimensions positionnelles</h2> <ol> <li><strong>Monotonie</strong> : $h(p)$ doit être strictement croissante ou décroissante.</li> <li><strong>Smoothness</strong> : dérivée seconde faible (évolution “douce”).</li> <li> <p>Choix de la dimension $t$ et de l’échelle $s&lt;1$ minimisant la perte</p> \[\arg\min_{h_t,s&lt;1} \mathbb{E}\left[\sum_{i=1}^{|P|} \mathcal{L}(x,y,p_i; F(\theta,h_t,s))\right]\] <p>avec $F(\theta,h_t,s)$ le modèle scaled sur la $t$-ième dimension.</p> </li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/positional_biais/algo.PNG" sizes="95vw"/> <img src="/assets/img/positional_biais/algo.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="construction-de-hp_i">Construction de $h(p_i)$</h2> <ul> <li>Approximation par <strong>moindres carrés segmentés</strong> pour lisser le signal.</li> <li>Hypothèse : $\textbf{hidden_state}_i = h(p_i) + \epsilon_i$.</li> <li>Permet de distinguer tendance monotone et bruit.</li> </ul> <h2 id="résultats-de-lidentification">Résultats de l’identification</h2> <ul> <li>Tendance monotone dès la couche 1, s’amplifie dans les couches supérieures.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/positional_biais/example_mistral.PNG" sizes="95vw"/> <img src="/assets/img/positional_biais/example_mistral.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="dimensions-causées-par-le-masque-causal">Dimensions causées par le masque causal</h2> <ul> <li>Réduction du PE de 200 pour positions 400–600 : effet mineur.</li> <li>Rogner le masque causal pour ces positions : fortes fluctuations.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/positional_biais/causal_mask_modification_perturb_pos_dim.PNG" sizes="95vw"/> <img src="/assets/img/positional_biais/causal_mask_modification_perturb_pos_dim.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="correction-proposée">Correction proposée</h2> <ul> <li>Prouvé que la performance biasée vient des patterns d’attention : en doublant le score à la position 25, on déplace l’attention.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/positional_biais/attn_weights_distrib.PNG" sizes="95vw"/> <img src="/assets/img/positional_biais/attn_weights_distrib.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p><strong>Modification</strong> : scaler la dimension $p$ uniquement pour le calcul du score d’attention du dernier token $l$ :</p> \[z = \begin{cases} \mathrm{Softmax}\left((q_i K^\top + \mathrm{Mask})/\sqrt d\right)V, &amp; i&lt;l,\\ \mathrm{Softmax}\left((\bar q_l\;\bar K^\top)/\sqrt d\right)V, &amp; i=l. \end{cases}\] </li> </ul> <h2 id="effet-du-scaling">Effet du scaling</h2> <ul> <li>$s&gt;1$ → focus début ; $s&lt;0$ → focus fin.</li> <li>$s\in[-1,0.5]$ → distribution équilibrée.</li> </ul> <div class="col-md-6"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/positional_biais/scaling_pos_hidden_states.PNG" sizes="95vw"/> <img src="/assets/img/positional_biais/scaling_pos_hidden_states.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-md-6"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/positional_biais/scaling_factor.PNG" sizes="95vw"/> <img src="/assets/img/positional_biais/scaling_factor.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <ul> <li><strong>Varie selon les modèles</strong> :</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/positional_biais/examples_scaling_factors.PNG" sizes="95vw"/> <img src="/assets/img/positional_biais/examples_scaling_factors.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="performances">Performances</h2> <p>Gain significatif sur LongBench :</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/positional_biais/longbench_perf.PNG" sizes="95vw"/> <img src="/assets/img/positional_biais/longbench_perf.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="limitations-selon-moi">Limitations (selon moi)</h2> <ol> <li>Hypothèses de monotonie et smoothness de $h(p)$ très discutables et infondées.</li> <li>Lien causal masque =&gt; hidden-states pas rigoureusement démontré. le fait qu’il y ait du biais positionnel dans les états cachés est un postulat de départ de ce papier non prouvé proprement.</li> <li>Approche de scaling de “la dimension positionnelle” discutable</li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/positional_biais/limits-480.webp 480w,/assets/img/positional_biais/limits-800.webp 800w,/assets/img/positional_biais/limits-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/positional_biais/limits.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="points-forts-du-papier">Points forts du papier</h2> <ul> <li>Séparation mécanique sémantique vs. position. Un autre papier étudie cette séparation pour comprendre quelle tête ont les parties positionnelles et sémantiques dans les transformers (notamment l’aspect low-rank et low-frequency de la partie positionnelle): <a href="https://openreview.net/pdf?id=1M0qIxVKf6">Uncovering hidden geometry in Transformers via disentangling position and context</a>. Un autre papier (<a href="https://openreview.net/pdf?id=zeYyq0GpXO">Exploring Context Window of Large Language Models via Decomposed Positional Vectors</a>) cherche à décomposer les états cachés de chaque token en partie sémantique et positionnelle en faisant une estimation empirique des vecteurs positionnels sur un grand nombre d’exemples.</li> <li>Peu de papiers visent à corriger le biais positionnel dû au masque causal, donc j’ai trouvé ce papier intéressant qui s’attaque à cela au lieu de s’attaquer au biais positionnel dû à l’encodage positionnel.</li> </ul> <h1 id="pour-aller-plus-loin--découpler-sémantique-et-position">Pour aller plus loin : découpler sémantique et position</h1> <h2 id="spline-based-transformers-eccv25">Spline-based Transformers (ECCV’25)</h2> <ul> <li>Suppression de l’encodage positionnel.</li> <li> <p>Introduction de <em>tokens de contrôle</em> formant une spline :</p> \[s(t)=\sum_{i=0}^n N_{i,k}(t)\,\mathbf p_i\] </li> <li>Position implicite via la géométrie.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/positional_biais/spline_based_transformers.PNG" sizes="95vw"/> <img src="/assets/img/positional_biais/spline_based_transformers.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="decomposed-positional-vectors-neurips">Decomposed Positional Vectors (NeurIPS)</h2> <ul> <li>Décomposition : $h_{l,t}=p_{l,t}+cs_{l,t}$.</li> <li>Estimation : $p_{l,t}=\frac{1}{N}\sum_{s=1}^N h^{(s)}_{l,t}$.</li> <li>Isolation : $cs_{l,t}=h_{l,t}-p_{l,t}$.</li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="XAI,"/><category term="biais,"/><category term="transformers"/><summary type="html"><![CDATA[Description du biais positionnel dans les transformers auto-régressifs]]></summary></entry></feed>